

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.ico">
  <link rel="icon" href="/img/favicon.ico">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#84674f">
  <meta name="author" content="Ryan LI">
  <meta name="keywords" content="">
  
    <meta name="description" content="Published in Dec 2021, this new work by Kaiming He draws a lot of attention from the community. The astounding result of unsupervised transfer learning and the capability of reconstructing highly mas">
<meta property="og:type" content="article">
<meta property="og:title" content="paper reading: MAE">
<meta property="og:url" content="https://daydreamatnight.github.io/2022/04/27/paper-reading-MAE/index.html">
<meta property="og:site_name" content="ShouRou">
<meta property="og:description" content="Published in Dec 2021, this new work by Kaiming He draws a lot of attention from the community. The astounding result of unsupervised transfer learning and the capability of reconstructing highly mas">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://daydreamatnight.github.io/2022/04/27/paper-reading-MAE/MAE%20architecture.png">
<meta property="og:image" content="https://daydreamatnight.github.io/2022/04/27/paper-reading-MAE/paper-reading-MAE/MAE%20result.png">
<meta property="og:image" content="https://daydreamatnight.github.io/2022/04/27/paper-reading-MAE/paper-reading-MAE/MAE%20result2.png">
<meta property="og:image" content="https://daydreamatnight.github.io/2022/04/27/paper-reading-MAE/paper-reading-MAE/MAE%20mask%20ratio.png">
<meta property="article:published_time" content="2022-04-26T18:00:38.000Z">
<meta property="article:modified_time" content="2022-04-30T19:31:11.529Z">
<meta property="article:author" content="Ryan LI">
<meta property="article:tag" content="deep learning">
<meta property="article:tag" content="paper reading">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://daydreamatnight.github.io/2022/04/27/paper-reading-MAE/MAE%20architecture.png">
  
  
  <title>paper reading: MAE - ShouRou</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/a11y-light.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"daydreamatnight.github.io","root":"/","version":"1.8.14","typing":{"enable":true,"typeSpeed":1,"cursorChar":"","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":"♪"},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":true,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.0.0"></head>


<body>
  <header style="height: 30vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>ShouRou</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/article_banner.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.1)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="paper reading: MAE">
              
            </span>

            
              <div class="mt-3">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-author" aria-hidden="true"></i>
      Ryan LI
    </span>
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2022-04-27 02:00" pubdate>
        April 27, 2022 am
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      6.3k words
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      21 minutes
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">paper reading: MAE</h1>
            
              <p class="note note-info">
                
                  Last update：15 hours ago
                
              </p>
            
            <div class="markdown-body">
              <blockquote>
<p>Published in Dec 2021, this new work by Kaiming He draws a lot of attention from the community. The astounding result of unsupervised transfer learning and the capability of reconstructing highly masked (up to 90%) images might herald a new era in CV.</p>
</blockquote>
<blockquote>
<p>This is a <a href="https://daydreamatnight.github.io/2022/04/02/paper-reading-start/">series of paper reading notes</a>, hopefully, to push me to read paper casually and to leave some record of what I've learned.</p>
</blockquote>
<span id="more"></span>
<p>Paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2111.06377">Masked autoencoders are scalable vision learners</a></p>
<p>Useful link: https://www.bilibili.com/video/BV1sq4y1q77t/</p>
<h3 id="abstract">Abstract</h3>
<p>Inspired by BERT and ViT, this paper proposes an asymmetric, transformer-based, denoising auto-encoder architecture. The unsupervised pre-training task is to reconstruct highly masked input images. The pre-training time is reduced by 3 times with competitive accuracy. The transfer performance is even better than the supervised pre-training models.</p>
<h3 id="conclusion">Conclusion</h3>
<p>The results show that MAE makes scaleable unsupervised pre-training in CV applicable, a similar route to that of NLP.</p>
<p>They claim the semantic density difference between text and image leads to different masking operations. Besides, the patch masking operation does not separate semantic entities, meaning one masked patch may include more than one piece of semantic information, unlike language. However, the reconstruction results show that the model manages to learn from complex semantics.</p>
<h3 id="introduction">Introduction</h3>
<p>Although yielding excellent success in NLP, applying scalable unsupervised models in CV is still a challenging problem. But why? i.e. <strong>what makes masked autoencoding different between vision and language? </strong>3 reasons are discussed:</p>
<ul>
<li><p>The architecture difference between convolution and transformer: it's hard to integrate masked embedding or positional embedding to the convolution layer. --addressed by ViT.</p></li>
<li><p>The information density difference between text and image: Unlike high-semantic text, natural signals in images possess heavy spatial redundancy. --addressed by masking a very high portion of random patches.</p></li>
<li><p>Decoder difference: in NLP, take BERT as an example, a simple linear projection is used as a decoder, while in vision, a simple decoder is not powerful enough to reconstruct the semantic level information -- addressed by substituting linear projection with transformer layers.</p></li>
</ul>
<p>Then, the idea of MAE is on the front door. The encoder processes only the unmasked patches, while the lightweight decoder reconstructs the whole image from the encoded latent representation and the [mask] tokens. With a very high masking ratio(e.g. 75%), the pre-training time can be reduced by 3 times.</p>
<p>Besides, the data capacity and generalisation performance are great. SOTA accuracy is achieved with fine tuning on a medium-sized dataset.</p>
<h3 id="relate-work">Relate work</h3>
<p>Works in 4 areas are briefly reviewed.</p>

    <div class="markmap-container" style="height:300px">
      <svg data='{"t":"root","d":0,"v":"","c":[{"t":"list_item","d":2,"p":{"lines":[0,1]},"v":"<strong>Masked language modelling:</strong>","c":[{"t":"list_item","d":4,"p":{"lines":[2,3]},"v":"<a href=\"https://arxiv.org/abs/1810.04805\">BERT</a>"},{"t":"list_item","d":4,"p":{"lines":[4,5]},"v":"<a href=\"https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf\">GPT</a>"}]},{"t":"list_item","d":2,"p":{"lines":[5,6]},"v":"<strong>Auto-encoding:</strong>","c":[{"t":"list_item","d":4,"p":{"lines":[7,8]},"v":"<a href=\"https://proceedings.neurips.cc/paper/1993/hash/9e3cfc48eccf81a0d57663e129aef3cb-Abstract.html\">classic autoencoders</a>: PCA, k-means"},{"t":"list_item","d":4,"p":{"lines":[9,10]},"v":"<a href=\"https://dl.acm.org/doi/abs/10.1145/1390156.1390294\">denoising autoencoders(DAE)</a>"}]},{"t":"list_item","d":2,"p":{"lines":[10,11]},"v":"<strong>Masked image encoding:</strong>","c":[{"t":"list_item","d":4,"p":{"lines":[12,13]},"v":"classic","c":[{"t":"list_item","d":6,"p":{"lines":[13,14]},"v":"pioneer work <a href=\"https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf?ref=https://githubhelp.com\">SDAE</a>"},{"t":"list_item","d":6,"p":{"lines":[14,15]},"v":"<a href=\"http://openaccess.thecvf.com/content_cvpr_2016/html/Pathak_Context_Encoders_Feature_CVPR_2016_paper.html\">context encoder</a>"}]},{"t":"list_item","d":4,"p":{"lines":[16,17]},"v":"transformer based:","c":[{"t":"list_item","d":6,"p":{"lines":[17,18]},"v":"<a href=\"http://proceedings.mlr.press/v119/chen20s.html\">iGPT</a>"},{"t":"list_item","d":6,"p":{"lines":[18,19]},"v":"<a href=\"https://arxiv.org/abs/2010.11929\">ViT</a>"},{"t":"list_item","d":6,"p":{"lines":[19,20]},"v":"<a href=\"https://arxiv.org/abs/2106.08254\">BEiT</a>"}]}]},{"t":"list_item","d":2,"p":{"lines":[20,21]},"v":"<strong>Self-supervised learning:</strong>","c":[{"t":"list_item","d":4,"p":{"lines":[21,22]},"v":"CNN based:","c":[{"t":"list_item","d":6,"p":{"lines":[22,23]},"v":"<a href=\"http://openaccess.thecvf.com/content_iccv_2015/html/Wang_Unsupervised_Learning_of_ICCV_2015_paper.html\">Unsupervised learning of visual representations using videos</a>"},{"t":"list_item","d":6,"p":{"lines":[23,24]},"v":"<a href=\"https://link.springer.com/chapter/10.1007/978-3-319-46466-4_5\">CFN</a>"},{"t":"list_item","d":6,"p":{"lines":[24,25]},"v":"<a href=\"https://link.springer.com/chapter/10.1007/978-3-319-46487-9_40\">Colorful Image Colorization</a>"},{"t":"list_item","d":6,"p":{"lines":[25,26]},"v":"<a href=\"https://arxiv.org/abs/1803.07728\">Unsupervised representation learning by predicting image rotations</a>"}]},{"t":"list_item","d":4,"p":{"lines":[26,27]},"v":"Transformer based:","c":[{"t":"list_item","d":6,"p":{"lines":[27,28]},"v":"<a href=\"https://arxiv.org/abs/2010.11929\">ViT</a>"}]},{"t":"list_item","d":4,"p":{"lines":[28,29]},"v":"Contrastive learning based:","c":[{"t":"list_item","d":6,"p":{"lines":[29,30]},"v":"<a href=\"http://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.html\">Unsupervised feature learning via non-parametric instance discrimination</a>"},{"t":"list_item","d":6,"p":{"lines":[30,31]},"v":"<a href=\"https://arxiv.org/abs/1807.03748\">Representation learning with contrastive predictive coding</a>"},{"t":"list_item","d":6,"p":{"lines":[31,32]},"v":"<a href=\"http://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.html\">MOCO</a>"}]}]}],"p":{}}'></svg>
    </div>
  
<h3 id="approach">Approach</h3>
<p>The architecture and the training approach is briefly covered in the sketch below:</p>
<p><img src="MAE architecture.png" srcset="/img/loading.gif" lazyload alt="MAE architecture" style="zoom:40%;" /></p>
<p>Additional details about the architecture: For per-processing, non-overlapped patching and random uniform masking are adopted. The [mask] token is shared, and another position embedding is introduced to the decoder input so that the inputs are different on different masked area. But it is unclear whether the positional embedding is performed only on the [mask] token or on the whole input, i.e. encoded patches + [mask] token. Furthermore, the default decoder has &lt;10% computation per token compared with the encoder.</p>
<p>Reconstruction target: The decoder aims to recreate the pixels of masked patches. The loss function is the mean squared error (MSE) between the output and the original image, only on the masked region of course. Besides, a variation reconstructing the normalised pixels shows an improvement in representation quality.</p>
<h3 id="imagenet-experiments">ImageNet experiments</h3>
<p>First, the most astonish reconstruction results are shown below:</p>
<p><img src="paper-reading-MAE/MAE result.png" srcset="/img/loading.gif" lazyload alt="MAE result" style="zoom:50%;" /></p>
<p><img src="paper-reading-MAE/MAE result2.png" srcset="/img/loading.gif" lazyload alt="MAE result2" style="zoom:30%;" /></p>
<p>Mask ratio is higher than <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">BERT</a>(15%) and <a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v119/chen20s.html">iGPT</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.11929">ViT</a> and <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.08254">BEiT</a>(25%-50%),</p>
<p><img src="paper-reading-MAE/MAE mask ratio.png" srcset="/img/loading.gif" lazyload alt="MAE mask ratio" style="zoom:60%;" /></p>
<h3 id="transfer-learning-experiments">Transfer learning experiments</h3>
<h3 id="reference">Reference</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. <em>arXiv preprint arXiv:1810.04805</em>.</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf">Radford, A., Narasimhan, K., Salimans, T., &amp; Sutskever, I. (2018). Improving language understanding by generative pre-training.</a></p>
<p><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/1993/hash/9e3cfc48eccf81a0d57663e129aef3cb-Abstract.html">Hinton, G. E., &amp; Zemel, R. (1993). Autoencoders, minimum description length and Helmholtz free energy. <em>Advances in neural information processing systems</em>, <em>6</em>.</a></p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/1390156.1390294">Vincent, P., Larochelle, H., Bengio, Y., &amp; Manzagol, P. A. (2008, July). Extracting and composing robust features with denoising autoencoders. In <em>Proceedings of the 25th international conference on Machine learning</em> (pp. 1096-1103).</a></p>
<p><a target="_blank" rel="noopener" href="https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf?ref=https://githubhelp.com">Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P. A., &amp; Bottou, L. (2010). Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. <em>Journal of machine learning research</em>, <em>11</em>(12).</a></p>
<p><a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2016/html/Pathak_Context_Encoders_Feature_CVPR_2016_paper.html">Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., &amp; Efros, A. A. (2016). Context encoders: Feature learning by inpainting. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> (pp. 2536-2544).</a></p>
<p><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v119/chen20s.html">Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., &amp; Sutskever, I. (2020, November). Generative pretraining from pixels. In <em>International Conference on Machine Learning</em> (pp. 1691-1703). PMLR.</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.11929">Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... &amp; Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. <em>arXiv preprint arXiv:2010.11929</em>.</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.08254">Bao, H., Dong, L., &amp; Wei, F. (2021). Beit: Bert pre-training of image transformers. <em>arXiv preprint arXiv:2106.08254</em>.</a></p>
<p><a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_iccv_2015/html/Wang_Unsupervised_Learning_of_ICCV_2015_paper.html">Wang, X., &amp; Gupta, A. (2015). Unsupervised learning of visual representations using videos. In <em>Proceedings of the IEEE international conference on computer vision</em> (pp. 2794-2802).</a></p>
<p><a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-319-46466-4_5">Noroozi, M., &amp; Favaro, P. (2016, October). Unsupervised learning of visual representations by solving jigsaw puzzles. In <em>European conference on computer vision</em> (pp. 69-84). Springer, Cham.</a></p>
<p><a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-319-46487-9_40">Zhang, R., Isola, P., &amp; Efros, A. A. (2016, October). Colorful image colorization. In <em>European conference on computer vision</em> (pp. 649-666). Springer, Cham.</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.07728">Gidaris, S., Singh, P., &amp; Komodakis, N. (2018). Unsupervised representation learning by predicting image rotations. <em>arXiv preprint arXiv:1803.07728</em>.</a></p>
<p><a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.html">Wu, Z., Xiong, Y., Yu, S. X., &amp; Lin, D. (2018). Unsupervised feature learning via non-parametric instance discrimination. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> (pp. 3733-3742).</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.03748">Oord, A. V. D., Li, Y., &amp; Vinyals, O. (2018). Representation learning with contrastive predictive coding. <em>arXiv preprint arXiv:1807.03748</em>.</a></p>
<p><a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.html">He, K., Fan, H., Wu, Y., Xie, S., &amp; Girshick, R. (2020). Momentum contrast for unsupervised visual representation learning. In <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em> (pp. 9729-9738).</a></p>

    <style>.markmap-container{display:flex;justify-content:center;margin:0 auto;width:90%;height:500px}.markmap-container svg{width:100%;height:100%}@media(max-width:768px){.markmap-container{height:400px}}</style>
    <script src="https://cdn.jsdelivr.net/npm/d3@6"></script>
    <script src="https://cdn.jsdelivr.net/npm/markmap-view"></script>
    <script> document.querySelectorAll('.markmap-container>svg').forEach(mindmap => markmap.Markmap.create(mindmap, null, JSON.parse(mindmap.getAttribute('data'))))</script>
  
            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/deep-learning/">deep learning</a>
                    
                      <a class="hover-with-bg" href="/tags/paper-reading/">paper reading</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    This blog is licensed under a <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 licience</a>
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/04/30/Switch-blog-theme-to-FLUID/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Switch blog theme to FLUID</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/04/21/paper-reading-Vision-Transformer/">
                        <span class="hidden-mobile">paper reading: Vision Transformer</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;TOC</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://daydreamatnight.github.io/about/" target="_blank" rel="nofollow noopener"><span>Shoushou</span></a> <i class="iconfont icon-love"></i> <a href="https://daydreamatnight.github.io/about/" target="_blank" rel="nofollow noopener"><span>Rourou</span></a> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- 不蒜子统计PV -->
        <span id="busuanzi_container_site_pv" style="display: none">
            Toal views: 
            <span id="busuanzi_value_site_pv"></span>
             
          </span>
      
      
        <!-- 不蒜子统计UV -->
        <span id="busuanzi_container_site_uv" style="display: none">
            Total visiters: 
            <span id="busuanzi_value_site_uv"></span>
            
          </span>
      
    
  </div>


  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    
      <script  src="/js/img-lazyload.js" ></script>
    
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
        typing(title);
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          load: ['ui/lazy']
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" ></script>

  










  
<script src="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/DynamicLine.min.js"></script>



<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
