{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"node_modules/hexo-theme-fluid/source/css/gitalk.css","path":"css/gitalk.css","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/img/avatar.png","path":"img/avatar.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/img/default.png","path":"img/default.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/img/favicon.png","path":"img/favicon.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/img/loading.gif","path":"img/loading.gif","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/img/police_beian.png","path":"img/police_beian.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/js/boot.js","path":"js/boot.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/js/color-schema.js","path":"js/color-schema.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/js/events.js","path":"js/events.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/js/img-lazyload.js","path":"js/img-lazyload.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/js/leancloud.js","path":"js/leancloud.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/js/local-search.js","path":"js/local-search.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/js/plugins.js","path":"js/plugins.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/js/utils.js","path":"js/utils.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/xml/local-search.xml","path":"xml/local-search.xml","modified":1,"renderable":1},{"_id":"source/img/about_banner.avif","path":"img/about_banner.avif","modified":1,"renderable":0},{"_id":"source/img/about_banner.png","path":"img/about_banner.png","modified":1,"renderable":0},{"_id":"source/img/article_banner.avif","path":"img/article_banner.avif","modified":1,"renderable":0},{"_id":"source/img/article_banner.png","path":"img/article_banner.png","modified":1,"renderable":0},{"_id":"source/img/avatar.jpeg","path":"img/avatar.jpeg","modified":1,"renderable":0},{"_id":"source/img/bg_banner.avif","path":"img/bg_banner.avif","modified":1,"renderable":0},{"_id":"source/img/bg_banner.png","path":"img/bg_banner.png","modified":1,"renderable":0},{"_id":"source/img/favicon.ico","path":"img/favicon.ico","modified":1,"renderable":0}],"Cache":[{"_id":"source/about/index.md","hash":"a1a623920ad4e0a4bae0f972e6c59a72baa25baf","modified":1651400617533},{"_id":"source/CV/index.md","hash":"e35f1e6f8c6432418b4cf776341b707e578b907b","modified":1651400751098},{"_id":"source/about/.DS_Store","hash":"25197d3222c56ee90af9f5dfd76962f5d206e9b7","modified":1651400829159},{"_id":"source/.DS_Store","hash":"91712c3ad1c8535727a2ad83f325932106965e79","modified":1651400829161},{"_id":"source/img/about_banner.avif","hash":"3a4cf09849f67a526772893a30036ffb15531a25","modified":1651332230077},{"_id":"source/img/.DS_Store","hash":"de8cb38213cb80cd4fe24e6840732289f18b97be","modified":1651369278053},{"_id":"source/img/about_banner.png","hash":"7c0f697200eb7da68dbc321c508e57018b1223b1","modified":1651369183036},{"_id":"source/img/article_banner.avif","hash":"e307fb78545012ac5896ec812c8a128356ab0c35","modified":1651332121630},{"_id":"source/img/article_banner.png","hash":"a6efcc3ef2391694e8096017954381b4941ea5c7","modified":1651369198602},{"_id":"source/img/bg_banner.png","hash":"112921badbfec0909ce6406ea94cf08147383392","modified":1651369209968},{"_id":"source/img/favicon.ico","hash":"7e55bf11b25279bbe79d6f9790374a4427e92eb6","modified":1645623605171},{"_id":"source/img/bg_banner.avif","hash":"06980e9258fab6a2d68019df1665099c6639a660","modified":1651332307652},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia.md","hash":"94f5dff7892483df04b82cfee59c0ff00388bb49","modified":1651306516219},{"_id":"source/_posts/Intro-and-Pytorch-Implementation-of-Label-Smoothing-Regularization-LSR.md","hash":"65cd3286c36dd1c721d54d9fc2644dffe2015215","modified":1651347053482},{"_id":"source/img/avatar.jpeg","hash":"c6575ca36dc695bf8330d6253ebd38e9761a9462","modified":1645526272735},{"_id":"source/_posts/Switch-blog-theme-to-FLUID.md","hash":"cbdb8018970ce1d29386d5caea669136fc0db2da","modified":1651347093652},{"_id":"source/_posts/Hello-ShouRou.md","hash":"b6a1445773adf71a24edd6f486a395ae1ad635af","modified":1645669337294},{"_id":"source/_posts/paper-reading-A-gentle-introduction-to-graph-neural-networks.md","hash":"7417a16c47918fe42b93724fbe68ac71fe39b75b","modified":1651347059010},{"_id":"source/_posts/install-d2l-moudule-on-apple-m1-chip-for-deep-learning.md","hash":"b5c6fb110d98d2d30147eda4f03a6ea692c21477","modified":1651347048590},{"_id":"source/_posts/.DS_Store","hash":"ef2cfee6ec849ae20b23ba87e69cec829e27b9e8","modified":1651395448147},{"_id":"source/_posts/paper-reading-AlexNet.md","hash":"e4d613f0ffd75cc04ac85e0f5eb61d8f1bdf8dec","modified":1651347061661},{"_id":"source/_posts/paper-reading-MAE.md","hash":"e3a2eb737a7e9941a68ee3d2851f6875c4a56969","modified":1651347071529},{"_id":"source/_posts/paper-reading-ResNet.md","hash":"3546ca51b56801afaa23d6a389c4215e80437398","modified":1651347074476},{"_id":"source/_posts/learning-rate-schedule.md","hash":"c3357777c550c1487e28e9fbbde1aac3e9eac240","modified":1651347056281},{"_id":"source/_posts/paper-reading-Vision-Transformer.md","hash":"3d18de2b1860e29573c92ae1856a3f96963c2602","modified":1651347090325},{"_id":"source/_posts/paper-reading-bert.md","hash":"e90cf5cdf1053b34bbb626a6cb51fb93004b0da5","modified":1651347064409},{"_id":"source/_posts/paper-reading-GPT1-3.md","hash":"5dbf62d7060a275ce0e9aeb8c20d26601f53776a","modified":1651347068178},{"_id":"source/_posts/paper-reading-transformer.md","hash":"a4ce0cfe185cc2cc38555396c46592048c26dd51","modified":1651347086294},{"_id":"source/_posts/paper-reading-start.md","hash":"09e4767538f32b74668ba42343f5fac4277707c7","modified":1651347081778},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/Google sitemap inspect 3.png","hash":"091b41b10a1cc1c91cb81f48b062d32bc57b79d5","modified":1646148166525},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/bing sitemap.png","hash":"f8ca2e88f4c6e1969395b7617075010a4a697565","modified":1646148166528},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/bing search console success.png","hash":"03acd2a59a1e8af3276f36039a235c44afa5a218","modified":1646148166528},{"_id":"source/_posts/Intro-and-Pytorch-Implementation-of-Label-Smoothing-Regularization-LSR/Cross entropy loss function.png","hash":"f39593d083f6b6a97eb20780a4be745e19dd0452","modified":1646740133650},{"_id":"source/_posts/learning-rate-schedule/2d cyclic learning rate schedule.png","hash":"c8123233438e88551c7453ecd19706baa436bc4a","modified":1646740133663},{"_id":"source/_posts/paper-reading-MAE/.DS_Store","hash":"53726100b46699d968c3f266745655dc85f278b6","modified":1651247570642},{"_id":"source/_posts/paper-reading-GPT1-3/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1650507038805},{"_id":"source/_posts/paper-reading-GPT1-3/GPT fine tune f1.png","hash":"f5d63e0fdf6eb56aaa20a2da422bd0ef3b7dbfba","modified":1650374963267},{"_id":"source/_posts/paper-reading-GPT1-3/GPT loss.png","hash":"9b87e4122b93191ebd69aca5195f5d216cab968d","modified":1650358114793},{"_id":"source/_posts/paper-reading-GPT1-3/GPT fine tune f2.png","hash":"5b0c974e64cede1083a266d5149d53ee972208f8","modified":1650375181726},{"_id":"source/_posts/paper-reading-GPT1-3/GPT fine tune loss.png","hash":"85d0003ae86d00ffd5b8c8ef719f028dac417394","modified":1650375292584},{"_id":"source/_posts/paper-reading-Vision-Transformer/.DS_Store","hash":"a6838ca67d58c3de9944aa3aa2cc2795e16e09d5","modified":1650549185032},{"_id":"source/_posts/paper-reading-bert/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1650358481672},{"_id":"source/_posts/paper-reading-transformer/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1649899178188},{"_id":"source/_posts/paper-reading-bert/model size graph.webp","hash":"faed538a605818e2cef417ed29692bf18745daf5","modified":1650268807051},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/baidu console sitemap.png","hash":"8ecba7e34a53e980586ae8ab154511d951867aee","modified":1645610381532},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/being sitemap connect google.png","hash":"2f7067940f7d336d1c87987c26562bb87b4f90bf","modified":1646148166528},{"_id":"source/_posts/Intro-and-Pytorch-Implementation-of-Label-Smoothing-Regularization-LSR/accuracy curve applying label smoothing.png","hash":"4aeeb4d6d7711a4396e2149e4d1f8130d183edcc","modified":1646740133652},{"_id":"source/_posts/learning-rate-schedule/SGD with learning rate decay.png","hash":"83c473cfe0a99facc1b1ab54af01115989e11b0c","modified":1646740133663},{"_id":"source/_posts/learning-rate-schedule/source ppt.pptx","hash":"1cd5b8c2dcc76d002d10173f1c0071bf331898e4","modified":1646837491091},{"_id":"source/_posts/paper-reading-GPT1-3/GPT timeline.png","hash":"109115b9d17368fd43c4c536fe50449deec95fcf","modified":1650290802211},{"_id":"source/_posts/paper-reading-Vision-Transformer/ViT variants.png","hash":"d4f9886bdfcfd28b0c84ca95cfef3b6982acfa05","modified":1650951107015},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/can't fetch sitemap.png","hash":"f33cf775460666ebefa5e8aa56b768a1ce4d1af4","modified":1645608343832},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/google console varification.png","hash":"54b854a7bc2152a565ae61ab6f513fe8fad31139","modified":1645606581799},{"_id":"source/_posts/learning-rate-schedule/warmup on large batches.png","hash":"c7c61a90faa6fbe7cbb58f35c651cfacc5357db8","modified":1646740133665},{"_id":"source/_posts/paper-reading-ResNet/ResNet figure 2.png","hash":"4aae59eb537b2676e973e61b714f5e2260592443","modified":1649660775712},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/baidu console.png","hash":"1b21c2b62d1217fc07402a2b5a93ade16b9d06a9","modified":1645608661819},{"_id":"source/_posts/paper-reading-bert/BERT learnable paramters.png","hash":"8839786a937ceb1f0a58110f5dc72b06a48024b8","modified":1650129616220},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/check google search.png","hash":"a283f4c31f86a0f29ffc685c572431a00e5199c7","modified":1645604738508},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/typora setting.png","hash":"4bbaf27946caa82912470a69221a7f494b985bf9","modified":1645586028925},{"_id":"source/_posts/Intro-and-Pytorch-Implementation-of-Label-Smoothing-Regularization-LSR/accuracy curve compare label smoothing with hard label.png","hash":"cdae6eacdd69d900057bf680a2d321e58dda17d9","modified":1646740133653},{"_id":"source/_posts/learning-rate-schedule/CLR.png","hash":"e7bbe3fb1a452bf830806ae0dba4ec0eb6ae236b","modified":1646837491081},{"_id":"source/_posts/learning-rate-schedule/SGDR.png","hash":"ef63d3bb9bf8b0083de55ef9b3c9fc4a0d57608d","modified":1646837491082},{"_id":"source/_posts/paper-reading-A-gentle-introduction-to-graph-neural-networks/GNN graph level task.png","hash":"e1af92d12c8f7e0b9943f7ed839198b03088a7e4","modified":1649922057273},{"_id":"source/_posts/paper-reading-GPT1-3/GPT pretraining process.png","hash":"a634a1d5a49fa244f8d0cdcb6c982f4c9289d563","modified":1650358856100},{"_id":"source/_posts/paper-reading-GPT1-3/GPT3 models.png","hash":"93adbab90cb1e999d9a6969ebfea7b1e82672d68","modified":1650507028352},{"_id":"source/_posts/paper-reading-GPT1-3/GPT-3 result 2.png","hash":"646a38e14e8bef9fd0f5615acf0cbd43c5042205","modified":1650542544491},{"_id":"source/_posts/paper-reading-GPT1-3/GPT3 race.png","hash":"3ceb1c3a74634b2c40b608e3c6a9529a141907f1","modified":1650544334100},{"_id":"source/_posts/paper-reading-Vision-Transformer/VIT algorithm.png","hash":"49c6702a90c2be602b51765d7e9824f5aa722522","modified":1650940497282},{"_id":"source/_posts/paper-reading-ResNet/ResNet figure 5.png","hash":"535021caab08b1436c194688c6f15cbbce199308","modified":1649691635486},{"_id":"source/_posts/paper-reading-Vision-Transformer/VIT positional embedding ablation.png","hash":"774bb656e0f545459f58b0b0b521c1883fe5df01","modified":1650940248041},{"_id":"source/_posts/paper-reading-Vision-Transformer/VIT model overview.png","hash":"25618195462760c8e7bcccf86e02e43cff1ee0c9","modified":1650701858569},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/google console.png","hash":"240520671ae14a4db700b002e99ee276bf1fe650","modified":1645605064121},{"_id":"source/_posts/Intro-and-Pytorch-Implementation-of-Label-Smoothing-Regularization-LSR/Label smoothing feature norm.png","hash":"47841e9b48ef8aaa61bdedf70b593a1d563840ca","modified":1646740133651},{"_id":"source/_posts/Intro-and-Pytorch-Implementation-of-Label-Smoothing-Regularization-LSR/leave class code.png","hash":"224786316ca7dc910a29d3c5acb42f755f3bc662","modified":1646740133655},{"_id":"source/_posts/learning-rate-schedule/cyclic learning rate schedule.png","hash":"0c093af7caf829e60f29eea8f72140ba75a85ba3","modified":1646740133664},{"_id":"source/_posts/paper-reading-AlexNet/unsupervise learning cake.png","hash":"adcb79d097ff6fdea2021bb4668f8eeb8a57358d","modified":1649349696051},{"_id":"source/_posts/paper-reading-MAE/MAE mask ratio.png","hash":"75bc0ff014fb48e2e3adff8f2a64298c498ba98d","modified":1651300996771},{"_id":"source/_posts/paper-reading-GPT1-3/GPT3 gender.png","hash":"49460dd410dcef903e806afb83aeaa82013049cf","modified":1650544313625},{"_id":"source/_posts/paper-reading-GPT1-3/GPT3 training data.png","hash":"d8c11af3e71e7ce5d5aa4cd47e6836d816f04a5b","modified":1650541614411},{"_id":"source/_posts/Intro-and-Pytorch-Implementation-of-Label-Smoothing-Regularization-LSR/hard label.png","hash":"8e83c8d43703867f9faf7a1db317fcb040a24045","modified":1646740133654},{"_id":"source/_posts/Intro-and-Pytorch-Implementation-of-Label-Smoothing-Regularization-LSR/soft label.png","hash":"2d6357cd03c4b4149f5191e91d781cc5a554a897","modified":1646740133657},{"_id":"source/_posts/paper-reading-GPT1-3/GPT-3 approach.png","hash":"21e4e6703fcf07679e8cb6cf42931b444145426b","modified":1650504600314},{"_id":"source/_posts/paper-reading-GPT1-3/GPT3 result 3.png","hash":"9360c9a0adb1051e96ff62e6273e4ad747297b6b","modified":1650542713778},{"_id":"source/_posts/paper-reading-GPT1-3/GPT3 result.png","hash":"3c4192ef7ff33164dba01b3ea9754c3b02011bda","modified":1650542377401},{"_id":"source/_posts/paper-reading-Vision-Transformer/ViT results.png","hash":"5506ce41ac7b84fc8c10ce73c980d169531145ad","modified":1650952271263},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/Last snapshot.png","hash":"9ccbadb43f29a86f56d4820913b7daaa8d31e211","modified":1651306179573},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/baidu console varification.png","hash":"09c2e3d1f3b52807bf6dd97d8ec42f7cfd1fe1c7","modified":1645609658080},{"_id":"source/_posts/paper-reading-transformer/transformer.png","hash":"6eb38ed16fbaf3660d18b4266763411eadc5a452","modified":1649857283101},{"_id":"source/_posts/learning-rate-schedule/SGDR_REsult.png","hash":"f118bc7b7c5ea51afa9ae366f6bce17757963467","modified":1646837491084},{"_id":"source/_posts/paper-reading-A-gentle-introduction-to-graph-neural-networks/GNN interative archtecture.png","hash":"06e8477aee1a6f12385895a4151dea90ca23e382","modified":1649916716129},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_tag/tag.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1651305803051},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_category/category.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1651305803039},{"_id":"source/_posts/paper-reading-GPT1-3/GPT3 performance with compute.png","hash":"e39d32f750f4853f15783654367e544c0bd70d50","modified":1650542001278},{"_id":"source/_posts/paper-reading-GPT1-3/GPT3 result4.png","hash":"c261a581c8c8f50e1d9ef50b576415c5116e3467","modified":1650542693426},{"_id":"source/_posts/paper-reading-ResNet/ResNet table 1.png","hash":"782d634274c1221c9db07ea5c9367c4fef5fbe4b","modified":1649692137505},{"_id":"source/_posts/paper-reading-bert/BERT alibation study 1.png","hash":"f71b28cdf11f80dcf3fc5b8c89ad2624d6caad1c","modified":1650266735874},{"_id":"source/_posts/paper-reading-transformer/transformer table1.png","hash":"637292c4c8f161ffa35d2b41ebcf1a37019638f3","modified":1649873535743},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/github page.png","hash":"5262aa8e506f60a57b7ddc69a76c860bca21ffce","modified":1645583716321},{"_id":"node_modules/hexo-theme-fluid/.editorconfig","hash":"33218fbd623feb43edf5f99f15965392cecc44a6","modified":1651305800213},{"_id":"node_modules/hexo-theme-fluid/.gitattributes","hash":"a54f902957d49356376b59287b894b1a3d7a003f","modified":1651305800213},{"_id":"node_modules/hexo-theme-fluid/.eslintrc","hash":"4bc2b19ce2b8c4d242f97d4ccf2d741e68ab0097","modified":1651305800213},{"_id":"node_modules/hexo-theme-fluid/LICENSE","hash":"df5b54be535593d5442cebafbea34eb9bd69b987","modified":1651305800214},{"_id":"node_modules/hexo-theme-fluid/package.json","hash":"4e3992cacd64c6888218b0346283258e8f82a27d","modified":1651305803028},{"_id":"node_modules/hexo-theme-fluid/_config.yml","hash":"8a7f1534c228538e2ab56249d5a65829650170ed","modified":1651305803053},{"_id":"node_modules/hexo-theme-fluid/languages/de.yml","hash":"13a6a799415fc2f6f69ebd1a399fb44426a5d641","modified":1651305803053},{"_id":"node_modules/hexo-theme-fluid/README_en.md","hash":"ca8fd19a4948de1f253616a62c0e8a7d81f692f5","modified":1651305803030},{"_id":"node_modules/hexo-theme-fluid/languages/eo.yml","hash":"a0c7984495d4f2d33b64adfa33adebbf768a5ac3","modified":1651305803054},{"_id":"node_modules/hexo-theme-fluid/languages/ja.yml","hash":"91020031a847c0361a6fd7ab990c7be4bf17529b","modified":1651305803054},{"_id":"node_modules/hexo-theme-fluid/layout/archive.ejs","hash":"472d0813ca5b88000a7bc6039f33b7e27b5a3216","modified":1651305800815},{"_id":"node_modules/hexo-theme-fluid/languages/zh-CN.yml","hash":"21307b4137c3d9b04bb58243747e75af0abc5a71","modified":1651305803054},{"_id":"node_modules/hexo-theme-fluid/languages/zh-TW.yml","hash":"1a6d415446da11dee5c5f400e7d67544fbe743ea","modified":1651305803055},{"_id":"node_modules/hexo-theme-fluid/layout/categories.ejs","hash":"20c2a195a109d2a263b5fa6e79cbcc62932508ad","modified":1651305800817},{"_id":"node_modules/hexo-theme-fluid/layout/category.ejs","hash":"58291dfec65c36889dfce0ddc603540b67e4c598","modified":1651305800817},{"_id":"node_modules/hexo-theme-fluid/README.md","hash":"03cfa8e5f149514b57ef80dcb84eb7fea261370d","modified":1651305803030},{"_id":"node_modules/hexo-theme-fluid/layout/about.ejs","hash":"ad6fed7b646d3ca961db83db0fbe020e3a5d42ad","modified":1651305800215},{"_id":"node_modules/hexo-theme-fluid/layout/404.ejs","hash":"689d9f4efd2a7f5edfd9b24561a7ade69d46617c","modified":1651305800215},{"_id":"node_modules/hexo-theme-fluid/layout/layout.ejs","hash":"7f566edf750241e62d7c54abfbb0c504fdab850a","modified":1651305800827},{"_id":"node_modules/hexo-theme-fluid/layout/index.ejs","hash":"f3ae4395e751c4a02d5895e07856b1e8edfdda08","modified":1651305800827},{"_id":"node_modules/hexo-theme-fluid/layout/links.ejs","hash":"2a7b49f0f9aecf07550b5a0b99242aab5654cf2b","modified":1651305800828},{"_id":"node_modules/hexo-theme-fluid/languages/en.yml","hash":"a85dcc5cc21f9cab50df31e5001b8818ee62d1e2","modified":1651305803054},{"_id":"node_modules/hexo-theme-fluid/layout/page.ejs","hash":"1014b901d396f4fc445cb1ffc938d5380d894d71","modified":1651305802994},{"_id":"node_modules/hexo-theme-fluid/layout/tags.ejs","hash":"1d06af34b6cf1d8a20d2eb565e309326ceba309f","modified":1651305802999},{"_id":"node_modules/hexo-theme-fluid/.github/ISSUE_TEMPLATE/bug_report.md","hash":"16d33eb89ecf90f4046720fde5395d972c7ba1fd","modified":1651305803030},{"_id":"node_modules/hexo-theme-fluid/.github/ISSUE_TEMPLATE/feature_request.md","hash":"c134dd57ffd269b93402ccfffe7dbe0f0b583bec","modified":1651305803030},{"_id":"node_modules/hexo-theme-fluid/layout/post.ejs","hash":"79e3679a7069351a6172c281b9d09f59d7580484","modified":1651305802995},{"_id":"node_modules/hexo-theme-fluid/.github/ISSUE_TEMPLATE/feature_request_zh.md","hash":"ed08574b196447376dd74411cca664ac9227a5d4","modified":1651305803030},{"_id":"node_modules/hexo-theme-fluid/.github/ISSUE_TEMPLATE/question_zh.md","hash":"e24b470f7aa8044499a4f5e39634e5dc43899011","modified":1651305803030},{"_id":"node_modules/hexo-theme-fluid/.github/workflows/limit.yaml","hash":"f8bd2edeb4424ee7a055b31583445d5d5dff91a4","modified":1651305803052},{"_id":"node_modules/hexo-theme-fluid/layout/tag.ejs","hash":"0ad89eb7c92a822980fa9a85285e6d94ad845d1d","modified":1651305802999},{"_id":"node_modules/hexo-theme-fluid/.github/ISSUE_TEMPLATE/question.md","hash":"ab5eab9e3ff889c4ba7fd82846e7f5b7ae15bebc","modified":1651305803030},{"_id":"node_modules/hexo-theme-fluid/layout/_partial/archive-list.ejs","hash":"53a4f6029373a40394a87aba9284696a71610f90","modified":1651305800815},{"_id":"node_modules/hexo-theme-fluid/layout/_partial/beian.ejs","hash":"58b4bbe36386de4305a8da5ffd7d56802df23049","modified":1651305800816},{"_id":"node_modules/hexo-theme-fluid/layout/_partial/footer.ejs","hash":"a62278c38a310da495d96c39abacacef266945cb","modified":1651305800820},{"_id":"node_modules/hexo-theme-fluid/.github/ISSUE_TEMPLATE/bug_report_zh.md","hash":"af977ed0792508bb0766ea8afe82d34ef1e8fb3c","modified":1651305803029},{"_id":"node_modules/hexo-theme-fluid/layout/_partial/head.ejs","hash":"f7f3494ed001e8cdcdc3e8a1d2cd1195cff2ded5","modified":1651305800822},{"_id":"node_modules/hexo-theme-fluid/layout/_partial/css.ejs","hash":"c363829a4b80f74fc1c565e41f6dab41c95006ea","modified":1651305800818},{"_id":"node_modules/hexo-theme-fluid/layout/_partial/paginator.ejs","hash":"0f38a2c238169edcb63fc46c23bfc529ff3859b7","modified":1651305802994},{"_id":"node_modules/hexo-theme-fluid/layout/_partial/post-meta.ejs","hash":"32a17edadeed40da6db21d2d8031bd47d2fc9bf4","modified":1651305802994},{"_id":"node_modules/hexo-theme-fluid/scripts/events/index.js","hash":"44faef3e77ab08b91e4c5c6f1cd9087a9faff443","modified":1651305803022},{"_id":"node_modules/hexo-theme-fluid/layout/_partial/scripts.ejs","hash":"0ee838b6fcd895d21a29d9d67dbb99b752d623d1","modified":1651305802996},{"_id":"node_modules/hexo-theme-fluid/layout/_partial/search.ejs","hash":"cdd7919fa01f6ef7ccc09938d662ff3d77f5d999","modified":1651305802996},{"_id":"node_modules/hexo-theme-fluid/scripts/filters/locals.js","hash":"58d0fec976f6b1d35e7ea03edc45414088acf05c","modified":1651305803026},{"_id":"node_modules/hexo-theme-fluid/scripts/filters/post-filter.js","hash":"6c37e9f1ac1d6d00b3c32794e02e244dba942cd9","modified":1651305803028},{"_id":"node_modules/hexo-theme-fluid/scripts/generators/pages.js","hash":"d9971f15fbb6b775e3d31a1b9b45011959395010","modified":1651305803027},{"_id":"node_modules/hexo-theme-fluid/layout/_partial/statistics.ejs","hash":"920bc618d357d48d2b96f8758f6ae8f9488fc4d8","modified":1651305802997},{"_id":"node_modules/hexo-theme-fluid/scripts/generators/local-search.js","hash":"fc2c50405b771b06b7f6cfc4e9de97b992691555","modified":1651305803023},{"_id":"node_modules/hexo-theme-fluid/scripts/helpers/export-config.js","hash":"cde964c8cd3217268a231de5e018a62c53c2e047","modified":1651305803019},{"_id":"node_modules/hexo-theme-fluid/scripts/helpers/utils.js","hash":"3aa5b4ea879cd34d3a32468d88da18d72cbcc8e0","modified":1651305803028},{"_id":"node_modules/hexo-theme-fluid/scripts/helpers/url.js","hash":"2a6a8288176d0e0f6ec008056bf2745a86e8943e","modified":1651305803028},{"_id":"node_modules/hexo-theme-fluid/scripts/tags/button.js","hash":"3eb43a8cdea0a64576ad6b31b4df6c2bf5698d4c","modified":1651305803015},{"_id":"node_modules/hexo-theme-fluid/scripts/helpers/wordcount.js","hash":"b917b893b1777e6ffcb53188f9f5644510e5f20d","modified":1651305803028},{"_id":"node_modules/hexo-theme-fluid/scripts/tags/group-image.js","hash":"4aeebb797026f1df25646a5d69f7fde79b1bcd26","modified":1651305803020},{"_id":"node_modules/hexo-theme-fluid/layout/_partial/toc.ejs","hash":"3d2fb5552f373e5a0c56bc356702d807bcbcb411","modified":1651305802999},{"_id":"node_modules/hexo-theme-fluid/layout/_partial/nav.ejs","hash":"e71b3c4aa263163597d31b1f91e5a1a877084cfd","modified":1651305802990},{"_id":"node_modules/hexo-theme-fluid/scripts/tags/note.js","hash":"f52f3a005b41f48b4da274ac64710177c8d4502f","modified":1651305803026},{"_id":"node_modules/hexo-theme-fluid/scripts/tags/checkbox.js","hash":"63468f7875c09d9557fe8315afc97175745d9087","modified":1651305803016},{"_id":"node_modules/hexo-theme-fluid/scripts/tags/mermaid.js","hash":"75160561e1ef3603b6d2ad2938464ab1cb77fd38","modified":1651305803026},{"_id":"node_modules/hexo-theme-fluid/scripts/utils/compare-versions.js","hash":"dbbc928c914fc2bd242cd66aa0c45971aec13a5d","modified":1651305803017},{"_id":"node_modules/hexo-theme-fluid/scripts/helpers/page.js","hash":"4607607445233b3029ef20ed5e91de0da0a7f9c5","modified":1651305803027},{"_id":"node_modules/hexo-theme-fluid/scripts/utils/url-join.js","hash":"718aab5e7b2059a06b093ca738de420d9afa44ba","modified":1651305803028},{"_id":"node_modules/hexo-theme-fluid/scripts/utils/object.js","hash":"649457796374c79e49a19bd541e4ad8e78fe8995","modified":1651305803026},{"_id":"node_modules/hexo-theme-fluid/source/img/default.png","hash":"7bb2b8ee07db305bcadee2985b81b942027ae940","modified":1651305803033},{"_id":"node_modules/hexo-theme-fluid/source/img/favicon.png","hash":"64b215db2cb3af98fe639e94537cb5209f959c78","modified":1651305803033},{"_id":"node_modules/hexo-theme-fluid/source/css/gitalk.css","hash":"a57b3cc8e04a0a4a27aefa07facf5b5e7bca0e76","modified":1651305800214},{"_id":"node_modules/hexo-theme-fluid/source/img/police_beian.png","hash":"90efded6baa2dde599a9d6b1387973e8e64923ea","modified":1651305803034},{"_id":"node_modules/hexo-theme-fluid/source/js/boot.js","hash":"3de344ee619da989f6dccf7c2ae459fe91075983","modified":1651305803015},{"_id":"node_modules/hexo-theme-fluid/source/js/color-schema.js","hash":"f1fdd512f3ef92ff5db4a49f5a3143d5ddea9858","modified":1651305803016},{"_id":"node_modules/hexo-theme-fluid/source/img/avatar.png","hash":"fe739a158cc128f70f780eb5fa96f388b81d478f","modified":1651305803031},{"_id":"node_modules/hexo-theme-fluid/source/js/events.js","hash":"4b9d2676c9544db9cc40a8c7d18456792299ba86","modified":1651305803019},{"_id":"node_modules/hexo-theme-fluid/source/css/main.styl","hash":"d5a8a59c8d1fd17d699a951e59c4ce9ae44c419d","modified":1651305803045},{"_id":"node_modules/hexo-theme-fluid/source/js/img-lazyload.js","hash":"cbdeca434ec4da51f488c821d51b4d23c73294af","modified":1651305803021},{"_id":"node_modules/hexo-theme-fluid/source/img/loading.gif","hash":"2d2fc0f947940f98c21afafef39ecf226a2e8d55","modified":1651305803011},{"_id":"node_modules/hexo-theme-fluid/source/xml/local-search.xml","hash":"8c96ba6a064705602ce28d096fd7dd9069630a55","modified":1651305803052},{"_id":"node_modules/hexo-theme-fluid/layout/_partial/comments/gitalk.ejs","hash":"843bc141a4545eb20d1c92fb63c85d459b4271ec","modified":1651305800821},{"_id":"node_modules/hexo-theme-fluid/source/js/leancloud.js","hash":"53987b7a668ea0623370eb83ed5311766221b557","modified":1651305803022},{"_id":"node_modules/hexo-theme-fluid/layout/_partial/comments/disqus.ejs","hash":"aab4a4d24c55231a37db308ae94414319cecdd9b","modified":1651305800820},{"_id":"node_modules/hexo-theme-fluid/source/js/plugins.js","hash":"d058f30bd09b28769c4d8313428ff23dfc8d52dd","modified":1651305803027},{"_id":"node_modules/hexo-theme-fluid/layout/_partial/comments/livere.ejs","hash":"2264758fed57542a7389c7aa9f00f1aefa17eb87","modified":1651305800828},{"_id":"node_modules/hexo-theme-fluid/source/js/utils.js","hash":"4a43f2700e91937650bef511fd438825b001c4c6","modified":1651305803028},{"_id":"node_modules/hexo-theme-fluid/layout/_partial/comments/remark42.ejs","hash":"d4e9532feeb02aed61bd15eda536b5b631454dac","modified":1651305802996},{"_id":"node_modules/hexo-theme-fluid/source/js/local-search.js","hash":"633f0142c657805359b0197f287e12ae4bcde731","modified":1651305803024},{"_id":"node_modules/hexo-theme-fluid/layout/_partial/comments/utterances.ejs","hash":"e1ed6530dfd7310f91060a75766a93ac3c39be3a","modified":1651305803001},{"_id":"node_modules/hexo-theme-fluid/layout/_partial/comments/twikoo.ejs","hash":"1af53bc0be642610a3a4d4e7c05287854a821508","modified":1651305803000},{"_id":"node_modules/hexo-theme-fluid/scripts/tags/label.js","hash":"f05a6d32cca79535b22907dc03edb9d3fa2d8176","modified":1651305803022},{"_id":"node_modules/hexo-theme-fluid/layout/_partial/comments/waline.ejs","hash":"21e00443054802e893aac1f668b69d5bb4b39b3a","modified":1651305803001},{"_id":"node_modules/hexo-theme-fluid/layout/_partial/comments/cusdis.ejs","hash":"5f9dc012be27040bbe874d0c093c0d53958cc987","modified":1651305800819},{"_id":"node_modules/hexo-theme-fluid/scripts/events/lib/compatible-configs.js","hash":"b5fd5a2d9c463eb59318af0f47c591c485b6ad27","modified":1651305803018},{"_id":"node_modules/hexo-theme-fluid/scripts/events/lib/footnote.js","hash":"3b2abc5f5e3b681874637e98e047dc4969eb1983","modified":1651305803020},{"_id":"node_modules/hexo-theme-fluid/scripts/events/lib/highlight.js","hash":"fd5fcb6a61ad865197a778eeae889b80484227dd","modified":1651305803021},{"_id":"node_modules/hexo-theme-fluid/scripts/events/lib/hello.js","hash":"28e186c32576eb3d5d923273471a001c47fe8071","modified":1651305803021},{"_id":"node_modules/hexo-theme-fluid/layout/_partial/comments/changyan.ejs","hash":"c9b2d68ed3d375f1953e7007307d2a3f75ed6249","modified":1651305800818},{"_id":"node_modules/hexo-theme-fluid/layout/_partial/plugins/mermaid.ejs","hash":"10ed1f9a611449d37736e17c4e251127b38b3772","modified":1651305802990},{"_id":"node_modules/hexo-theme-fluid/layout/_partial/comments/valine.ejs","hash":"4052ab2a8f78efa92f0fe17abe8f66135943390a","modified":1651305803001},{"_id":"node_modules/hexo-theme-fluid/scripts/events/lib/lazyload.js","hash":"9ba0d4bc224e22af8a5a48d6ff13e5a0fcfee2a4","modified":1651305803022},{"_id":"node_modules/hexo-theme-fluid/layout/_partial/plugins/math.ejs","hash":"df6941bd3b860180d01fd39ee859ed2d42f4d1f0","modified":1651305802985},{"_id":"node_modules/hexo-theme-fluid/layout/_partial/plugins/typed.ejs","hash":"e8e01c5db46b383748855452aecd70fcda99f598","modified":1651305803000},{"_id":"node_modules/hexo-theme-fluid/layout/_partial/plugins/nprogress.ejs","hash":"4c2d39ce816b8a6dcd6b53113c8695f8bd650a23","modified":1651305802993},{"_id":"node_modules/hexo-theme-fluid/source/css/_functions/base.styl","hash":"2e46f3f4e2c9fe34c1ff1c598738fc7349ae8188","modified":1651305803036},{"_id":"node_modules/hexo-theme-fluid/source/css/_mixins/base.styl","hash":"542e306ee9494e8a78e44d6d7d409605d94caeb3","modified":1651305803036},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/pages.styl","hash":"b8e887bc7fb3b765a1f8ec9448eff8603a41984f","modified":1651305803049},{"_id":"node_modules/hexo-theme-fluid/source/css/_variables/base.styl","hash":"08b455b848b21d57e0563b87071c4bae2b63bafe","modified":1651305803037},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_about/about.styl","hash":"97fe42516ea531fdad771489b68aa8b2a7f6ae46","modified":1651305803034},{"_id":"node_modules/hexo-theme-fluid/scripts/events/lib/merge-configs.js","hash":"c1db1a4f9eca6e36b660530641e3a4fb6a30c8d8","modified":1651305803026},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_archive/archive.styl","hash":"6e6f22b664199772370b59ce1678b0c148b5849f","modified":1651305803035},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/base.styl","hash":"c2d8bfd04bf0734b387c049402b46a06a05fc582","modified":1651305803036},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/keyframes.styl","hash":"94065ea50f5bef7566d184f2422f6ac20866ba22","modified":1651305803044},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_category/categories.styl","hash":"1ab7db37c2f7dc7ccdb994dcb41c16a4c8920397","modified":1651305803039},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_links/links.styl","hash":"7e32a3268accf3d524209c213e15e2d5d5e2e1a6","modified":1651305803045},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_post/tag_plugin.styl","hash":"b89b96c8a6a433a6f372b42710554b05cab85a24","modified":1651305803051},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/color-schema.styl","hash":"80098e8354069631bde8edcd1181a53091a92949","modified":1651305803041},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/rewrite.styl","hash":"a2993f23701de9a83e3f428300e62c5c52b4ff4b","modified":1651305803051},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_post/post.styl","hash":"5e86487de0f16c30ca3e16460ab94b57620e31c5","modified":1651305803050},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_tag/tags.styl","hash":"65bfc01c76abc927fa1a23bf2422892b0d566c3f","modified":1651305803051},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/banner.styl","hash":"30f8fab95a5214d79df0ccc02b937df8bd885676","modified":1651305803035},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/board.styl","hash":"32d90bcc8bf2fd5d8d78e86a567973d4b69bcfa1","modified":1651305803037},{"_id":"node_modules/hexo-theme-fluid/layout/_partial/plugins/analytics.ejs","hash":"557077a8825fffc0a2c7fe2b29f319287950244f","modified":1651305800805},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/inline.styl","hash":"d547ab0b91f84eb0acd0bc0c5d716ce17c30361a","modified":1651305803044},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_index/index.styl","hash":"616c1f7147078c3d532dd1cfd2af09c0c3a816f0","modified":1651305803044},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/copy-btn.styl","hash":"9f932ca3f9625c13aa5353f58319881e62c0c653","modified":1651305803041},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/header.styl","hash":"d8011325756eb6e4ce619b3e7b4d6d80c2de8a57","modified":1651305803043},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/qrcode.styl","hash":"78704a94c0436097abfb0e0a57abeb3429c749b7","modified":1651305803051},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/footnote.styl","hash":"ae9289cc89649af2042907f8a003303b987f3404","modified":1651305803042},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/footer.styl","hash":"0ce7552dc4993926426019398d73e817cfd841a1","modified":1651305803042},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/search.styl","hash":"10f7e91a91e681fb9fe46f9df7707b9ef78707c8","modified":1651305803051},{"_id":"source/_posts/paper-reading-GPT1-3/GPT objectives.png","hash":"34aa333335dbb1d0292b65258750ac163a03df2f","modified":1650361267165},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/scroll-btn.styl","hash":"55e10a6965462f8f62f85e75fd5e143af02a4b44","modified":1651305803051},{"_id":"source/_posts/paper-reading-transformer/layer norm.png","hash":"9e0a63ba884eb2efa0604a74dedb92d0cad7420b","modified":1649859296148},{"_id":"source/_posts/paper-reading-GPT1-3/BERT result with GPT.png","hash":"a922a5d9ec6bb140b431baa56ec969dc9d019395","modified":1650373672293},{"_id":"source/_posts/paper-reading-GPT1-3/GPT2 performance.png","hash":"ed244f4af159e064c2862a8933cd891754a17b7d","modified":1650389129294},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/Google sitemap inspect.png","hash":"90618421c6cab7c23ef65da92bc0c48090bc93ea","modified":1646148166528},{"_id":"source/_posts/paper-reading-A-gentle-introduction-to-graph-neural-networks/GNN graph message passing.png","hash":"46c17f41a8ff34811854ff5c913693d167933833","modified":1649941190066},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/node js install.png","hash":"49b580f0049a4e224ea17cbb30a33dc34e8acc3a","modified":1645586237414},{"_id":"source/_posts/paper-reading-Vision-Transformer/VIT class token ablation.png","hash":"5c6b482cd85546e81183f7c6090cb110791f8dd4","modified":1650939933644},{"_id":"source/_posts/Intro-and-Pytorch-Implementation-of-Label-Smoothing-Regularization-LSR/source ppt.pptx","hash":"4e53421cbb49964ea368ac4c9259914407a20749","modified":1646740133660},{"_id":"source/_posts/paper-reading-GPT1-3/GPT 3 result 5.png","hash":"5bb6e24fd0d52e4153e80828ee9c250d15465c7b","modified":1650542927000},{"_id":"source/_posts/paper-reading-ResNet/ResNet table 3.png","hash":"62e26b0634b37d7dc652e5b301937fabd7ba69dd","modified":1649691518798},{"_id":"source/_posts/paper-reading-ResNet/ResNet table 6.png","hash":"61fd7fcd8b823045cedbf6be8a3c91582124ee6f","modified":1649765185869},{"_id":"source/_posts/paper-reading-bert/BERT segment embedding.png","hash":"b3916ee35fc24b258adaf99313b3358e0de35510","modified":1650207033624},{"_id":"source/_posts/paper-reading-MAE/MAE architecture.png","hash":"bd19b6cb51c6edf1b32eb6ed2ebf9423539e2cc6","modified":1651155594707},{"_id":"source/_posts/paper-reading-Vision-Transformer/ViT inspecting.png","hash":"3da9c71e7aaa955e3ff2ffe75c03fad55ff74004","modified":1650988729775},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/Google sitemap inspect 2.png","hash":"acaf3254cd16099f517b0f1e1ab58aa89552e6cd","modified":1646148166525},{"_id":"source/_posts/paper-reading-ResNet/ResNet figure 4.png","hash":"b3e1f256838b4036e47d0584a8f84f8a7cd7be08","modified":1649658958011},{"_id":"source/_posts/paper-reading-GPT1-3/GPT3 performance.png","hash":"3d82eb6c71f15523cb51c14bf33573debdd38717","modified":1650439746354},{"_id":"source/_posts/paper-reading-Vision-Transformer/CNN first layer filters.png","hash":"b33e246344bda2b7c32f402c7724b6ae07889114","modified":1650988099709},{"_id":"source/_posts/paper-reading-ResNet/ResNet figure 1.png","hash":"765c606c3ae4bafcdd03dba4cda928f782204576","modified":1649658733804},{"_id":"source/_posts/paper-reading-transformer/transformer table 3.png","hash":"f0c745ce7dbb8cbad5628f1a1b7f4ab207a8ebd7","modified":1649874432197},{"_id":"source/_posts/paper-reading-Vision-Transformer/VIT properties.png","hash":"f394d65267154e4aacb9329b6a1cd257a3165ddb","modified":1650549232037},{"_id":"source/_posts/paper-reading-MAE/MAE result2.png","hash":"65a47208b3ab66a13c5925ce51a1f03906c126c0","modified":1651198712753},{"_id":"source/_posts/paper-reading-Vision-Transformer/ViT ablation 2.png","hash":"8b05e385a40bf6b53c6b2d33f5555d22f845fe19","modified":1650983346443},{"_id":"source/_posts/learning-rate-schedule/experiment.png","hash":"6aebe78be3d9979e72f67b7b044cba9aace879f9","modified":1646837491090},{"_id":"source/_posts/paper-reading-MAE/MAE result.png","hash":"de7915af273cca92a55890508a69dfa41fe81e15","modified":1651198517173},{"_id":"source/_posts/paper-reading-AlexNet/alexnet results.png","hash":"69f3bbb71d125bbbf5941a9b01fbe42766b35cc6","modified":1649425379662},{"_id":"public/baidusitemap.xml","hash":"c3d9a740f4bf4e195ced7a8480f55f576295574e","modified":1651400835569},{"_id":"public/content.json","hash":"eff0fb18136cf102c177edda4767747c73f2612d","modified":1651400835569},{"_id":"public/local-search.xml","hash":"4d5829b14e71c31b547d99e3ef3c130d238be6c0","modified":1651400835569},{"_id":"public/sitemap.xml","hash":"4502d4b323e71aefc97e249e8cf9a40a44aba6fa","modified":1651400835569},{"_id":"public/about/index.html","hash":"679a942c2f8fe897ffcc88041aec3a6eb1817930","modified":1651400835569},{"_id":"public/2022/02/22/Hello-ShouRou/index.html","hash":"1fec0ae733dacfd3ab87a8e34348c5214a4a27f3","modified":1651400835569},{"_id":"public/archives/index.html","hash":"5b3d41496ce7916a8ec6016e1ed01634fafb2e1a","modified":1651400835569},{"_id":"public/archives/2022/page/2/index.html","hash":"01b191faaa1542afa8881bd6ef93c37e235cce1c","modified":1651400835569},{"_id":"public/archives/2022/index.html","hash":"e4d9faadee765d03944ab6a380062bc94506f9b0","modified":1651400835569},{"_id":"public/archives/page/2/index.html","hash":"b44251ff0da9824e1c65af236d04aa1a17bfc653","modified":1651400835569},{"_id":"public/archives/2022/04/index.html","hash":"c9a87d7835d92d355b90ff22e9713dce9cac5028","modified":1651400835569},{"_id":"public/archives/2022/03/index.html","hash":"0e9d79c01a9be35b38512986779ffcefe7d12083","modified":1651400835569},{"_id":"public/archives/2022/02/index.html","hash":"c82bced0b6737fc6be6da883af86bb5e39cf986b","modified":1651400835569},{"_id":"public/tags/deep-learning-tricks/index.html","hash":"e6b528e8582a5ec1eb8c0c19e82cad5b74a35458","modified":1651400835569},{"_id":"public/tags/deep-learning/page/2/index.html","hash":"8038187ee3bd1d81bd7ca1037f58fab649e33447","modified":1651400835569},{"_id":"public/tags/deep-learning/index.html","hash":"aeac247a7e9da3b40f4beb472be534c907474464","modified":1651400835569},{"_id":"public/tags/First-Blog/index.html","hash":"8f65deb7291afca44cda9f53b7d60d4419b4ffc5","modified":1651400835569},{"_id":"public/tags/hexo/index.html","hash":"b7d45e287edc42a493d95f95cf0ec4aeeab746a8","modified":1651400835569},{"_id":"public/tags/paper-reading/index.html","hash":"0e339f7d3090754bd9292e7bbf60ebf943eded9e","modified":1651400835569},{"_id":"public/tags/m1-mac/index.html","hash":"c91c263cf269e483c1ee09e0355a6f81f36968af","modified":1651400835569},{"_id":"public/page/2/index.html","hash":"0c4463c434d6b509ba28ed169ca7a09c9a6501de","modified":1651400835569},{"_id":"public/tags/index.html","hash":"43f2dba69b22b0d9b5930df8196fa41765c2290c","modified":1651400835569},{"_id":"public/tags/blog/index.html","hash":"c87ba5e2f452283712b23485f844dd107f38b20f","modified":1651400835569},{"_id":"public/CV/index.html","hash":"79106f0b7d5a6d622b37459581dd294197e86ccf","modified":1651400835569},{"_id":"public/2022/04/30/Switch-blog-theme-to-FLUID/index.html","hash":"4a52707d960905d98a1c66c5af13d57fb80fd922","modified":1651400835569},{"_id":"public/2022/04/21/paper-reading-Vision-Transformer/index.html","hash":"3505f6b3d50dcaa7fde137d137fed871a9eccfb5","modified":1651400835569},{"_id":"public/2022/04/18/paper-reading-GPT1-3/index.html","hash":"0ed1df12fbd125af442708ec47291f56a2af6f73","modified":1651400835569},{"_id":"public/2022/04/15/paper-reading-bert/index.html","hash":"a36397751427628754c4560ff39ac7387fa27d56","modified":1651400835569},{"_id":"public/2022/04/14/paper-reading-A-gentle-introduction-to-graph-neural-networks/index.html","hash":"90631384b13d6eeffc13b8c91e71576d5bd1e1df","modified":1651400835569},{"_id":"public/2022/04/12/paper-reading-transformer/index.html","hash":"ccb4a5acf8b5bcffa3661f22712f5ecb2bba9489","modified":1651400835569},{"_id":"public/2022/04/27/paper-reading-MAE/index.html","hash":"0a1bd8b6a8c74b18973a46516a566b401c0f0bb0","modified":1651400835569},{"_id":"public/2022/04/09/paper-reading-ResNet/index.html","hash":"d8d8a4281300fe063f0673ebb71209016ac265a6","modified":1651400835569},{"_id":"public/2022/04/07/paper-reading-AlexNet/index.html","hash":"584fa9d75263c87a6ce8d9c1c20e1df9ff7fee17","modified":1651400835569},{"_id":"public/2022/04/02/paper-reading-start/index.html","hash":"457799d5ced86fd4cccc8fbb4b5bb2af6266d739","modified":1651400835569},{"_id":"public/2022/03/08/learning-rate-schedule/index.html","hash":"acd506d3ecee829c6eb7d3ebce804165a92a725b","modified":1651400835569},{"_id":"public/2022/03/04/Intro-and-Pytorch-Implementation-of-Label-Smoothing-Regularization-LSR/index.html","hash":"0000d5857f95cba4652bc980378e0c21234c9674","modified":1651400835569},{"_id":"public/2022/02/28/install-d2l-moudule-on-apple-m1-chip-for-deep-learning/index.html","hash":"bc940535b6842522db4eceefccf769671736b3aa","modified":1651400835569},{"_id":"public/2022/02/22/Build-and-configure-a-personal-blog-via-hexo-and-yilia/index.html","hash":"1e98d6872db946a974def03a9a3e2b54bee36b49","modified":1651400835569},{"_id":"public/index.html","hash":"2897912b1490e9455e390b3c30257b0fda665d25","modified":1651400835569},{"_id":"public/img/police_beian.png","hash":"90efded6baa2dde599a9d6b1387973e8e64923ea","modified":1651400835569},{"_id":"public/img/loading.gif","hash":"2d2fc0f947940f98c21afafef39ecf226a2e8d55","modified":1651400835569},{"_id":"public/img/default.png","hash":"7bb2b8ee07db305bcadee2985b81b942027ae940","modified":1651400835569},{"_id":"public/img/article_banner.avif","hash":"e307fb78545012ac5896ec812c8a128356ab0c35","modified":1651400835569},{"_id":"public/img/favicon.png","hash":"64b215db2cb3af98fe639e94537cb5209f959c78","modified":1651400835569},{"_id":"public/img/about_banner.avif","hash":"3a4cf09849f67a526772893a30036ffb15531a25","modified":1651400835569},{"_id":"public/img/about_banner.png","hash":"7c0f697200eb7da68dbc321c508e57018b1223b1","modified":1651400835569},{"_id":"public/img/bg_banner.avif","hash":"06980e9258fab6a2d68019df1665099c6639a660","modified":1651400835569},{"_id":"public/xml/local-search.xml","hash":"8c96ba6a064705602ce28d096fd7dd9069630a55","modified":1651400835569},{"_id":"public/img/avatar.png","hash":"fe739a158cc128f70f780eb5fa96f388b81d478f","modified":1651400835569},{"_id":"public/img/article_banner.png","hash":"a6efcc3ef2391694e8096017954381b4941ea5c7","modified":1651400835569},{"_id":"public/img/favicon.ico","hash":"7e55bf11b25279bbe79d6f9790374a4427e92eb6","modified":1651400835569},{"_id":"public/img/avatar.jpeg","hash":"c6575ca36dc695bf8330d6253ebd38e9761a9462","modified":1651400835569},{"_id":"public/2022/03/04/Intro-and-Pytorch-Implementation-of-Label-Smoothing-Regularization-LSR/Cross entropy loss function.png","hash":"f39593d083f6b6a97eb20780a4be745e19dd0452","modified":1651400835569},{"_id":"public/2022/02/22/Build-and-configure-a-personal-blog-via-hexo-and-yilia/Google sitemap inspect 3.png","hash":"091b41b10a1cc1c91cb81f48b062d32bc57b79d5","modified":1651400835569},{"_id":"public/img/bg_banner.png","hash":"112921badbfec0909ce6406ea94cf08147383392","modified":1651400835569},{"_id":"public/2022/02/22/Build-and-configure-a-personal-blog-via-hexo-and-yilia/bing search console success.png","hash":"03acd2a59a1e8af3276f36039a235c44afa5a218","modified":1651400835569},{"_id":"public/2022/02/22/Build-and-configure-a-personal-blog-via-hexo-and-yilia/bing sitemap.png","hash":"f8ca2e88f4c6e1969395b7617075010a4a697565","modified":1651400835569},{"_id":"public/2022/04/18/paper-reading-GPT1-3/GPT fine tune f1.png","hash":"f5d63e0fdf6eb56aaa20a2da422bd0ef3b7dbfba","modified":1651400835569},{"_id":"public/2022/04/18/paper-reading-GPT1-3/GPT fine tune f2.png","hash":"5b0c974e64cede1083a266d5149d53ee972208f8","modified":1651400835569},{"_id":"public/2022/04/18/paper-reading-GPT1-3/GPT fine tune loss.png","hash":"85d0003ae86d00ffd5b8c8ef719f028dac417394","modified":1651400835569},{"_id":"public/2022/04/18/paper-reading-GPT1-3/GPT loss.png","hash":"9b87e4122b93191ebd69aca5195f5d216cab968d","modified":1651400835569},{"_id":"public/2022/04/15/paper-reading-bert/model size graph.webp","hash":"faed538a605818e2cef417ed29692bf18745daf5","modified":1651400835569},{"_id":"public/2022/03/08/learning-rate-schedule/2d cyclic learning rate schedule.png","hash":"c8123233438e88551c7453ecd19706baa436bc4a","modified":1651400835569},{"_id":"public/2022/03/04/Intro-and-Pytorch-Implementation-of-Label-Smoothing-Regularization-LSR/accuracy curve applying label smoothing.png","hash":"4aeeb4d6d7711a4396e2149e4d1f8130d183edcc","modified":1651400835569},{"_id":"public/2022/02/22/Build-and-configure-a-personal-blog-via-hexo-and-yilia/being sitemap connect google.png","hash":"2f7067940f7d336d1c87987c26562bb87b4f90bf","modified":1651400835569},{"_id":"public/2022/02/22/Build-and-configure-a-personal-blog-via-hexo-and-yilia/baidu console sitemap.png","hash":"8ecba7e34a53e980586ae8ab154511d951867aee","modified":1651400835569},{"_id":"public/2022/04/18/paper-reading-GPT1-3/GPT timeline.png","hash":"109115b9d17368fd43c4c536fe50449deec95fcf","modified":1651400835569},{"_id":"public/2022/04/21/paper-reading-Vision-Transformer/ViT variants.png","hash":"d4f9886bdfcfd28b0c84ca95cfef3b6982acfa05","modified":1651400835569},{"_id":"public/2022/03/08/learning-rate-schedule/SGD with learning rate decay.png","hash":"83c473cfe0a99facc1b1ab54af01115989e11b0c","modified":1651400835569},{"_id":"public/2022/03/08/learning-rate-schedule/source ppt.pptx","hash":"1cd5b8c2dcc76d002d10173f1c0071bf331898e4","modified":1651400835569},{"_id":"public/js/boot.js","hash":"3de344ee619da989f6dccf7c2ae459fe91075983","modified":1651400835569},{"_id":"public/css/gitalk.css","hash":"a57b3cc8e04a0a4a27aefa07facf5b5e7bca0e76","modified":1651400835569},{"_id":"public/js/img-lazyload.js","hash":"cbdeca434ec4da51f488c821d51b4d23c73294af","modified":1651400835569},{"_id":"public/js/plugins.js","hash":"d058f30bd09b28769c4d8313428ff23dfc8d52dd","modified":1651400835569},{"_id":"public/js/color-schema.js","hash":"f1fdd512f3ef92ff5db4a49f5a3143d5ddea9858","modified":1651400835569},{"_id":"public/js/leancloud.js","hash":"53987b7a668ea0623370eb83ed5311766221b557","modified":1651400835569},{"_id":"public/js/utils.js","hash":"4a43f2700e91937650bef511fd438825b001c4c6","modified":1651400835569},{"_id":"public/js/events.js","hash":"4b9d2676c9544db9cc40a8c7d18456792299ba86","modified":1651400835569},{"_id":"public/js/local-search.js","hash":"633f0142c657805359b0197f287e12ae4bcde731","modified":1651400835569},{"_id":"public/css/main.css","hash":"6283cf5ed62989c97cf220485c59c1a8b1bdc14c","modified":1651400835569},{"_id":"public/2022/02/22/Build-and-configure-a-personal-blog-via-hexo-and-yilia/can't fetch sitemap.png","hash":"f33cf775460666ebefa5e8aa56b768a1ce4d1af4","modified":1651400835569},{"_id":"public/2022/02/22/Build-and-configure-a-personal-blog-via-hexo-and-yilia/google console varification.png","hash":"54b854a7bc2152a565ae61ab6f513fe8fad31139","modified":1651400835569},{"_id":"public/2022/04/09/paper-reading-ResNet/ResNet figure 2.png","hash":"4aae59eb537b2676e973e61b714f5e2260592443","modified":1651400835569},{"_id":"public/2022/04/15/paper-reading-bert/BERT learnable paramters.png","hash":"8839786a937ceb1f0a58110f5dc72b06a48024b8","modified":1651400835569},{"_id":"public/2022/03/08/learning-rate-schedule/warmup on large batches.png","hash":"c7c61a90faa6fbe7cbb58f35c651cfacc5357db8","modified":1651400835569},{"_id":"public/2022/03/04/Intro-and-Pytorch-Implementation-of-Label-Smoothing-Regularization-LSR/accuracy curve compare label smoothing with hard label.png","hash":"cdae6eacdd69d900057bf680a2d321e58dda17d9","modified":1651400835569},{"_id":"public/2022/02/22/Build-and-configure-a-personal-blog-via-hexo-and-yilia/baidu console.png","hash":"1b21c2b62d1217fc07402a2b5a93ade16b9d06a9","modified":1651400835569},{"_id":"public/2022/02/22/Build-and-configure-a-personal-blog-via-hexo-and-yilia/check google search.png","hash":"a283f4c31f86a0f29ffc685c572431a00e5199c7","modified":1651400835569},{"_id":"public/2022/04/14/paper-reading-A-gentle-introduction-to-graph-neural-networks/GNN graph level task.png","hash":"e1af92d12c8f7e0b9943f7ed839198b03088a7e4","modified":1651400835569},{"_id":"public/2022/02/22/Build-and-configure-a-personal-blog-via-hexo-and-yilia/typora setting.png","hash":"4bbaf27946caa82912470a69221a7f494b985bf9","modified":1651400835569},{"_id":"public/2022/04/18/paper-reading-GPT1-3/GPT3 models.png","hash":"93adbab90cb1e999d9a6969ebfea7b1e82672d68","modified":1651400835569},{"_id":"public/2022/04/18/paper-reading-GPT1-3/GPT pretraining process.png","hash":"a634a1d5a49fa244f8d0cdcb6c982f4c9289d563","modified":1651400835569},{"_id":"public/2022/04/18/paper-reading-GPT1-3/GPT-3 result 2.png","hash":"646a38e14e8bef9fd0f5615acf0cbd43c5042205","modified":1651400835569},{"_id":"public/2022/04/18/paper-reading-GPT1-3/GPT3 race.png","hash":"3ceb1c3a74634b2c40b608e3c6a9529a141907f1","modified":1651400835569},{"_id":"public/2022/04/09/paper-reading-ResNet/ResNet figure 5.png","hash":"535021caab08b1436c194688c6f15cbbce199308","modified":1651400835569},{"_id":"public/2022/04/21/paper-reading-Vision-Transformer/VIT model overview.png","hash":"25618195462760c8e7bcccf86e02e43cff1ee0c9","modified":1651400835569},{"_id":"public/2022/04/21/paper-reading-Vision-Transformer/VIT algorithm.png","hash":"49c6702a90c2be602b51765d7e9824f5aa722522","modified":1651400835569},{"_id":"public/2022/03/08/learning-rate-schedule/CLR.png","hash":"e7bbe3fb1a452bf830806ae0dba4ec0eb6ae236b","modified":1651400835569},{"_id":"public/2022/04/21/paper-reading-Vision-Transformer/VIT positional embedding ablation.png","hash":"774bb656e0f545459f58b0b0b521c1883fe5df01","modified":1651400835569},{"_id":"public/2022/03/08/learning-rate-schedule/SGDR.png","hash":"ef63d3bb9bf8b0083de55ef9b3c9fc4a0d57608d","modified":1651400835569},{"_id":"public/2022/03/04/Intro-and-Pytorch-Implementation-of-Label-Smoothing-Regularization-LSR/Label smoothing feature norm.png","hash":"47841e9b48ef8aaa61bdedf70b593a1d563840ca","modified":1651400835569},{"_id":"public/2022/03/04/Intro-and-Pytorch-Implementation-of-Label-Smoothing-Regularization-LSR/leave class code.png","hash":"224786316ca7dc910a29d3c5acb42f755f3bc662","modified":1651400835569},{"_id":"public/2022/02/22/Build-and-configure-a-personal-blog-via-hexo-and-yilia/google console.png","hash":"240520671ae14a4db700b002e99ee276bf1fe650","modified":1651400835569},{"_id":"public/2022/04/07/paper-reading-AlexNet/unsupervise learning cake.png","hash":"adcb79d097ff6fdea2021bb4668f8eeb8a57358d","modified":1651400835569},{"_id":"public/2022/04/27/paper-reading-MAE/MAE mask ratio.png","hash":"75bc0ff014fb48e2e3adff8f2a64298c498ba98d","modified":1651400835569},{"_id":"public/2022/04/18/paper-reading-GPT1-3/GPT3 gender.png","hash":"49460dd410dcef903e806afb83aeaa82013049cf","modified":1651400835569},{"_id":"public/2022/04/18/paper-reading-GPT1-3/GPT3 training data.png","hash":"d8c11af3e71e7ce5d5aa4cd47e6836d816f04a5b","modified":1651400835569},{"_id":"public/2022/03/08/learning-rate-schedule/cyclic learning rate schedule.png","hash":"0c093af7caf829e60f29eea8f72140ba75a85ba3","modified":1651400835569},{"_id":"public/2022/03/04/Intro-and-Pytorch-Implementation-of-Label-Smoothing-Regularization-LSR/hard label.png","hash":"8e83c8d43703867f9faf7a1db317fcb040a24045","modified":1651400835569},{"_id":"public/2022/03/04/Intro-and-Pytorch-Implementation-of-Label-Smoothing-Regularization-LSR/soft label.png","hash":"2d6357cd03c4b4149f5191e91d781cc5a554a897","modified":1651400835569},{"_id":"public/2022/04/18/paper-reading-GPT1-3/GPT-3 approach.png","hash":"21e4e6703fcf07679e8cb6cf42931b444145426b","modified":1651400835569},{"_id":"public/2022/04/18/paper-reading-GPT1-3/GPT3 result.png","hash":"3c4192ef7ff33164dba01b3ea9754c3b02011bda","modified":1651400835569},{"_id":"public/2022/04/18/paper-reading-GPT1-3/GPT3 result 3.png","hash":"9360c9a0adb1051e96ff62e6273e4ad747297b6b","modified":1651400835569},{"_id":"public/2022/04/21/paper-reading-Vision-Transformer/ViT results.png","hash":"5506ce41ac7b84fc8c10ce73c980d169531145ad","modified":1651400835569},{"_id":"public/2022/04/12/paper-reading-transformer/transformer.png","hash":"6eb38ed16fbaf3660d18b4266763411eadc5a452","modified":1651400835569},{"_id":"public/2022/02/22/Build-and-configure-a-personal-blog-via-hexo-and-yilia/Last snapshot.png","hash":"9ccbadb43f29a86f56d4820913b7daaa8d31e211","modified":1651400835569},{"_id":"public/2022/02/22/Build-and-configure-a-personal-blog-via-hexo-and-yilia/baidu console varification.png","hash":"09c2e3d1f3b52807bf6dd97d8ec42f7cfd1fe1c7","modified":1651400835569},{"_id":"public/2022/04/14/paper-reading-A-gentle-introduction-to-graph-neural-networks/GNN interative archtecture.png","hash":"06e8477aee1a6f12385895a4151dea90ca23e382","modified":1651400835569},{"_id":"public/2022/04/18/paper-reading-GPT1-3/GPT3 performance with compute.png","hash":"e39d32f750f4853f15783654367e544c0bd70d50","modified":1651400835569},{"_id":"public/2022/04/18/paper-reading-GPT1-3/GPT3 result4.png","hash":"c261a581c8c8f50e1d9ef50b576415c5116e3467","modified":1651400835569},{"_id":"public/2022/04/09/paper-reading-ResNet/ResNet table 1.png","hash":"782d634274c1221c9db07ea5c9367c4fef5fbe4b","modified":1651400835569},{"_id":"public/2022/04/15/paper-reading-bert/BERT alibation study 1.png","hash":"f71b28cdf11f80dcf3fc5b8c89ad2624d6caad1c","modified":1651400835569},{"_id":"public/2022/03/08/learning-rate-schedule/SGDR_REsult.png","hash":"f118bc7b7c5ea51afa9ae366f6bce17757963467","modified":1651400835569},{"_id":"public/2022/04/12/paper-reading-transformer/transformer table1.png","hash":"637292c4c8f161ffa35d2b41ebcf1a37019638f3","modified":1651400835569},{"_id":"public/2022/02/22/Build-and-configure-a-personal-blog-via-hexo-and-yilia/github page.png","hash":"5262aa8e506f60a57b7ddc69a76c860bca21ffce","modified":1651400835569},{"_id":"public/2022/04/18/paper-reading-GPT1-3/GPT objectives.png","hash":"34aa333335dbb1d0292b65258750ac163a03df2f","modified":1651400835569},{"_id":"public/2022/04/12/paper-reading-transformer/layer norm.png","hash":"9e0a63ba884eb2efa0604a74dedb92d0cad7420b","modified":1651400835569},{"_id":"public/2022/04/18/paper-reading-GPT1-3/BERT result with GPT.png","hash":"a922a5d9ec6bb140b431baa56ec969dc9d019395","modified":1651400835569},{"_id":"public/2022/04/18/paper-reading-GPT1-3/GPT2 performance.png","hash":"ed244f4af159e064c2862a8933cd891754a17b7d","modified":1651400835569},{"_id":"public/2022/02/22/Build-and-configure-a-personal-blog-via-hexo-and-yilia/Google sitemap inspect.png","hash":"90618421c6cab7c23ef65da92bc0c48090bc93ea","modified":1651400835569},{"_id":"public/2022/04/14/paper-reading-A-gentle-introduction-to-graph-neural-networks/GNN graph message passing.png","hash":"46c17f41a8ff34811854ff5c913693d167933833","modified":1651400835569},{"_id":"public/2022/02/22/Build-and-configure-a-personal-blog-via-hexo-and-yilia/node js install.png","hash":"49b580f0049a4e224ea17cbb30a33dc34e8acc3a","modified":1651400835569},{"_id":"public/2022/04/21/paper-reading-Vision-Transformer/VIT class token ablation.png","hash":"5c6b482cd85546e81183f7c6090cb110791f8dd4","modified":1651400835569},{"_id":"public/2022/03/04/Intro-and-Pytorch-Implementation-of-Label-Smoothing-Regularization-LSR/source ppt.pptx","hash":"4e53421cbb49964ea368ac4c9259914407a20749","modified":1651400835569},{"_id":"public/2022/04/18/paper-reading-GPT1-3/GPT 3 result 5.png","hash":"5bb6e24fd0d52e4153e80828ee9c250d15465c7b","modified":1651400835569},{"_id":"public/2022/04/09/paper-reading-ResNet/ResNet table 3.png","hash":"62e26b0634b37d7dc652e5b301937fabd7ba69dd","modified":1651400835569},{"_id":"public/2022/04/15/paper-reading-bert/BERT segment embedding.png","hash":"b3916ee35fc24b258adaf99313b3358e0de35510","modified":1651400835569},{"_id":"public/2022/04/09/paper-reading-ResNet/ResNet table 6.png","hash":"61fd7fcd8b823045cedbf6be8a3c91582124ee6f","modified":1651400835569},{"_id":"public/2022/04/27/paper-reading-MAE/MAE architecture.png","hash":"bd19b6cb51c6edf1b32eb6ed2ebf9423539e2cc6","modified":1651400835569},{"_id":"public/2022/04/21/paper-reading-Vision-Transformer/ViT inspecting.png","hash":"3da9c71e7aaa955e3ff2ffe75c03fad55ff74004","modified":1651400835569},{"_id":"public/2022/02/22/Build-and-configure-a-personal-blog-via-hexo-and-yilia/Google sitemap inspect 2.png","hash":"acaf3254cd16099f517b0f1e1ab58aa89552e6cd","modified":1651400835569},{"_id":"public/2022/04/09/paper-reading-ResNet/ResNet figure 4.png","hash":"b3e1f256838b4036e47d0584a8f84f8a7cd7be08","modified":1651400835569},{"_id":"public/2022/04/18/paper-reading-GPT1-3/GPT3 performance.png","hash":"3d82eb6c71f15523cb51c14bf33573debdd38717","modified":1651400835569},{"_id":"public/2022/04/09/paper-reading-ResNet/ResNet figure 1.png","hash":"765c606c3ae4bafcdd03dba4cda928f782204576","modified":1651400835569},{"_id":"public/2022/04/21/paper-reading-Vision-Transformer/CNN first layer filters.png","hash":"b33e246344bda2b7c32f402c7724b6ae07889114","modified":1651400835569},{"_id":"public/2022/04/12/paper-reading-transformer/transformer table 3.png","hash":"f0c745ce7dbb8cbad5628f1a1b7f4ab207a8ebd7","modified":1651400835569},{"_id":"public/2022/04/21/paper-reading-Vision-Transformer/VIT properties.png","hash":"f394d65267154e4aacb9329b6a1cd257a3165ddb","modified":1651400835569},{"_id":"public/2022/04/27/paper-reading-MAE/MAE result2.png","hash":"65a47208b3ab66a13c5925ce51a1f03906c126c0","modified":1651400835569},{"_id":"public/2022/04/21/paper-reading-Vision-Transformer/ViT ablation 2.png","hash":"8b05e385a40bf6b53c6b2d33f5555d22f845fe19","modified":1651400835569},{"_id":"public/2022/03/08/learning-rate-schedule/experiment.png","hash":"6aebe78be3d9979e72f67b7b044cba9aace879f9","modified":1651400835569},{"_id":"public/2022/04/27/paper-reading-MAE/MAE result.png","hash":"de7915af273cca92a55890508a69dfa41fe81e15","modified":1651400835569},{"_id":"public/2022/04/07/paper-reading-AlexNet/alexnet results.png","hash":"69f3bbb71d125bbbf5941a9b01fbe42766b35cc6","modified":1651400835569}],"Category":[],"Data":[],"Page":[{"title":"CV","layout":"about","date":"2022-05-01T08:58:03.000Z","_content":"\n## Work Experience\n\n### Automotive CFD Engineer \n\n<div style=\"margin-top:-3%; text-align: right\"> 04/2021- present </div> \n\nPre R&D, [Shanghai Volkswagen Automotive Co., Ltd](https://volkswagengroupchina.com.cn/en/partner/saicvolkswagen).\n\n- OpenFOAM developing\n- Intelligent aided fluid dynamic\n\nEducation Background\n---\n\n### MSc: [Advanced Computational Methods for Aeronautics, Flow Management and Fluid Structure Interaction](https://www.imperial.ac.uk/study/pg/aeronautics/computational-methods/) \n\n<div style=\"margin-top:-3%; text-align: right\"> 09/2019-11/2020 </div> \n\nDepartment of Aeronautics, Imperial college London\n\n- Grade: 74.3/100 (Merit)\n- Modules include: Computational fluid dynamics, High-Performance Computing, Aeroservoelasticity, introduce to flow control, Hydrodynamic Stability, Separated Flows and Fluid Structure Interaction, etc.  \n\n### BEng: [Flight Vehicle Propulsion Engineering](https://en.nwpu.edu.cn/index.htm)\n\n<div style=\"margin-top:-3%; text-align: right\"> 09/2015-07/2019 </div> \n\nSchool of Power and Energy, Northwestern Polytechnical University \n\n- Grade: 86.5/100\n- Modules include: Fundamentals of Gas Dynamics, Fluid Mechanics, Heat Transfer, Mechanical theory, Turbomachinery etc. \n\n#### Pre-university Qualification: Total Score of NCEE ([GaoKao](https://en.wikipedia.org/wiki/Gaokao)): 619/750\n\nAcademic Projects\n---\n\n#### Bifurcation and Oscillation Effects of Gyrotactic Swimming Microorganism Suspension in Vertical Pipe\n\n<div style=\"margin-top:-2%; text-align: right\"> 05/2020-10/2020 </div> \n\nImperial College London MSc Individual Project\n\nDirector: [Dr. Yongyun Hwang](https://www.imperial.ac.uk/people/y.hwang)\n\n- Learning concepts of bioreactors and behaviours of micro-algae;\n- Developing numerical solvers for microorganism suspension;\n- Exploring and analysing bifurcations and instabilities under different flow conditions.\n\n\n\n#### Flow Field Analysis Based on RANS Solver and BiGlobal Stability Theory\n\n<div style=\"margin-top:-3%; text-align: right\"> 02/2019-06/2019 </div> \n\nUndergraduate Graduation Project & [Erasmus+ Scholarship Programme](https://erasmus-plus.ec.europa.eu/)\n\nInstitutions: [Polytechnic University of Madrid](http://www.upm.es/internacional)& [Northwestern Polytechnical University](https://en.nwpu.edu.cn/)\n\nDirector: [Professor Eusebio Valero Sanchez](http://www.upm.es/observatorio/vi/index.jsp?pageac=investigador.jsp&idInvestigador=5214) & [Associate Professor Yaguo Lyu](https://teacher.nwpu.edu.cn/yaguo)\n\n- Handling of the necessary tool, DLR TAU-Code;\n- Performing simulations of non-stationary problems;\n- Analysing the stability of the flow fields by POD and DMD.\n\n\n\n#### Design Research on Bionic Anti-drag Propeller (Project Manager)\n\n<div style=\"margin-top:-3%; text-align: right\"> 04/2017-04/2018 </div> \n\n[China college students “Internet+” Innovation and Entrepreneurship Competition](https://www.pilcchina.org/?locale=en)\n\nDirector: [Professor Wang yangang](https://teacher.nwpu.edu.cn/wangyg.html)\n\n- Proposing and design a novel drone propeller with serrated leading edge;\n- Carrying out 3D modelling, CFD simulations and data analysis;\n\n\n\n#### Experimental Fluid Mechanics (Summer School)\n\n<div style=\"margin-top:-3%; text-align: right\"> 8/2018 </div> \n\n[International Scholarship Summer School Programme](https://odin.sdu.dk/sitecore/index.php?a=fagbesk&id=79256&lang=en), University of Southern Denmark\n\n- Performing a team project to optimise a wind turbine airfoil;\n- Data analysing of wind tunnel and water channel PIV results;\n- Visiting the [LM Wind Power Test and Validation Centre](https://www.lmwindpower.com/en/services/technology-centers/test-and-validation-center-denmark).\n\n\n\n#### Starting Test of Pulse Jet Engine (In Pairs)\n\n<div style=\"margin-top:-3%; text-align: right\"> 07/2017-08/2017 </div> \n\nScientific Research Practice Programme\n\nDirector: [Professer Yan Hong](https://teacher.nwpu.edu.cn/m/en/2010010152.html)\n\n- Setting up the experiment platform;\n- Measuring the thrust and pressure pulse frequency of a valveless pulse engine.\n\nProfessional Skills\n---\n\nI.T. SKILLS: Python, C++\n\nRelated software: OpenFOAM, CATIA\n\n","source":"CV/index.md","raw":"---\ntitle: CV\nlayout: about\ndate: 2022-05-01 16:58:03\n---\n\n## Work Experience\n\n### Automotive CFD Engineer \n\n<div style=\"margin-top:-3%; text-align: right\"> 04/2021- present </div> \n\nPre R&D, [Shanghai Volkswagen Automotive Co., Ltd](https://volkswagengroupchina.com.cn/en/partner/saicvolkswagen).\n\n- OpenFOAM developing\n- Intelligent aided fluid dynamic\n\nEducation Background\n---\n\n### MSc: [Advanced Computational Methods for Aeronautics, Flow Management and Fluid Structure Interaction](https://www.imperial.ac.uk/study/pg/aeronautics/computational-methods/) \n\n<div style=\"margin-top:-3%; text-align: right\"> 09/2019-11/2020 </div> \n\nDepartment of Aeronautics, Imperial college London\n\n- Grade: 74.3/100 (Merit)\n- Modules include: Computational fluid dynamics, High-Performance Computing, Aeroservoelasticity, introduce to flow control, Hydrodynamic Stability, Separated Flows and Fluid Structure Interaction, etc.  \n\n### BEng: [Flight Vehicle Propulsion Engineering](https://en.nwpu.edu.cn/index.htm)\n\n<div style=\"margin-top:-3%; text-align: right\"> 09/2015-07/2019 </div> \n\nSchool of Power and Energy, Northwestern Polytechnical University \n\n- Grade: 86.5/100\n- Modules include: Fundamentals of Gas Dynamics, Fluid Mechanics, Heat Transfer, Mechanical theory, Turbomachinery etc. \n\n#### Pre-university Qualification: Total Score of NCEE ([GaoKao](https://en.wikipedia.org/wiki/Gaokao)): 619/750\n\nAcademic Projects\n---\n\n#### Bifurcation and Oscillation Effects of Gyrotactic Swimming Microorganism Suspension in Vertical Pipe\n\n<div style=\"margin-top:-2%; text-align: right\"> 05/2020-10/2020 </div> \n\nImperial College London MSc Individual Project\n\nDirector: [Dr. Yongyun Hwang](https://www.imperial.ac.uk/people/y.hwang)\n\n- Learning concepts of bioreactors and behaviours of micro-algae;\n- Developing numerical solvers for microorganism suspension;\n- Exploring and analysing bifurcations and instabilities under different flow conditions.\n\n\n\n#### Flow Field Analysis Based on RANS Solver and BiGlobal Stability Theory\n\n<div style=\"margin-top:-3%; text-align: right\"> 02/2019-06/2019 </div> \n\nUndergraduate Graduation Project & [Erasmus+ Scholarship Programme](https://erasmus-plus.ec.europa.eu/)\n\nInstitutions: [Polytechnic University of Madrid](http://www.upm.es/internacional)& [Northwestern Polytechnical University](https://en.nwpu.edu.cn/)\n\nDirector: [Professor Eusebio Valero Sanchez](http://www.upm.es/observatorio/vi/index.jsp?pageac=investigador.jsp&idInvestigador=5214) & [Associate Professor Yaguo Lyu](https://teacher.nwpu.edu.cn/yaguo)\n\n- Handling of the necessary tool, DLR TAU-Code;\n- Performing simulations of non-stationary problems;\n- Analysing the stability of the flow fields by POD and DMD.\n\n\n\n#### Design Research on Bionic Anti-drag Propeller (Project Manager)\n\n<div style=\"margin-top:-3%; text-align: right\"> 04/2017-04/2018 </div> \n\n[China college students “Internet+” Innovation and Entrepreneurship Competition](https://www.pilcchina.org/?locale=en)\n\nDirector: [Professor Wang yangang](https://teacher.nwpu.edu.cn/wangyg.html)\n\n- Proposing and design a novel drone propeller with serrated leading edge;\n- Carrying out 3D modelling, CFD simulations and data analysis;\n\n\n\n#### Experimental Fluid Mechanics (Summer School)\n\n<div style=\"margin-top:-3%; text-align: right\"> 8/2018 </div> \n\n[International Scholarship Summer School Programme](https://odin.sdu.dk/sitecore/index.php?a=fagbesk&id=79256&lang=en), University of Southern Denmark\n\n- Performing a team project to optimise a wind turbine airfoil;\n- Data analysing of wind tunnel and water channel PIV results;\n- Visiting the [LM Wind Power Test and Validation Centre](https://www.lmwindpower.com/en/services/technology-centers/test-and-validation-center-denmark).\n\n\n\n#### Starting Test of Pulse Jet Engine (In Pairs)\n\n<div style=\"margin-top:-3%; text-align: right\"> 07/2017-08/2017 </div> \n\nScientific Research Practice Programme\n\nDirector: [Professer Yan Hong](https://teacher.nwpu.edu.cn/m/en/2010010152.html)\n\n- Setting up the experiment platform;\n- Measuring the thrust and pressure pulse frequency of a valveless pulse engine.\n\nProfessional Skills\n---\n\nI.T. SKILLS: Python, C++\n\nRelated software: OpenFOAM, CATIA\n\n","updated":"2022-05-01T10:25:51.098Z","path":"CV/index.html","comments":1,"_id":"cl2n5f4030000p9yb11v3gutd","content":"<h2 id=\"work-experience\">Work Experience</h2>\n<h3 id=\"automotive-cfd-engineer\">Automotive CFD Engineer</h3>\n<div style=\"margin-top:-3%; text-align: right\">\n04/2021- present\n</div>\n<p>Pre R&amp;D, <a href=\"https://volkswagengroupchina.com.cn/en/partner/saicvolkswagen\">Shanghai Volkswagen Automotive Co., Ltd</a>.</p>\n<ul>\n<li>OpenFOAM developing</li>\n<li>Intelligent aided fluid dynamic</li>\n</ul>\n<h2 id=\"education-background\">Education Background</h2>\n<h3 id=\"msc-advanced-computational-methods-for-aeronautics-flow-management-and-fluid-structure-interaction\">MSc: <a href=\"https://www.imperial.ac.uk/study/pg/aeronautics/computational-methods/\">Advanced Computational Methods for Aeronautics, Flow Management and Fluid Structure Interaction</a></h3>\n<div style=\"margin-top:-3%; text-align: right\">\n09/2019-11/2020\n</div>\n<p>Department of Aeronautics, Imperial college London</p>\n<ul>\n<li>Grade: 74.3/100 (Merit)</li>\n<li>Modules include: Computational fluid dynamics, High-Performance Computing, Aeroservoelasticity, introduce to flow control, Hydrodynamic Stability, Separated Flows and Fluid Structure Interaction, etc.</li>\n</ul>\n<h3 id=\"beng-flight-vehicle-propulsion-engineering\">BEng: <a href=\"https://en.nwpu.edu.cn/index.htm\">Flight Vehicle Propulsion Engineering</a></h3>\n<div style=\"margin-top:-3%; text-align: right\">\n09/2015-07/2019\n</div>\n<p>School of Power and Energy, Northwestern Polytechnical University</p>\n<ul>\n<li>Grade: 86.5/100</li>\n<li>Modules include: Fundamentals of Gas Dynamics, Fluid Mechanics, Heat Transfer, Mechanical theory, Turbomachinery etc.</li>\n</ul>\n<h4 id=\"pre-university-qualification-total-score-of-ncee-gaokao-619750\">Pre-university Qualification: Total Score of NCEE (<a href=\"https://en.wikipedia.org/wiki/Gaokao\">GaoKao</a>): 619/750</h4>\n<h2 id=\"academic-projects\">Academic Projects</h2>\n<h4 id=\"bifurcation-and-oscillation-effects-of-gyrotactic-swimming-microorganism-suspension-in-vertical-pipe\">Bifurcation and Oscillation Effects of Gyrotactic Swimming Microorganism Suspension in Vertical Pipe</h4>\n<div style=\"margin-top:-2%; text-align: right\">\n05/2020-10/2020\n</div>\n<p>Imperial College London MSc Individual Project</p>\n<p>Director: <a href=\"https://www.imperial.ac.uk/people/y.hwang\">Dr. Yongyun Hwang</a></p>\n<ul>\n<li>Learning concepts of bioreactors and behaviours of micro-algae;</li>\n<li>Developing numerical solvers for microorganism suspension;</li>\n<li>Exploring and analysing bifurcations and instabilities under different flow conditions.</li>\n</ul>\n<h4 id=\"flow-field-analysis-based-on-rans-solver-and-biglobal-stability-theory\">Flow Field Analysis Based on RANS Solver and BiGlobal Stability Theory</h4>\n<div style=\"margin-top:-3%; text-align: right\">\n02/2019-06/2019\n</div>\n<p>Undergraduate Graduation Project &amp; <a href=\"https://erasmus-plus.ec.europa.eu/\">Erasmus+ Scholarship Programme</a></p>\n<p>Institutions: <a href=\"http://www.upm.es/internacional\">Polytechnic University of Madrid</a>&amp; <a href=\"https://en.nwpu.edu.cn/\">Northwestern Polytechnical University</a></p>\n<p>Director: <a href=\"http://www.upm.es/observatorio/vi/index.jsp?pageac=investigador.jsp&amp;idInvestigador=5214\">Professor Eusebio Valero Sanchez</a> &amp; <a href=\"https://teacher.nwpu.edu.cn/yaguo\">Associate Professor Yaguo Lyu</a></p>\n<ul>\n<li>Handling of the necessary tool, DLR TAU-Code;</li>\n<li>Performing simulations of non-stationary problems;</li>\n<li>Analysing the stability of the flow fields by POD and DMD.</li>\n</ul>\n<h4 id=\"design-research-on-bionic-anti-drag-propeller-project-manager\">Design Research on Bionic Anti-drag Propeller (Project Manager)</h4>\n<div style=\"margin-top:-3%; text-align: right\">\n04/2017-04/2018\n</div>\n<p><a href=\"https://www.pilcchina.org/?locale=en\">China college students “Internet+” Innovation and Entrepreneurship Competition</a></p>\n<p>Director: <a href=\"https://teacher.nwpu.edu.cn/wangyg.html\">Professor Wang yangang</a></p>\n<ul>\n<li>Proposing and design a novel drone propeller with serrated leading edge;</li>\n<li>Carrying out 3D modelling, CFD simulations and data analysis;</li>\n</ul>\n<h4 id=\"experimental-fluid-mechanics-summer-school\">Experimental Fluid Mechanics (Summer School)</h4>\n<div style=\"margin-top:-3%; text-align: right\">\n8/2018\n</div>\n<p><a href=\"https://odin.sdu.dk/sitecore/index.php?a=fagbesk&amp;id=79256&amp;lang=en\">International Scholarship Summer School Programme</a>, University of Southern Denmark</p>\n<ul>\n<li>Performing a team project to optimise a wind turbine airfoil;</li>\n<li>Data analysing of wind tunnel and water channel PIV results;</li>\n<li>Visiting the <a href=\"https://www.lmwindpower.com/en/services/technology-centers/test-and-validation-center-denmark\">LM Wind Power Test and Validation Centre</a>.</li>\n</ul>\n<h4 id=\"starting-test-of-pulse-jet-engine-in-pairs\">Starting Test of Pulse Jet Engine (In Pairs)</h4>\n<div style=\"margin-top:-3%; text-align: right\">\n07/2017-08/2017\n</div>\n<p>Scientific Research Practice Programme</p>\n<p>Director: <a href=\"https://teacher.nwpu.edu.cn/m/en/2010010152.html\">Professer Yan Hong</a></p>\n<ul>\n<li>Setting up the experiment platform;</li>\n<li>Measuring the thrust and pressure pulse frequency of a valveless pulse engine.</li>\n</ul>\n<h2 id=\"professional-skills\">Professional Skills</h2>\n<p>I.T. SKILLS: Python, C++</p>\n<p>Related software: OpenFOAM, CATIA</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"work-experience\">Work Experience</h2>\n<h3 id=\"automotive-cfd-engineer\">Automotive CFD Engineer</h3>\n<div style=\"margin-top:-3%; text-align: right\">\n04/2021- present\n</div>\n<p>Pre R&amp;D, <a href=\"https://volkswagengroupchina.com.cn/en/partner/saicvolkswagen\">Shanghai Volkswagen Automotive Co., Ltd</a>.</p>\n<ul>\n<li>OpenFOAM developing</li>\n<li>Intelligent aided fluid dynamic</li>\n</ul>\n<h2 id=\"education-background\">Education Background</h2>\n<h3 id=\"msc-advanced-computational-methods-for-aeronautics-flow-management-and-fluid-structure-interaction\">MSc: <a href=\"https://www.imperial.ac.uk/study/pg/aeronautics/computational-methods/\">Advanced Computational Methods for Aeronautics, Flow Management and Fluid Structure Interaction</a></h3>\n<div style=\"margin-top:-3%; text-align: right\">\n09/2019-11/2020\n</div>\n<p>Department of Aeronautics, Imperial college London</p>\n<ul>\n<li>Grade: 74.3/100 (Merit)</li>\n<li>Modules include: Computational fluid dynamics, High-Performance Computing, Aeroservoelasticity, introduce to flow control, Hydrodynamic Stability, Separated Flows and Fluid Structure Interaction, etc.</li>\n</ul>\n<h3 id=\"beng-flight-vehicle-propulsion-engineering\">BEng: <a href=\"https://en.nwpu.edu.cn/index.htm\">Flight Vehicle Propulsion Engineering</a></h3>\n<div style=\"margin-top:-3%; text-align: right\">\n09/2015-07/2019\n</div>\n<p>School of Power and Energy, Northwestern Polytechnical University</p>\n<ul>\n<li>Grade: 86.5/100</li>\n<li>Modules include: Fundamentals of Gas Dynamics, Fluid Mechanics, Heat Transfer, Mechanical theory, Turbomachinery etc.</li>\n</ul>\n<h4 id=\"pre-university-qualification-total-score-of-ncee-gaokao-619750\">Pre-university Qualification: Total Score of NCEE (<a href=\"https://en.wikipedia.org/wiki/Gaokao\">GaoKao</a>): 619/750</h4>\n<h2 id=\"academic-projects\">Academic Projects</h2>\n<h4 id=\"bifurcation-and-oscillation-effects-of-gyrotactic-swimming-microorganism-suspension-in-vertical-pipe\">Bifurcation and Oscillation Effects of Gyrotactic Swimming Microorganism Suspension in Vertical Pipe</h4>\n<div style=\"margin-top:-2%; text-align: right\">\n05/2020-10/2020\n</div>\n<p>Imperial College London MSc Individual Project</p>\n<p>Director: <a href=\"https://www.imperial.ac.uk/people/y.hwang\">Dr. Yongyun Hwang</a></p>\n<ul>\n<li>Learning concepts of bioreactors and behaviours of micro-algae;</li>\n<li>Developing numerical solvers for microorganism suspension;</li>\n<li>Exploring and analysing bifurcations and instabilities under different flow conditions.</li>\n</ul>\n<h4 id=\"flow-field-analysis-based-on-rans-solver-and-biglobal-stability-theory\">Flow Field Analysis Based on RANS Solver and BiGlobal Stability Theory</h4>\n<div style=\"margin-top:-3%; text-align: right\">\n02/2019-06/2019\n</div>\n<p>Undergraduate Graduation Project &amp; <a href=\"https://erasmus-plus.ec.europa.eu/\">Erasmus+ Scholarship Programme</a></p>\n<p>Institutions: <a href=\"http://www.upm.es/internacional\">Polytechnic University of Madrid</a>&amp; <a href=\"https://en.nwpu.edu.cn/\">Northwestern Polytechnical University</a></p>\n<p>Director: <a href=\"http://www.upm.es/observatorio/vi/index.jsp?pageac=investigador.jsp&amp;idInvestigador=5214\">Professor Eusebio Valero Sanchez</a> &amp; <a href=\"https://teacher.nwpu.edu.cn/yaguo\">Associate Professor Yaguo Lyu</a></p>\n<ul>\n<li>Handling of the necessary tool, DLR TAU-Code;</li>\n<li>Performing simulations of non-stationary problems;</li>\n<li>Analysing the stability of the flow fields by POD and DMD.</li>\n</ul>\n<h4 id=\"design-research-on-bionic-anti-drag-propeller-project-manager\">Design Research on Bionic Anti-drag Propeller (Project Manager)</h4>\n<div style=\"margin-top:-3%; text-align: right\">\n04/2017-04/2018\n</div>\n<p><a href=\"https://www.pilcchina.org/?locale=en\">China college students “Internet+” Innovation and Entrepreneurship Competition</a></p>\n<p>Director: <a href=\"https://teacher.nwpu.edu.cn/wangyg.html\">Professor Wang yangang</a></p>\n<ul>\n<li>Proposing and design a novel drone propeller with serrated leading edge;</li>\n<li>Carrying out 3D modelling, CFD simulations and data analysis;</li>\n</ul>\n<h4 id=\"experimental-fluid-mechanics-summer-school\">Experimental Fluid Mechanics (Summer School)</h4>\n<div style=\"margin-top:-3%; text-align: right\">\n8/2018\n</div>\n<p><a href=\"https://odin.sdu.dk/sitecore/index.php?a=fagbesk&amp;id=79256&amp;lang=en\">International Scholarship Summer School Programme</a>, University of Southern Denmark</p>\n<ul>\n<li>Performing a team project to optimise a wind turbine airfoil;</li>\n<li>Data analysing of wind tunnel and water channel PIV results;</li>\n<li>Visiting the <a href=\"https://www.lmwindpower.com/en/services/technology-centers/test-and-validation-center-denmark\">LM Wind Power Test and Validation Centre</a>.</li>\n</ul>\n<h4 id=\"starting-test-of-pulse-jet-engine-in-pairs\">Starting Test of Pulse Jet Engine (In Pairs)</h4>\n<div style=\"margin-top:-3%; text-align: right\">\n07/2017-08/2017\n</div>\n<p>Scientific Research Practice Programme</p>\n<p>Director: <a href=\"https://teacher.nwpu.edu.cn/m/en/2010010152.html\">Professer Yan Hong</a></p>\n<ul>\n<li>Setting up the experiment platform;</li>\n<li>Measuring the thrust and pressure pulse frequency of a valveless pulse engine.</li>\n</ul>\n<h2 id=\"professional-skills\">Professional Skills</h2>\n<p>I.T. SKILLS: Python, C++</p>\n<p>Related software: OpenFOAM, CATIA</p>\n","wordcount":2429},{"title":"about","layout":"about","date":"2022-04-30T08:23:48.000Z","_content":"\n## This is\n\nThis is the author of ShouRou, \n\nCurrently an automotive CFD engineer in [Shanghai Volkswagen Automotive Co., Ltd](https://volkswagengroupchina.com.cn/en/partner/saicvolkswagen). \n\nGraduated from [Imperial College London](https://www.imperial.ac.uk/) with a degree in [MSc Advanced Computational Methods for Aeronautics](https://www.imperial.ac.uk/study/pg/aeronautics/computational-methods/)\n\n### Interested in\n\nComputational fluid dynamic\n\nDeep Learning\n\nCoding(Debugging) with Goldberg Variations\n\n###### More on my [CV](/CV/)\n\n","source":"about/index.md","raw":"---\ntitle: about\nlayout: about\ndate: 2022-04-30 16:23:48\n\n---\n\n## This is\n\nThis is the author of ShouRou, \n\nCurrently an automotive CFD engineer in [Shanghai Volkswagen Automotive Co., Ltd](https://volkswagengroupchina.com.cn/en/partner/saicvolkswagen). \n\nGraduated from [Imperial College London](https://www.imperial.ac.uk/) with a degree in [MSc Advanced Computational Methods for Aeronautics](https://www.imperial.ac.uk/study/pg/aeronautics/computational-methods/)\n\n### Interested in\n\nComputational fluid dynamic\n\nDeep Learning\n\nCoding(Debugging) with Goldberg Variations\n\n###### More on my [CV](/CV/)\n\n","updated":"2022-05-01T10:23:37.533Z","path":"about/index.html","comments":1,"_id":"cl2n5f4060002p9ybbs6h9qjh","content":"<h2 id=\"this-is\">This is</h2>\n<p>This is the author of ShouRou,</p>\n<p>Currently an automotive CFD engineer in <a href=\"https://volkswagengroupchina.com.cn/en/partner/saicvolkswagen\">Shanghai Volkswagen Automotive Co., Ltd</a>.</p>\n<p>Graduated from <a href=\"https://www.imperial.ac.uk/\">Imperial College London</a> with a degree in <a href=\"https://www.imperial.ac.uk/study/pg/aeronautics/computational-methods/\">MSc Advanced Computational Methods for Aeronautics</a></p>\n<h3 id=\"interested-in\">Interested in</h3>\n<p>Computational fluid dynamic</p>\n<p>Deep Learning</p>\n<p>Coding(Debugging) with Goldberg Variations</p>\n<h6 id=\"more-on-my-cv\">More on my <a href=\"/CV/\">CV</a></h6>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"this-is\">This is</h2>\n<p>This is the author of ShouRou,</p>\n<p>Currently an automotive CFD engineer in <a href=\"https://volkswagengroupchina.com.cn/en/partner/saicvolkswagen\">Shanghai Volkswagen Automotive Co., Ltd</a>.</p>\n<p>Graduated from <a href=\"https://www.imperial.ac.uk/\">Imperial College London</a> with a degree in <a href=\"https://www.imperial.ac.uk/study/pg/aeronautics/computational-methods/\">MSc Advanced Computational Methods for Aeronautics</a></p>\n<h3 id=\"interested-in\">Interested in</h3>\n<p>Computational fluid dynamic</p>\n<p>Deep Learning</p>\n<p>Coding(Debugging) with Goldberg Variations</p>\n<h6 id=\"more-on-my-cv\">More on my <a href=\"/CV/\">CV</a></h6>\n","wordcount":291}],"Post":[{"title":"Intro and Pytorch Implementation of Label Smoothing Regularization (LSR)","author":"Ryan LI","toc":true,"declare":true,"date":"2022-03-04T05:59:58.000Z","_content":"\n> Soft label is a commonly used trick to prevent overfitting. It can always gain some extra points on the image classification tasks. In this article, I have put together useful information from theory to implementation of it.\n\n> Recently, I joined a [Kaggle image classification competition](https://www.kaggle.com/c/classify-leaves/), I used the pretrained ResNet50 plus other tricks and here is to record some of them I've learned for now. \n\n\n<!-- more -->\n\n### Introduction: from hard label to soft label\n\nIn deep learning, the neural network is basically a super powerful non-linear regression machine aimed to fit a function between the input and the label. And the result is always called label.\n\nHard label, in another word:  the one-hot vector, is the most commonly type of label that is used. For example, in this [Kaggle image classification competition](https://www.kaggle.com/c/classify-leaves/), to digitalize the different name of the leaves, it is intuitive to encode the leaves categories as: 0, 1, 2, 3. And the factorized target labels would be somehow like [1,3,0...] where each element stands for the categories of the data. With the resulting category dictionary, it can be easily decoded after the training.\n\n<img src=\"leave%20class%20code.png\" alt=\"leave class code\" style=\"zoom:22%;\" />\n\nActually, there is a slightly difference in the binary world. What usually do is, the previously factorized label will be extended to be a 2-dimensional \"on-hot\" matrix where the elements stands for the probability of each class. And the network is aimed to train itself to make inference label nearest to the target label.\n\n<img src=\"hard%20label.png\" alt=\"hard label\" style=\"zoom:22%;\" />\n\nSoft label is just slightly deteriorate the strong one-hot label into a weaker one.\n\n<img src=\"soft%20label.png\" alt=\"soft label\" style=\"zoom:22%;\" />\n\n### Simple explanation: How loss function lost information?\n\nIn the cross entropy loss function, where `y_inference` and `y_grountruth` stands for inference and target label, n stands for the number of class.\n\n<img src=\"Cross%20entropy%20loss%20function.png\" alt=\"Cross entropy loss function\" style=\"zoom:22%;\" />\n\nWith the one-hot label, the components are 0 except for the true category. In a other word, the `y_inference` of the wrong category is not considered at all i.e. the information of the wrong category is lost. Which is against the real word classification. \n\n### Effectiveness: Visualization\n\nIn [When does label smoothing help?](https://arxiv.org/pdf/1906.02629.pdf)  Hinton shows the feature map difference between without and with LSR:\n\n<img src=\"Label%20smoothing%20feature%20norm.png\" alt=\"Label smoothing feature norm\" style=\"zoom:80%;\" />\n\n>- When label smoothing is applied, the clusters are much tighter because label smoothing encourages that each example in the training set is to be equidistant from all other class’s templates.\n>- With hard targets, the clusters for semantically similar classes (for example different breed of dogs in ImageNet), are isotropic whereas, with label smoothing, clusters lie in an arc as shown in the third row. If you mix two semantically similar classes with a third semantically different class, the clusters are still much better than the ones obtained with hard targets as shown in the fourth row.\n\n### Experiment: apply in competition\n\nLabel smoothing can be easily applied in [Tensorflow](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy), but there is no such thing in PyTorch.  So overwrite the Cross-entropy loss function with LSR (implemented in 2 ways): \n\n```python\nclass LSR(nn.Module):\n    \"\"\"NLL loss with label smoothing.\n    \"\"\"\n    def __init__(self, smoothing=0.0):\n        \"\"\"Constructor for the LSR module.\n        :param smoothing: label smoothing factor\n        \"\"\"\n        super(LSR, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n\n    def forward(self, x, target):\n        logprobs = torch.nn.functional.log_softmax(x, dim=-1)\n        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n        nll_loss = nll_loss.squeeze(1)\n        smooth_loss = -logprobs.mean(dim=-1)\n        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n        return loss.mean()\n    \nloss = LSR(0.1)\n```\n\n```python\nclass LSR2(nn.Module):\n\n    def __init__(self, e=0.01,reduction='mean'):\n        super().__init__()\n\n        self.log_softmax = nn.LogSoftmax(dim=1)\n        self.e = e\n        self.reduction = reduction\n\n    def _one_hot(self, labels, classes, value=1):\n        \"\"\"\n            Convert labels to one hot vectors\n\n        Args:\n            labels: torch tensor in format [label1, label2, label3, ...]\n            classes: int, number of classes\n            value: label value in one hot vector, default to 1\n\n        Returns:\n            return one hot format labels in shape [batchsize, classes]\n        \"\"\"\n        #print(\"classes\", classes)\n        one_hot = torch.zeros(labels.size(0), classes)\n\n        # labels and value_added  size must match\n        labels = labels.view(labels.size(0), -1)\n        value_added = torch.Tensor(labels.size(0), 1).fill_(value)\n\n        value_added = value_added.to(labels.device)\n        one_hot = one_hot.to(labels.device)\n\n        one_hot.scatter_add_(1, labels, value_added)\n\n        return one_hot\n\n    def _smooth_label(self, target, length, smooth_factor):\n        \"\"\"convert targets to one-hot format, and smooth\n        them.\n\n        Args:\n            target: target in form with [label1, label2, label_batchsize]\n            length: length of one-hot format(number of classes)\n            smooth_factor: smooth factor for label smooth\n\n        Returns:\n            smoothed labels in one hot format\n        \"\"\"\n        #print(\"length\", length)\n        #print(\"smooth_fact\", smooth_factor)\n        one_hot = self._one_hot(target, length, value=1 - smooth_factor)\n        one_hot += smooth_factor / length\n\n        return one_hot.to(target.device)\n\n    def forward(self, x, target):\n\n        if x.size(0) != target.size(0):\n            raise ValueError('Expected input batchsize ({}) to match target batch_size({})'\n                             .format(x.size(0), target.size(0)))\n\n        if x.dim() < 2:\n            raise ValueError('Expected input tensor to have least 2 dimensions(got {})'\n                             .format(x.size(0)))\n\n        if x.dim() != 2:\n            raise ValueError('Only 2 dimension tensor are implemented, (got {})'\n                             .format(x.size()))\n        #print(\"x: \", x)\n        #print(\"target\", target)\n\n        smoothed_target = self._smooth_label(target, x.size(1), self.e)\n        x = self.log_softmax(x)\n        loss = torch.sum(- x * smoothed_target, dim=1)\n        if self.reduction == 'none':\n            return loss\n\n        elif self.reduction == 'sum':\n            return torch.sum(loss)\n\n        elif self.reduction == 'mean':\n            return torch.mean(loss)\n\n        else:\n            raise ValueError('unrecognized option, expect reduction to be one of none, mean, sum')\n            \nloss = LSR2(0.1)\n```\n\nPretrained ResNet50 is in use\n\n```shell\nlr, num_epochs, batch_size = 0.01, 10, 256\n```\n\n<img src=\"accuracy%20curve%20compare%20label%20smoothing%20with%20hard%20label.png\" alt=\"accuracy curve compare label smoothing with hard label\" style=\"zoom:67%;\" />\n\nIt can bee seen that the under same `random seed`, `batch_size`, `lr`, and `num_epochs`, the overall accuracy has a fascinating rise of 0.5. \n\nThen apply the LSR and run 50 epochs, with learning rate 0.005 and batch size 256, the result turns to be:\n\n<img src=\"accuracy%20curve%20applying%20label%20smoothing.png\" alt=\"accuracy curve applying label smoothing\" style=\"zoom:67%;\" />\n\nIt is a exciting improvement, but more tricks still in need.\n\n### Conclusion\n\n3 disadvantaged of the hard label:\n\n- the relationship between the true label and the others is neglected, tend to be overfitting\n- the model is tend to be over confident i.e. less generalizable\n- more sensitive to label with noise, wrong labeled for example.\n\nSeveral good things about label smoothing:\n\n- data augmentation by add more information, compensates for the lack of supervisory signals \n- Improves generalizability\n- Improves noise robust\n- lower the feature norm\n- Improves model calibration \n\nBad things about label smoothing:\n\n-  label smoothing can't give real relationship between labels. It simply adds random noise, under fitting might happen under certain scenarios.\n- If distill in use, the teach network preforms worse when apply label smoothing, more explanation in  [When does label smoothing help?](https://arxiv.org/pdf/1906.02629.pdf) \n\n### Reference\n\n [标签平滑 - Label Smoothing概述 - 云+社区 - 腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/1815786) \n\n [大道至简：算法工程师炼丹Trick手册 (qq.com)](https://mp.weixin.qq.com/s?__biz=Mzg4MzU1NjQ2Mw==&mid=2247495228&idx=1&sn=ec685adcf8a274e8235c177718868a34&scene=21#wechat_redirect) \n\n[深度学习trick--labelsmooth](https://cloud.tencent.com/developer/article/1684298?from=article.detail.1815786)\n\n [Label Smoothing 标签平滑 (Label smooth regularization, LSR)_hxxjxw的博客-CSDN博客](https://blog.csdn.net/hxxjxw/article/details/115298103) \n\n [When Does Label Smoothing Help?](https://medium.com/@nainaakash012/when-does-label-smoothing-help-89654ec75326)\n\n[ suvojit-0x55aa](https://gist.github.com/suvojit-0x55aa)/[label_smoothing.py](https://gist.github.com/suvojit-0x55aa/0afb3eefbb26d33f54e1fb9f94d6b609)\n\n","source":"_posts/Intro-and-Pytorch-Implementation-of-Label-Smoothing-Regularization-LSR.md","raw":"---\ntitle: Intro and Pytorch Implementation of Label Smoothing Regularization (LSR)\nauthor: Ryan LI\ntoc: true\ndeclare: true\ndate: 2022-03-04 13:59:58\ntags:\n\t- deep learning\n\t- deep learning tricks\n---\n\n> Soft label is a commonly used trick to prevent overfitting. It can always gain some extra points on the image classification tasks. In this article, I have put together useful information from theory to implementation of it.\n\n> Recently, I joined a [Kaggle image classification competition](https://www.kaggle.com/c/classify-leaves/), I used the pretrained ResNet50 plus other tricks and here is to record some of them I've learned for now. \n\n\n<!-- more -->\n\n### Introduction: from hard label to soft label\n\nIn deep learning, the neural network is basically a super powerful non-linear regression machine aimed to fit a function between the input and the label. And the result is always called label.\n\nHard label, in another word:  the one-hot vector, is the most commonly type of label that is used. For example, in this [Kaggle image classification competition](https://www.kaggle.com/c/classify-leaves/), to digitalize the different name of the leaves, it is intuitive to encode the leaves categories as: 0, 1, 2, 3. And the factorized target labels would be somehow like [1,3,0...] where each element stands for the categories of the data. With the resulting category dictionary, it can be easily decoded after the training.\n\n<img src=\"leave%20class%20code.png\" alt=\"leave class code\" style=\"zoom:22%;\" />\n\nActually, there is a slightly difference in the binary world. What usually do is, the previously factorized label will be extended to be a 2-dimensional \"on-hot\" matrix where the elements stands for the probability of each class. And the network is aimed to train itself to make inference label nearest to the target label.\n\n<img src=\"hard%20label.png\" alt=\"hard label\" style=\"zoom:22%;\" />\n\nSoft label is just slightly deteriorate the strong one-hot label into a weaker one.\n\n<img src=\"soft%20label.png\" alt=\"soft label\" style=\"zoom:22%;\" />\n\n### Simple explanation: How loss function lost information?\n\nIn the cross entropy loss function, where `y_inference` and `y_grountruth` stands for inference and target label, n stands for the number of class.\n\n<img src=\"Cross%20entropy%20loss%20function.png\" alt=\"Cross entropy loss function\" style=\"zoom:22%;\" />\n\nWith the one-hot label, the components are 0 except for the true category. In a other word, the `y_inference` of the wrong category is not considered at all i.e. the information of the wrong category is lost. Which is against the real word classification. \n\n### Effectiveness: Visualization\n\nIn [When does label smoothing help?](https://arxiv.org/pdf/1906.02629.pdf)  Hinton shows the feature map difference between without and with LSR:\n\n<img src=\"Label%20smoothing%20feature%20norm.png\" alt=\"Label smoothing feature norm\" style=\"zoom:80%;\" />\n\n>- When label smoothing is applied, the clusters are much tighter because label smoothing encourages that each example in the training set is to be equidistant from all other class’s templates.\n>- With hard targets, the clusters for semantically similar classes (for example different breed of dogs in ImageNet), are isotropic whereas, with label smoothing, clusters lie in an arc as shown in the third row. If you mix two semantically similar classes with a third semantically different class, the clusters are still much better than the ones obtained with hard targets as shown in the fourth row.\n\n### Experiment: apply in competition\n\nLabel smoothing can be easily applied in [Tensorflow](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy), but there is no such thing in PyTorch.  So overwrite the Cross-entropy loss function with LSR (implemented in 2 ways): \n\n```python\nclass LSR(nn.Module):\n    \"\"\"NLL loss with label smoothing.\n    \"\"\"\n    def __init__(self, smoothing=0.0):\n        \"\"\"Constructor for the LSR module.\n        :param smoothing: label smoothing factor\n        \"\"\"\n        super(LSR, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n\n    def forward(self, x, target):\n        logprobs = torch.nn.functional.log_softmax(x, dim=-1)\n        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n        nll_loss = nll_loss.squeeze(1)\n        smooth_loss = -logprobs.mean(dim=-1)\n        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n        return loss.mean()\n    \nloss = LSR(0.1)\n```\n\n```python\nclass LSR2(nn.Module):\n\n    def __init__(self, e=0.01,reduction='mean'):\n        super().__init__()\n\n        self.log_softmax = nn.LogSoftmax(dim=1)\n        self.e = e\n        self.reduction = reduction\n\n    def _one_hot(self, labels, classes, value=1):\n        \"\"\"\n            Convert labels to one hot vectors\n\n        Args:\n            labels: torch tensor in format [label1, label2, label3, ...]\n            classes: int, number of classes\n            value: label value in one hot vector, default to 1\n\n        Returns:\n            return one hot format labels in shape [batchsize, classes]\n        \"\"\"\n        #print(\"classes\", classes)\n        one_hot = torch.zeros(labels.size(0), classes)\n\n        # labels and value_added  size must match\n        labels = labels.view(labels.size(0), -1)\n        value_added = torch.Tensor(labels.size(0), 1).fill_(value)\n\n        value_added = value_added.to(labels.device)\n        one_hot = one_hot.to(labels.device)\n\n        one_hot.scatter_add_(1, labels, value_added)\n\n        return one_hot\n\n    def _smooth_label(self, target, length, smooth_factor):\n        \"\"\"convert targets to one-hot format, and smooth\n        them.\n\n        Args:\n            target: target in form with [label1, label2, label_batchsize]\n            length: length of one-hot format(number of classes)\n            smooth_factor: smooth factor for label smooth\n\n        Returns:\n            smoothed labels in one hot format\n        \"\"\"\n        #print(\"length\", length)\n        #print(\"smooth_fact\", smooth_factor)\n        one_hot = self._one_hot(target, length, value=1 - smooth_factor)\n        one_hot += smooth_factor / length\n\n        return one_hot.to(target.device)\n\n    def forward(self, x, target):\n\n        if x.size(0) != target.size(0):\n            raise ValueError('Expected input batchsize ({}) to match target batch_size({})'\n                             .format(x.size(0), target.size(0)))\n\n        if x.dim() < 2:\n            raise ValueError('Expected input tensor to have least 2 dimensions(got {})'\n                             .format(x.size(0)))\n\n        if x.dim() != 2:\n            raise ValueError('Only 2 dimension tensor are implemented, (got {})'\n                             .format(x.size()))\n        #print(\"x: \", x)\n        #print(\"target\", target)\n\n        smoothed_target = self._smooth_label(target, x.size(1), self.e)\n        x = self.log_softmax(x)\n        loss = torch.sum(- x * smoothed_target, dim=1)\n        if self.reduction == 'none':\n            return loss\n\n        elif self.reduction == 'sum':\n            return torch.sum(loss)\n\n        elif self.reduction == 'mean':\n            return torch.mean(loss)\n\n        else:\n            raise ValueError('unrecognized option, expect reduction to be one of none, mean, sum')\n            \nloss = LSR2(0.1)\n```\n\nPretrained ResNet50 is in use\n\n```shell\nlr, num_epochs, batch_size = 0.01, 10, 256\n```\n\n<img src=\"accuracy%20curve%20compare%20label%20smoothing%20with%20hard%20label.png\" alt=\"accuracy curve compare label smoothing with hard label\" style=\"zoom:67%;\" />\n\nIt can bee seen that the under same `random seed`, `batch_size`, `lr`, and `num_epochs`, the overall accuracy has a fascinating rise of 0.5. \n\nThen apply the LSR and run 50 epochs, with learning rate 0.005 and batch size 256, the result turns to be:\n\n<img src=\"accuracy%20curve%20applying%20label%20smoothing.png\" alt=\"accuracy curve applying label smoothing\" style=\"zoom:67%;\" />\n\nIt is a exciting improvement, but more tricks still in need.\n\n### Conclusion\n\n3 disadvantaged of the hard label:\n\n- the relationship between the true label and the others is neglected, tend to be overfitting\n- the model is tend to be over confident i.e. less generalizable\n- more sensitive to label with noise, wrong labeled for example.\n\nSeveral good things about label smoothing:\n\n- data augmentation by add more information, compensates for the lack of supervisory signals \n- Improves generalizability\n- Improves noise robust\n- lower the feature norm\n- Improves model calibration \n\nBad things about label smoothing:\n\n-  label smoothing can't give real relationship between labels. It simply adds random noise, under fitting might happen under certain scenarios.\n- If distill in use, the teach network preforms worse when apply label smoothing, more explanation in  [When does label smoothing help?](https://arxiv.org/pdf/1906.02629.pdf) \n\n### Reference\n\n [标签平滑 - Label Smoothing概述 - 云+社区 - 腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/1815786) \n\n [大道至简：算法工程师炼丹Trick手册 (qq.com)](https://mp.weixin.qq.com/s?__biz=Mzg4MzU1NjQ2Mw==&mid=2247495228&idx=1&sn=ec685adcf8a274e8235c177718868a34&scene=21#wechat_redirect) \n\n[深度学习trick--labelsmooth](https://cloud.tencent.com/developer/article/1684298?from=article.detail.1815786)\n\n [Label Smoothing 标签平滑 (Label smooth regularization, LSR)_hxxjxw的博客-CSDN博客](https://blog.csdn.net/hxxjxw/article/details/115298103) \n\n [When Does Label Smoothing Help?](https://medium.com/@nainaakash012/when-does-label-smoothing-help-89654ec75326)\n\n[ suvojit-0x55aa](https://gist.github.com/suvojit-0x55aa)/[label_smoothing.py](https://gist.github.com/suvojit-0x55aa/0afb3eefbb26d33f54e1fb9f94d6b609)\n\n","slug":"Intro-and-Pytorch-Implementation-of-Label-Smoothing-Regularization-LSR","published":1,"updated":"2022-04-30T19:30:53.482Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl2n5f4040001p9yb876h7dm4","content":"<blockquote>\n<p>Soft label is a commonly used trick to prevent overfitting. It can always gain some extra points on the image classification tasks. In this article, I have put together useful information from theory to implementation of it.</p>\n</blockquote>\n<blockquote>\n<p>Recently, I joined a <a href=\"https://www.kaggle.com/c/classify-leaves/\">Kaggle image classification competition</a>, I used the pretrained ResNet50 plus other tricks and here is to record some of them I've learned for now.</p>\n</blockquote>\n<span id=\"more\"></span>\n<h3 id=\"introduction-from-hard-label-to-soft-label\">Introduction: from hard label to soft label</h3>\n<p>In deep learning, the neural network is basically a super powerful non-linear regression machine aimed to fit a function between the input and the label. And the result is always called label.</p>\n<p>Hard label, in another word: the one-hot vector, is the most commonly type of label that is used. For example, in this <a href=\"https://www.kaggle.com/c/classify-leaves/\">Kaggle image classification competition</a>, to digitalize the different name of the leaves, it is intuitive to encode the leaves categories as: 0, 1, 2, 3. And the factorized target labels would be somehow like [1,3,0...] where each element stands for the categories of the data. With the resulting category dictionary, it can be easily decoded after the training.</p>\n<p><img src=\"leave%20class%20code.png\" srcset=\"/img/loading.gif\" lazyload alt=\"leave class code\" style=\"zoom:22%;\" /></p>\n<p>Actually, there is a slightly difference in the binary world. What usually do is, the previously factorized label will be extended to be a 2-dimensional \"on-hot\" matrix where the elements stands for the probability of each class. And the network is aimed to train itself to make inference label nearest to the target label.</p>\n<p><img src=\"hard%20label.png\" srcset=\"/img/loading.gif\" lazyload alt=\"hard label\" style=\"zoom:22%;\" /></p>\n<p>Soft label is just slightly deteriorate the strong one-hot label into a weaker one.</p>\n<p><img src=\"soft%20label.png\" srcset=\"/img/loading.gif\" lazyload alt=\"soft label\" style=\"zoom:22%;\" /></p>\n<h3 id=\"simple-explanation-how-loss-function-lost-information\">Simple explanation: How loss function lost information?</h3>\n<p>In the cross entropy loss function, where <code>y_inference</code> and <code>y_grountruth</code> stands for inference and target label, n stands for the number of class.</p>\n<p><img src=\"Cross%20entropy%20loss%20function.png\" srcset=\"/img/loading.gif\" lazyload alt=\"Cross entropy loss function\" style=\"zoom:22%;\" /></p>\n<p>With the one-hot label, the components are 0 except for the true category. In a other word, the <code>y_inference</code> of the wrong category is not considered at all i.e. the information of the wrong category is lost. Which is against the real word classification.</p>\n<h3 id=\"effectiveness-visualization\">Effectiveness: Visualization</h3>\n<p>In <a href=\"https://arxiv.org/pdf/1906.02629.pdf\">When does label smoothing help?</a> Hinton shows the feature map difference between without and with LSR:</p>\n<p><img src=\"Label%20smoothing%20feature%20norm.png\" srcset=\"/img/loading.gif\" lazyload alt=\"Label smoothing feature norm\" style=\"zoom:80%;\" /></p>\n<blockquote>\n<ul>\n<li>When label smoothing is applied, the clusters are much tighter because label smoothing encourages that each example in the training set is to be equidistant from all other class’s templates.</li>\n<li>With hard targets, the clusters for semantically similar classes (for example different breed of dogs in ImageNet), are isotropic whereas, with label smoothing, clusters lie in an arc as shown in the third row. If you mix two semantically similar classes with a third semantically different class, the clusters are still much better than the ones obtained with hard targets as shown in the fourth row.</li>\n</ul>\n</blockquote>\n<h3 id=\"experiment-apply-in-competition\">Experiment: apply in competition</h3>\n<p>Label smoothing can be easily applied in <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy\">Tensorflow</a>, but there is no such thing in PyTorch. So overwrite the Cross-entropy loss function with LSR (implemented in 2 ways):</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">LSR</span>(nn.Module):\n    <span class=\"hljs-string\">&quot;&quot;&quot;NLL loss with label smoothing.</span>\n<span class=\"hljs-string\">    &quot;&quot;&quot;</span>\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, smoothing=<span class=\"hljs-number\">0.0</span></span>):\n        <span class=\"hljs-string\">&quot;&quot;&quot;Constructor for the LSR module.</span>\n<span class=\"hljs-string\">        :param smoothing: label smoothing factor</span>\n<span class=\"hljs-string\">        &quot;&quot;&quot;</span>\n        <span class=\"hljs-built_in\">super</span>(LSR, self).__init__()\n        self.confidence = <span class=\"hljs-number\">1.0</span> - smoothing\n        self.smoothing = smoothing\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">forward</span>(<span class=\"hljs-params\">self, x, target</span>):\n        logprobs = torch.nn.functional.log_softmax(x, dim=-<span class=\"hljs-number\">1</span>)\n        nll_loss = -logprobs.gather(dim=-<span class=\"hljs-number\">1</span>, index=target.unsqueeze(<span class=\"hljs-number\">1</span>))\n        nll_loss = nll_loss.squeeze(<span class=\"hljs-number\">1</span>)\n        smooth_loss = -logprobs.mean(dim=-<span class=\"hljs-number\">1</span>)\n        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n        <span class=\"hljs-keyword\">return</span> loss.mean()\n    \nloss = LSR(<span class=\"hljs-number\">0.1</span>)</code></pre></div>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">LSR2</span>(nn.Module):\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, e=<span class=\"hljs-number\">0.01</span>,reduction=<span class=\"hljs-string\">&#x27;mean&#x27;</span></span>):\n        <span class=\"hljs-built_in\">super</span>().__init__()\n\n        self.log_softmax = nn.LogSoftmax(dim=<span class=\"hljs-number\">1</span>)\n        self.e = e\n        self.reduction = reduction\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">_one_hot</span>(<span class=\"hljs-params\">self, labels, classes, value=<span class=\"hljs-number\">1</span></span>):\n        <span class=\"hljs-string\">&quot;&quot;&quot;</span>\n<span class=\"hljs-string\">            Convert labels to one hot vectors</span>\n<span class=\"hljs-string\"></span>\n<span class=\"hljs-string\">        Args:</span>\n<span class=\"hljs-string\">            labels: torch tensor in format [label1, label2, label3, ...]</span>\n<span class=\"hljs-string\">            classes: int, number of classes</span>\n<span class=\"hljs-string\">            value: label value in one hot vector, default to 1</span>\n<span class=\"hljs-string\"></span>\n<span class=\"hljs-string\">        Returns:</span>\n<span class=\"hljs-string\">            return one hot format labels in shape [batchsize, classes]</span>\n<span class=\"hljs-string\">        &quot;&quot;&quot;</span>\n        <span class=\"hljs-comment\">#print(&quot;classes&quot;, classes)</span>\n        one_hot = torch.zeros(labels.size(<span class=\"hljs-number\">0</span>), classes)\n\n        <span class=\"hljs-comment\"># labels and value_added  size must match</span>\n        labels = labels.view(labels.size(<span class=\"hljs-number\">0</span>), -<span class=\"hljs-number\">1</span>)\n        value_added = torch.Tensor(labels.size(<span class=\"hljs-number\">0</span>), <span class=\"hljs-number\">1</span>).fill_(value)\n\n        value_added = value_added.to(labels.device)\n        one_hot = one_hot.to(labels.device)\n\n        one_hot.scatter_add_(<span class=\"hljs-number\">1</span>, labels, value_added)\n\n        <span class=\"hljs-keyword\">return</span> one_hot\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">_smooth_label</span>(<span class=\"hljs-params\">self, target, length, smooth_factor</span>):\n        <span class=\"hljs-string\">&quot;&quot;&quot;convert targets to one-hot format, and smooth</span>\n<span class=\"hljs-string\">        them.</span>\n<span class=\"hljs-string\"></span>\n<span class=\"hljs-string\">        Args:</span>\n<span class=\"hljs-string\">            target: target in form with [label1, label2, label_batchsize]</span>\n<span class=\"hljs-string\">            length: length of one-hot format(number of classes)</span>\n<span class=\"hljs-string\">            smooth_factor: smooth factor for label smooth</span>\n<span class=\"hljs-string\"></span>\n<span class=\"hljs-string\">        Returns:</span>\n<span class=\"hljs-string\">            smoothed labels in one hot format</span>\n<span class=\"hljs-string\">        &quot;&quot;&quot;</span>\n        <span class=\"hljs-comment\">#print(&quot;length&quot;, length)</span>\n        <span class=\"hljs-comment\">#print(&quot;smooth_fact&quot;, smooth_factor)</span>\n        one_hot = self._one_hot(target, length, value=<span class=\"hljs-number\">1</span> - smooth_factor)\n        one_hot += smooth_factor / length\n\n        <span class=\"hljs-keyword\">return</span> one_hot.to(target.device)\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">forward</span>(<span class=\"hljs-params\">self, x, target</span>):\n\n        <span class=\"hljs-keyword\">if</span> x.size(<span class=\"hljs-number\">0</span>) != target.size(<span class=\"hljs-number\">0</span>):\n            <span class=\"hljs-keyword\">raise</span> ValueError(<span class=\"hljs-string\">&#x27;Expected input batchsize (&#123;&#125;) to match target batch_size(&#123;&#125;)&#x27;</span>\n                             .<span class=\"hljs-built_in\">format</span>(x.size(<span class=\"hljs-number\">0</span>), target.size(<span class=\"hljs-number\">0</span>)))\n\n        <span class=\"hljs-keyword\">if</span> x.dim() &lt; <span class=\"hljs-number\">2</span>:\n            <span class=\"hljs-keyword\">raise</span> ValueError(<span class=\"hljs-string\">&#x27;Expected input tensor to have least 2 dimensions(got &#123;&#125;)&#x27;</span>\n                             .<span class=\"hljs-built_in\">format</span>(x.size(<span class=\"hljs-number\">0</span>)))\n\n        <span class=\"hljs-keyword\">if</span> x.dim() != <span class=\"hljs-number\">2</span>:\n            <span class=\"hljs-keyword\">raise</span> ValueError(<span class=\"hljs-string\">&#x27;Only 2 dimension tensor are implemented, (got &#123;&#125;)&#x27;</span>\n                             .<span class=\"hljs-built_in\">format</span>(x.size()))\n        <span class=\"hljs-comment\">#print(&quot;x: &quot;, x)</span>\n        <span class=\"hljs-comment\">#print(&quot;target&quot;, target)</span>\n\n        smoothed_target = self._smooth_label(target, x.size(<span class=\"hljs-number\">1</span>), self.e)\n        x = self.log_softmax(x)\n        loss = torch.<span class=\"hljs-built_in\">sum</span>(- x * smoothed_target, dim=<span class=\"hljs-number\">1</span>)\n        <span class=\"hljs-keyword\">if</span> self.reduction == <span class=\"hljs-string\">&#x27;none&#x27;</span>:\n            <span class=\"hljs-keyword\">return</span> loss\n\n        <span class=\"hljs-keyword\">elif</span> self.reduction == <span class=\"hljs-string\">&#x27;sum&#x27;</span>:\n            <span class=\"hljs-keyword\">return</span> torch.<span class=\"hljs-built_in\">sum</span>(loss)\n\n        <span class=\"hljs-keyword\">elif</span> self.reduction == <span class=\"hljs-string\">&#x27;mean&#x27;</span>:\n            <span class=\"hljs-keyword\">return</span> torch.mean(loss)\n\n        <span class=\"hljs-keyword\">else</span>:\n            <span class=\"hljs-keyword\">raise</span> ValueError(<span class=\"hljs-string\">&#x27;unrecognized option, expect reduction to be one of none, mean, sum&#x27;</span>)\n            \nloss = LSR2(<span class=\"hljs-number\">0.1</span>)</code></pre></div>\n<p>Pretrained ResNet50 is in use</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs shell\">lr, num_epochs, batch_size = 0.01, 10, 256</code></pre></div>\n<p><img src=\"accuracy%20curve%20compare%20label%20smoothing%20with%20hard%20label.png\" srcset=\"/img/loading.gif\" lazyload alt=\"accuracy curve compare label smoothing with hard label\" style=\"zoom:67%;\" /></p>\n<p>It can bee seen that the under same <code>random seed</code>, <code>batch_size</code>, <code>lr</code>, and <code>num_epochs</code>, the overall accuracy has a fascinating rise of 0.5.</p>\n<p>Then apply the LSR and run 50 epochs, with learning rate 0.005 and batch size 256, the result turns to be:</p>\n<p><img src=\"accuracy%20curve%20applying%20label%20smoothing.png\" srcset=\"/img/loading.gif\" lazyload alt=\"accuracy curve applying label smoothing\" style=\"zoom:67%;\" /></p>\n<p>It is a exciting improvement, but more tricks still in need.</p>\n<h3 id=\"conclusion\">Conclusion</h3>\n<p>3 disadvantaged of the hard label:</p>\n<ul>\n<li>the relationship between the true label and the others is neglected, tend to be overfitting</li>\n<li>the model is tend to be over confident i.e. less generalizable</li>\n<li>more sensitive to label with noise, wrong labeled for example.</li>\n</ul>\n<p>Several good things about label smoothing:</p>\n<ul>\n<li>data augmentation by add more information, compensates for the lack of supervisory signals</li>\n<li>Improves generalizability</li>\n<li>Improves noise robust</li>\n<li>lower the feature norm</li>\n<li>Improves model calibration</li>\n</ul>\n<p>Bad things about label smoothing:</p>\n<ul>\n<li>label smoothing can't give real relationship between labels. It simply adds random noise, under fitting might happen under certain scenarios.</li>\n<li>If distill in use, the teach network preforms worse when apply label smoothing, more explanation in <a href=\"https://arxiv.org/pdf/1906.02629.pdf\">When does label smoothing help?</a></li>\n</ul>\n<h3 id=\"reference\">Reference</h3>\n<p><a href=\"https://cloud.tencent.com/developer/article/1815786\">标签平滑 - Label Smoothing概述 - 云+社区 - 腾讯云 (tencent.com)</a></p>\n<p><a href=\"https://mp.weixin.qq.com/s?__biz=Mzg4MzU1NjQ2Mw==&amp;mid=2247495228&amp;idx=1&amp;sn=ec685adcf8a274e8235c177718868a34&amp;scene=21#wechat_redirect\">大道至简：算法工程师炼丹Trick手册 (qq.com)</a></p>\n<p><a href=\"https://cloud.tencent.com/developer/article/1684298?from=article.detail.1815786\">深度学习trick--labelsmooth</a></p>\n<p><a href=\"https://blog.csdn.net/hxxjxw/article/details/115298103\">Label Smoothing 标签平滑 (Label smooth regularization, LSR)_hxxjxw的博客-CSDN博客</a></p>\n<p><a href=\"https://medium.com/@nainaakash012/when-does-label-smoothing-help-89654ec75326\">When Does Label Smoothing Help?</a></p>\n<p><a href=\"https://gist.github.com/suvojit-0x55aa\">suvojit-0x55aa</a>/<a href=\"https://gist.github.com/suvojit-0x55aa/0afb3eefbb26d33f54e1fb9f94d6b609\">label_smoothing.py</a></p>\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>Soft label is a commonly used trick to prevent overfitting. It can always gain some extra points on the image classification tasks. In this article, I have put together useful information from theory to implementation of it.</p>\n</blockquote>\n<blockquote>\n<p>Recently, I joined a <a href=\"https://www.kaggle.com/c/classify-leaves/\">Kaggle image classification competition</a>, I used the pretrained ResNet50 plus other tricks and here is to record some of them I've learned for now.</p>\n</blockquote>","more":"<h3 id=\"introduction-from-hard-label-to-soft-label\">Introduction: from hard label to soft label</h3>\n<p>In deep learning, the neural network is basically a super powerful non-linear regression machine aimed to fit a function between the input and the label. And the result is always called label.</p>\n<p>Hard label, in another word: the one-hot vector, is the most commonly type of label that is used. For example, in this <a href=\"https://www.kaggle.com/c/classify-leaves/\">Kaggle image classification competition</a>, to digitalize the different name of the leaves, it is intuitive to encode the leaves categories as: 0, 1, 2, 3. And the factorized target labels would be somehow like [1,3,0...] where each element stands for the categories of the data. With the resulting category dictionary, it can be easily decoded after the training.</p>\n<p><img src=\"leave%20class%20code.png\" alt=\"leave class code\" style=\"zoom:22%;\" /></p>\n<p>Actually, there is a slightly difference in the binary world. What usually do is, the previously factorized label will be extended to be a 2-dimensional \"on-hot\" matrix where the elements stands for the probability of each class. And the network is aimed to train itself to make inference label nearest to the target label.</p>\n<p><img src=\"hard%20label.png\" alt=\"hard label\" style=\"zoom:22%;\" /></p>\n<p>Soft label is just slightly deteriorate the strong one-hot label into a weaker one.</p>\n<p><img src=\"soft%20label.png\" alt=\"soft label\" style=\"zoom:22%;\" /></p>\n<h3 id=\"simple-explanation-how-loss-function-lost-information\">Simple explanation: How loss function lost information?</h3>\n<p>In the cross entropy loss function, where <code>y_inference</code> and <code>y_grountruth</code> stands for inference and target label, n stands for the number of class.</p>\n<p><img src=\"Cross%20entropy%20loss%20function.png\" alt=\"Cross entropy loss function\" style=\"zoom:22%;\" /></p>\n<p>With the one-hot label, the components are 0 except for the true category. In a other word, the <code>y_inference</code> of the wrong category is not considered at all i.e. the information of the wrong category is lost. Which is against the real word classification.</p>\n<h3 id=\"effectiveness-visualization\">Effectiveness: Visualization</h3>\n<p>In <a href=\"https://arxiv.org/pdf/1906.02629.pdf\">When does label smoothing help?</a> Hinton shows the feature map difference between without and with LSR:</p>\n<p><img src=\"Label%20smoothing%20feature%20norm.png\" alt=\"Label smoothing feature norm\" style=\"zoom:80%;\" /></p>\n<blockquote>\n<ul>\n<li>When label smoothing is applied, the clusters are much tighter because label smoothing encourages that each example in the training set is to be equidistant from all other class’s templates.</li>\n<li>With hard targets, the clusters for semantically similar classes (for example different breed of dogs in ImageNet), are isotropic whereas, with label smoothing, clusters lie in an arc as shown in the third row. If you mix two semantically similar classes with a third semantically different class, the clusters are still much better than the ones obtained with hard targets as shown in the fourth row.</li>\n</ul>\n</blockquote>\n<h3 id=\"experiment-apply-in-competition\">Experiment: apply in competition</h3>\n<p>Label smoothing can be easily applied in <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy\">Tensorflow</a>, but there is no such thing in PyTorch. So overwrite the Cross-entropy loss function with LSR (implemented in 2 ways):</p>\n<pre><code class=\"hljs python\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">LSR</span>(nn.Module):\n    <span class=\"hljs-string\">&quot;&quot;&quot;NLL loss with label smoothing.</span>\n<span class=\"hljs-string\">    &quot;&quot;&quot;</span>\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, smoothing=<span class=\"hljs-number\">0.0</span></span>):\n        <span class=\"hljs-string\">&quot;&quot;&quot;Constructor for the LSR module.</span>\n<span class=\"hljs-string\">        :param smoothing: label smoothing factor</span>\n<span class=\"hljs-string\">        &quot;&quot;&quot;</span>\n        <span class=\"hljs-built_in\">super</span>(LSR, self).__init__()\n        self.confidence = <span class=\"hljs-number\">1.0</span> - smoothing\n        self.smoothing = smoothing\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">forward</span>(<span class=\"hljs-params\">self, x, target</span>):\n        logprobs = torch.nn.functional.log_softmax(x, dim=-<span class=\"hljs-number\">1</span>)\n        nll_loss = -logprobs.gather(dim=-<span class=\"hljs-number\">1</span>, index=target.unsqueeze(<span class=\"hljs-number\">1</span>))\n        nll_loss = nll_loss.squeeze(<span class=\"hljs-number\">1</span>)\n        smooth_loss = -logprobs.mean(dim=-<span class=\"hljs-number\">1</span>)\n        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n        <span class=\"hljs-keyword\">return</span> loss.mean()\n    \nloss = LSR(<span class=\"hljs-number\">0.1</span>)</code></pre>\n<pre><code class=\"hljs python\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">LSR2</span>(nn.Module):\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, e=<span class=\"hljs-number\">0.01</span>,reduction=<span class=\"hljs-string\">&#x27;mean&#x27;</span></span>):\n        <span class=\"hljs-built_in\">super</span>().__init__()\n\n        self.log_softmax = nn.LogSoftmax(dim=<span class=\"hljs-number\">1</span>)\n        self.e = e\n        self.reduction = reduction\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">_one_hot</span>(<span class=\"hljs-params\">self, labels, classes, value=<span class=\"hljs-number\">1</span></span>):\n        <span class=\"hljs-string\">&quot;&quot;&quot;</span>\n<span class=\"hljs-string\">            Convert labels to one hot vectors</span>\n<span class=\"hljs-string\"></span>\n<span class=\"hljs-string\">        Args:</span>\n<span class=\"hljs-string\">            labels: torch tensor in format [label1, label2, label3, ...]</span>\n<span class=\"hljs-string\">            classes: int, number of classes</span>\n<span class=\"hljs-string\">            value: label value in one hot vector, default to 1</span>\n<span class=\"hljs-string\"></span>\n<span class=\"hljs-string\">        Returns:</span>\n<span class=\"hljs-string\">            return one hot format labels in shape [batchsize, classes]</span>\n<span class=\"hljs-string\">        &quot;&quot;&quot;</span>\n        <span class=\"hljs-comment\">#print(&quot;classes&quot;, classes)</span>\n        one_hot = torch.zeros(labels.size(<span class=\"hljs-number\">0</span>), classes)\n\n        <span class=\"hljs-comment\"># labels and value_added  size must match</span>\n        labels = labels.view(labels.size(<span class=\"hljs-number\">0</span>), -<span class=\"hljs-number\">1</span>)\n        value_added = torch.Tensor(labels.size(<span class=\"hljs-number\">0</span>), <span class=\"hljs-number\">1</span>).fill_(value)\n\n        value_added = value_added.to(labels.device)\n        one_hot = one_hot.to(labels.device)\n\n        one_hot.scatter_add_(<span class=\"hljs-number\">1</span>, labels, value_added)\n\n        <span class=\"hljs-keyword\">return</span> one_hot\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">_smooth_label</span>(<span class=\"hljs-params\">self, target, length, smooth_factor</span>):\n        <span class=\"hljs-string\">&quot;&quot;&quot;convert targets to one-hot format, and smooth</span>\n<span class=\"hljs-string\">        them.</span>\n<span class=\"hljs-string\"></span>\n<span class=\"hljs-string\">        Args:</span>\n<span class=\"hljs-string\">            target: target in form with [label1, label2, label_batchsize]</span>\n<span class=\"hljs-string\">            length: length of one-hot format(number of classes)</span>\n<span class=\"hljs-string\">            smooth_factor: smooth factor for label smooth</span>\n<span class=\"hljs-string\"></span>\n<span class=\"hljs-string\">        Returns:</span>\n<span class=\"hljs-string\">            smoothed labels in one hot format</span>\n<span class=\"hljs-string\">        &quot;&quot;&quot;</span>\n        <span class=\"hljs-comment\">#print(&quot;length&quot;, length)</span>\n        <span class=\"hljs-comment\">#print(&quot;smooth_fact&quot;, smooth_factor)</span>\n        one_hot = self._one_hot(target, length, value=<span class=\"hljs-number\">1</span> - smooth_factor)\n        one_hot += smooth_factor / length\n\n        <span class=\"hljs-keyword\">return</span> one_hot.to(target.device)\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">forward</span>(<span class=\"hljs-params\">self, x, target</span>):\n\n        <span class=\"hljs-keyword\">if</span> x.size(<span class=\"hljs-number\">0</span>) != target.size(<span class=\"hljs-number\">0</span>):\n            <span class=\"hljs-keyword\">raise</span> ValueError(<span class=\"hljs-string\">&#x27;Expected input batchsize (&#123;&#125;) to match target batch_size(&#123;&#125;)&#x27;</span>\n                             .<span class=\"hljs-built_in\">format</span>(x.size(<span class=\"hljs-number\">0</span>), target.size(<span class=\"hljs-number\">0</span>)))\n\n        <span class=\"hljs-keyword\">if</span> x.dim() &lt; <span class=\"hljs-number\">2</span>:\n            <span class=\"hljs-keyword\">raise</span> ValueError(<span class=\"hljs-string\">&#x27;Expected input tensor to have least 2 dimensions(got &#123;&#125;)&#x27;</span>\n                             .<span class=\"hljs-built_in\">format</span>(x.size(<span class=\"hljs-number\">0</span>)))\n\n        <span class=\"hljs-keyword\">if</span> x.dim() != <span class=\"hljs-number\">2</span>:\n            <span class=\"hljs-keyword\">raise</span> ValueError(<span class=\"hljs-string\">&#x27;Only 2 dimension tensor are implemented, (got &#123;&#125;)&#x27;</span>\n                             .<span class=\"hljs-built_in\">format</span>(x.size()))\n        <span class=\"hljs-comment\">#print(&quot;x: &quot;, x)</span>\n        <span class=\"hljs-comment\">#print(&quot;target&quot;, target)</span>\n\n        smoothed_target = self._smooth_label(target, x.size(<span class=\"hljs-number\">1</span>), self.e)\n        x = self.log_softmax(x)\n        loss = torch.<span class=\"hljs-built_in\">sum</span>(- x * smoothed_target, dim=<span class=\"hljs-number\">1</span>)\n        <span class=\"hljs-keyword\">if</span> self.reduction == <span class=\"hljs-string\">&#x27;none&#x27;</span>:\n            <span class=\"hljs-keyword\">return</span> loss\n\n        <span class=\"hljs-keyword\">elif</span> self.reduction == <span class=\"hljs-string\">&#x27;sum&#x27;</span>:\n            <span class=\"hljs-keyword\">return</span> torch.<span class=\"hljs-built_in\">sum</span>(loss)\n\n        <span class=\"hljs-keyword\">elif</span> self.reduction == <span class=\"hljs-string\">&#x27;mean&#x27;</span>:\n            <span class=\"hljs-keyword\">return</span> torch.mean(loss)\n\n        <span class=\"hljs-keyword\">else</span>:\n            <span class=\"hljs-keyword\">raise</span> ValueError(<span class=\"hljs-string\">&#x27;unrecognized option, expect reduction to be one of none, mean, sum&#x27;</span>)\n            \nloss = LSR2(<span class=\"hljs-number\">0.1</span>)</code></pre>\n<p>Pretrained ResNet50 is in use</p>\n<pre><code class=\"hljs shell\">lr, num_epochs, batch_size = 0.01, 10, 256</code></pre>\n<p><img src=\"accuracy%20curve%20compare%20label%20smoothing%20with%20hard%20label.png\" alt=\"accuracy curve compare label smoothing with hard label\" style=\"zoom:67%;\" /></p>\n<p>It can bee seen that the under same <code>random seed</code>, <code>batch_size</code>, <code>lr</code>, and <code>num_epochs</code>, the overall accuracy has a fascinating rise of 0.5.</p>\n<p>Then apply the LSR and run 50 epochs, with learning rate 0.005 and batch size 256, the result turns to be:</p>\n<p><img src=\"accuracy%20curve%20applying%20label%20smoothing.png\" alt=\"accuracy curve applying label smoothing\" style=\"zoom:67%;\" /></p>\n<p>It is a exciting improvement, but more tricks still in need.</p>\n<h3 id=\"conclusion\">Conclusion</h3>\n<p>3 disadvantaged of the hard label:</p>\n<ul>\n<li>the relationship between the true label and the others is neglected, tend to be overfitting</li>\n<li>the model is tend to be over confident i.e. less generalizable</li>\n<li>more sensitive to label with noise, wrong labeled for example.</li>\n</ul>\n<p>Several good things about label smoothing:</p>\n<ul>\n<li>data augmentation by add more information, compensates for the lack of supervisory signals</li>\n<li>Improves generalizability</li>\n<li>Improves noise robust</li>\n<li>lower the feature norm</li>\n<li>Improves model calibration</li>\n</ul>\n<p>Bad things about label smoothing:</p>\n<ul>\n<li>label smoothing can't give real relationship between labels. It simply adds random noise, under fitting might happen under certain scenarios.</li>\n<li>If distill in use, the teach network preforms worse when apply label smoothing, more explanation in <a href=\"https://arxiv.org/pdf/1906.02629.pdf\">When does label smoothing help?</a></li>\n</ul>\n<h3 id=\"reference\">Reference</h3>\n<p><a href=\"https://cloud.tencent.com/developer/article/1815786\">标签平滑 - Label Smoothing概述 - 云+社区 - 腾讯云 (tencent.com)</a></p>\n<p><a href=\"https://mp.weixin.qq.com/s?__biz=Mzg4MzU1NjQ2Mw==&amp;mid=2247495228&amp;idx=1&amp;sn=ec685adcf8a274e8235c177718868a34&amp;scene=21#wechat_redirect\">大道至简：算法工程师炼丹Trick手册 (qq.com)</a></p>\n<p><a href=\"https://cloud.tencent.com/developer/article/1684298?from=article.detail.1815786\">深度学习trick--labelsmooth</a></p>\n<p><a href=\"https://blog.csdn.net/hxxjxw/article/details/115298103\">Label Smoothing 标签平滑 (Label smooth regularization, LSR)_hxxjxw的博客-CSDN博客</a></p>\n<p><a href=\"https://medium.com/@nainaakash012/when-does-label-smoothing-help-89654ec75326\">When Does Label Smoothing Help?</a></p>\n<p><a href=\"https://gist.github.com/suvojit-0x55aa\">suvojit-0x55aa</a>/<a href=\"https://gist.github.com/suvojit-0x55aa/0afb3eefbb26d33f54e1fb9f94d6b609\">label_smoothing.py</a></p>","wordcount":6374},{"title":"Hello ShouRou","date":"2022-02-22T02:44:29.000Z","_content":"\n### Welcome\n\nThis is the first blog on ShouRou. The name of the website is extracted from the cute nikenames between a pair of good-looking lovers.\n\nShoushou the dumb and his girl Rourou the clever, cute and sexy will start up magnifisent careers of each own and be together as a happy couple.\n\n","source":"_posts/Hello-ShouRou.md","raw":"---\ntitle: Hello ShouRou\ndate: 2022-02-22 10:44:29\ntags: First Blog\n---\n\n### Welcome\n\nThis is the first blog on ShouRou. The name of the website is extracted from the cute nikenames between a pair of good-looking lovers.\n\nShoushou the dumb and his girl Rourou the clever, cute and sexy will start up magnifisent careers of each own and be together as a happy couple.\n\n","slug":"Hello-ShouRou","published":1,"updated":"2022-02-24T02:22:17.294Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl2n5f4060003p9ybc2ke3ys7","content":"<h3 id=\"welcome\">Welcome</h3>\n<p>This is the first blog on ShouRou. The name of the website is extracted from the cute nikenames between a pair of good-looking lovers.</p>\n<p>Shoushou the dumb and his girl Rourou the clever, cute and sexy will start up magnifisent careers of each own and be together as a happy couple.</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"welcome\">Welcome</h3>\n<p>This is the first blog on ShouRou. The name of the website is extracted from the cute nikenames between a pair of good-looking lovers.</p>\n<p>Shoushou the dumb and his girl Rourou the clever, cute and sexy will start up magnifisent careers of each own and be together as a happy couple.</p>\n","wordcount":236},{"title":"Switch blog theme to FLUID","author":"Ryan LI","toc":true,"declare":true,"date":"2022-04-30T08:05:02.000Z","_content":"\n> The former \"yilia\" theme starts to be buggy since it was no longer maintained. I switch to this \"FUILD\" theme, for now, hopefully it will stand.\n\n<!-- more -->\n\n### Reference docs\n\n[Docs](https://hexo.fluid-dev.com/docs/en/), [Preview](https://hexo.fluid-dev.com/posts/fluid-hitokoto/), [Github repo](https://github.com/fluid-dev/hexo-theme-fluid)\n\n### Switch theme to Fluid\n\n```shell\nnpm install --save hexo-theme-fluid\n```\n\nEdit `_config.yml` in the blog root directory as follows:\n\n```yaml\ntheme: fluid\n```\n\nCreate the about page manually:\n\n```bash\nhexo new page about\n```\n\nThen edit `/source/about/index.md` and add `layout` attribute.\n\nExecute the command in your blog directory：\n\n```bash\nnpm update --save hexo-theme-fluid\n```\n\n### Customise\n\ncreate `_config.fluid.yml` in the blog directory and copy the content of [_config.yml](https://github.com/fluid-dev/hexo-theme-fluid/blob/master/_config.yml)\n\n","source":"_posts/Switch-blog-theme-to-FLUID.md","raw":"---\ntitle: Switch blog theme to FLUID\nauthor: Ryan LI\ntoc: true\ndeclare: true\ndate: 2022-04-30 16:05:02\ntags:\n  - hexo\n  - blog\n---\n\n> The former \"yilia\" theme starts to be buggy since it was no longer maintained. I switch to this \"FUILD\" theme, for now, hopefully it will stand.\n\n<!-- more -->\n\n### Reference docs\n\n[Docs](https://hexo.fluid-dev.com/docs/en/), [Preview](https://hexo.fluid-dev.com/posts/fluid-hitokoto/), [Github repo](https://github.com/fluid-dev/hexo-theme-fluid)\n\n### Switch theme to Fluid\n\n```shell\nnpm install --save hexo-theme-fluid\n```\n\nEdit `_config.yml` in the blog root directory as follows:\n\n```yaml\ntheme: fluid\n```\n\nCreate the about page manually:\n\n```bash\nhexo new page about\n```\n\nThen edit `/source/about/index.md` and add `layout` attribute.\n\nExecute the command in your blog directory：\n\n```bash\nnpm update --save hexo-theme-fluid\n```\n\n### Customise\n\ncreate `_config.fluid.yml` in the blog directory and copy the content of [_config.yml](https://github.com/fluid-dev/hexo-theme-fluid/blob/master/_config.yml)\n\n","slug":"Switch-blog-theme-to-FLUID","published":1,"updated":"2022-04-30T19:31:33.652Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl2n5f4070005p9yb48u836hy","content":"<blockquote>\n<p>The former \"yilia\" theme starts to be buggy since it was no longer maintained. I switch to this \"FUILD\" theme, for now, hopefully it will stand.</p>\n</blockquote>\n<span id=\"more\"></span>\n<h3 id=\"reference-docs\">Reference docs</h3>\n<p><a href=\"https://hexo.fluid-dev.com/docs/en/\">Docs</a>, <a href=\"https://hexo.fluid-dev.com/posts/fluid-hitokoto/\">Preview</a>, <a href=\"https://github.com/fluid-dev/hexo-theme-fluid\">Github repo</a></p>\n<h3 id=\"switch-theme-to-fluid\">Switch theme to Fluid</h3>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs shell\">npm install --save hexo-theme-fluid</code></pre></div>\n<p>Edit <code>_config.yml</code> in the blog root directory as follows:</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs yaml\"><span class=\"hljs-attr\">theme:</span> <span class=\"hljs-string\">fluid</span></code></pre></div>\n<p>Create the about page manually:</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs bash\">hexo new page about</code></pre></div>\n<p>Then edit <code>/source/about/index.md</code> and add <code>layout</code> attribute.</p>\n<p>Execute the command in your blog directory：</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs bash\">npm update --save hexo-theme-fluid</code></pre></div>\n<h3 id=\"customise\">Customise</h3>\n<p>create <code>_config.fluid.yml</code> in the blog directory and copy the content of <a href=\"https://github.com/fluid-dev/hexo-theme-fluid/blob/master/_config.yml\">_config.yml</a></p>\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>The former \"yilia\" theme starts to be buggy since it was no longer maintained. I switch to this \"FUILD\" theme, for now, hopefully it will stand.</p>\n</blockquote>","more":"<h3 id=\"reference-docs\">Reference docs</h3>\n<p><a href=\"https://hexo.fluid-dev.com/docs/en/\">Docs</a>, <a href=\"https://hexo.fluid-dev.com/posts/fluid-hitokoto/\">Preview</a>, <a href=\"https://github.com/fluid-dev/hexo-theme-fluid\">Github repo</a></p>\n<h3 id=\"switch-theme-to-fluid\">Switch theme to Fluid</h3>\n<pre><code class=\"hljs shell\">npm install --save hexo-theme-fluid</code></pre>\n<p>Edit <code>_config.yml</code> in the blog root directory as follows:</p>\n<pre><code class=\"hljs yaml\"><span class=\"hljs-attr\">theme:</span> <span class=\"hljs-string\">fluid</span></code></pre>\n<p>Create the about page manually:</p>\n<pre><code class=\"hljs bash\">hexo new page about</code></pre>\n<p>Then edit <code>/source/about/index.md</code> and add <code>layout</code> attribute.</p>\n<p>Execute the command in your blog directory：</p>\n<pre><code class=\"hljs bash\">npm update --save hexo-theme-fluid</code></pre>\n<h3 id=\"customise\">Customise</h3>\n<p>create <code>_config.fluid.yml</code> in the blog directory and copy the content of <a href=\"https://github.com/fluid-dev/hexo-theme-fluid/blob/master/_config.yml\">_config.yml</a></p>","wordcount":506},{"title":"Build and configure a personal blog via hexo and yilia on github","date":"2022-02-22T08:15:03.000Z","toc":true,"declare":true,"_content":"\n> Technical blog has a hundred benefits and no harm\n>\n> This blog records the process of building and customizing this personal blog from 0 to 1\n\n> Unfortunately, the yilia theme has been no longer updating and it is too buggy right now. I switch to other theme.\n\n<!-- more -->\n\n## Preliminary\n\n### Why personal blog\n\n> Keeping a technical blog can be **a great way of documenting your growth as a developer**. This documentation can be particularly useful on a professional level. All software companies want to hire smart, thoughtful, communicative developers who can easily assimilate into a team, and who are ready to both teach and learn\n\n### Static vs dynamic blog\n\nthere are 2 types of mainstream personal blog: static and dynamic.\n\nStatic is recommended considering its simplicity, 0 maintenance and 0 safety worry. \n\n|              | Static blog                                                  | Dynamic blog                                                 |\n| :----------- | :----------------------------------------------------------- | :----------------------------------------------------------- |\n| Price        | low, 0 cost when the traffic is relatively low               | High, server is needed, cloud server of  high performance is usually very expensive. |\n| Features     | Limited, only third-party services can be used to complete certain \"dynamic\" functions, such as comments | Rich, in WordPress for example, basically any kind of plugins can be found. Featuers such as auto-resizing, media players, multiple authors, scheduled posts, user analysis can be easily realize. |\n| Speed        | Fast                                                         | Slow                                                         |\n| Maintainance | 0                                                            | Need to care about the sever                                 |\n| Markdown     | Supported yet it's the only choice                           | not supported                                                |\n| Geeky        | YES                                                          | No                                                           |\n\nAnd I also chose static because I'm geeky (poor of money) and results-driven (lazy to spend time on maintaining).\n\n## Build a static blog via hexo\n\n### Set environment\n\n1.check machine information: macOS on M1 MacBook\n\n2.Install Nodejs, including node and npm\n\nopen  https://nodejs.org/en/download/ and click download\n\n<img src=\"node js install.png\" alt=\"node js install\" style=\"zoom:50%;\" />\n\n3.Install Git\n\nAready installed\n\n4.Open terminal, check node, npm and git versions\n\n```shell\n$ npm -v\n$ node -v\n$ git --version\n8.3.1\nv16.14.0\ngit version 2.32.0 (Apple Git-132)\n```\n\n### Initialize blog\n\n1.install hexo via npm\n\n```shell\n$ sudo npm install -g hexo-cli\n$ hexo -v\nINFO  Validating config\nhexo: 6.0.0\nhexo-cli: 4.3.0\nos: darwin 21.2.0 12.1\n\nnode: 16.14.0\nv8: 9.4.146.24-node.20\nuv: 1.43.0\nzlib: 1.2.11\nbrotli: 1.0.9\nares: 1.18.1\nmodules: 93\nnghttp2: 1.45.1\nnapi: 8\nllhttp: 6.0.4\nopenssl: 1.1.1m+quic\ncldr: 40.0\nicu: 70.1\ntz: 2021a3\nunicode: 14.0\nngtcp2: 0.1.0-DEV\nnghttp3: 0.1.0-DEV\n```\n\n2.create a new folder in terminal and initialize the blog\n\n```shell\n$ cd ~/Documents/\n$ makedir self_blog\n$ cd self_blog/\n$ hexo init\nINFO  Cloning hexo-starter https://github.com/hexojs/hexo-starter.gitINFO  Install dependencies⸨#########⠂⠂⠂⠂⠂⠂⠂⠂⠂⸩ ⠹ idealTree:hexo-front-matter: timing idealTree:node_modules/hexo-front-matter Completed in 212msINFO  Start blogging with Hexo! \n```\n\n3.view the blog on localhost, s for start\n\n```shell\n$ hexo s\nINFO  Validating config\nINFO  Start processing\nINFO  Hexo is running at http://localhost:4000/ . Press Ctrl+C to stop.\n```\n\n\n### Write first blog\n\n1.write a new blog, n for new\n\n```shell\n$ hexo n 'Hello ShouRou'\nINFO  Validating config\nINFO  Created: ~/Documents/self_blog/source/_posts/Hello-ShouRou.md\n```\n\nthe blog can be written on any editor, Typora in use.\n\n```shell\nopen ~/Documents/self_blog/source/_posts/Hello-ShouRou.md\n```\n\n2.clean cache(not necessary)\n\n```shell\n$ hexo clean\n```\n\n3.generate the blog, g for generate\n\n```shell\n$ hexo g\nINFO  Validating config\nINFO  Start processing\nINFO  Files loaded in 61 ms\n(node:10719) Warning: Accessing non-existent property 'lineno' of module exports inside circular dependency\n(Use `node --trace-warnings ...` to show where the warning was created)\n(node:10719) Warning: Accessing non-existent property 'column' of module exports inside circular dependency\n(node:10719) Warning: Accessing non-existent property 'filename' of module exports inside circular dependency\n(node:10719) Warning: Accessing non-existent property 'lineno' of module exports inside circular dependency\n(node:10719) Warning: Accessing non-existent property 'column' of module exports inside circular dependency\n(node:10719) Warning: Accessing non-existent property 'filename' of module exports inside circular dependency\nINFO  Generated: archives/2022/index.html\nINFO  Generated: archives/index.html\nINFO  Generated: js/script.js\nINFO  Generated: fancybox/jquery.fancybox.min.css\nINFO  Generated: index.html\nINFO  Generated: css/style.css\nINFO  Generated: css/fonts/fontawesome-webfont.woff2\nINFO  Generated: fancybox/jquery.fancybox.min.js\nINFO  Generated: js/jquery-3.4.1.min.js\nINFO  Generated: archives/2022/02/index.html\nINFO  Generated: css/fonts/FontAwesome.otf\nINFO  Generated: css/fonts/fontawesome-webfont.woff\nINFO  Generated: css/fonts/fontawesome-webfont.eot\nINFO  Generated: css/fonts/fontawesome-webfont.ttf\nINFO  Generated: css/images/banner.jpg\nINFO  Generated: 2022/02/22/hello-world/index.html\nINFO  Generated: css/fonts/fontawesome-webfont.svg\nINFO  Generated: 2022/02/22/Hello-ShouRou/index.html\nINFO  18 files generated in 161 ms\n```\n\n### Deploy to remote (GitHub)\n\n1.Create a new repository with the name of $username.github.io\n\n<img src=\"github page.png\" alt=\"github page\" style=\"zoom:50%;\" />\n\nuse default setting\n\n2.open terminal, install plugin of deploying to git\n\n```shell\n$ npm install --save hexo-deployer-git\n```\n\n3.Open the `_config.yml` file in the blog `root` directory, add these lines afterwards\n\n```yml\ndeploy:\n type: git\n repo: git@github.com:DaydreamAtNight/DaydreamAtNight.github.io.git\n branch: master\n```\n\n4.Go to the blog `root`, deploy the blog to remote, d for deploy\n\n```shell\n$ hexo clean\n$ hexo g\n$ hexo d\n```\n\nOpen https://daydreamatnight.github.io/ to see if it works\n\n## Change theme to yilia\n\ndefault theme of hexo is called landscape and it's not beautiful enough to most of the people. Yilia is a fast, simple, elegant and popular theme. Thought it has not been updated since Nov 2017, it still a good choice for fresh bloggers.\n\n### Download and deploy yilia\n\n1.go to the blog `root`\n\n``` shell\n$ git clone https://github.com/litten/hexo-theme-yilia theme/yilia\n```\n\n2.eidt the `_config.yml` file, add\n\n```yml\ntheme: yilia\n```\n\n3.clean and deploy hexo\n\n```shell\n$ hexo clean\n$ hexo g\n$ hexo d\n```\n\n### Basic customize yillia\n\n#### Activate `aboutme` ‘left slider’ button\n\n1.go to terminal run\n\n```shell\n$ npm i hexo-generator-json-content --save\n```\n\n2.go to the blog `root` directory, add these lines to the `_config.yml` file\n\n```yml\njsonContent:\n    meta: false\n    pages: false\n    posts:\n      title: true\n      date: true\n      path: true\n      text: false\n      raw: false\n      content: false\n      slug: false\n      updated: false\n      comments: false\n      link: false\n      permalink: false\n      excerpt: false\n      categories: false\n      tags: true\n```\n\n#### Customize avatar\n\nput the avatar file in directory  `themes/yilia/source/img`\n\n> do not add to the public repository directly, or the img get cleaned every time running `hexo clean` , need to upload to the same dir again after this command.\n\n#### Set favicon (icon on the tab of website)\n\nput the favicon img in directory `themes/yilia/source/img`\n\n[Bitbug](https://www.bitbug.net/) is a way of converting image into .ico file.\n\n#### Other configuration\n\nSet file of yillia is in `themes/yilia/_config.yml` as:\n\n```yml\n# Header\nauthor: Ryan LI\nsubtitle: 'Daydreaming at night'\nmenu:\n  main: /\n  archives: /archives/index.html\n  learn: /tags/learn/\n\n# SubNav\nsubnav:\n  github: \"https://github.com/DaydreamAtNight\"\n  # weibo: \"#\"\n  # rss: \"#\"\n  # zhihu: \"#\"\n  #qq: \"#\"\n  #weixin: \"#\"\n  #jianshu: \"#\"\n  #douban: \"#\"\n  #segmentfault: \"#\"\n  #bilibili: \"#\"\n  #acfun: \"#\"\n  mail: \"mailto:lishoushou2019@gmail.com\"\n  #facebook: \"#\"\n  #google: \"#\"\n  #twitter: \"#\"\n  #linkedin: \"#\"\n\nrss: /atom.xml\n\n# 是否需要修改 root 路径\n# 如果您的网站存放在子目录中，例如 http://yoursite.com/blog，\n# 请将您的 url 设为 http://yoursite.com/blog 并把 root 设为 /blog/。\nroot: /\n\n# Content\n\n# 文章太长，截断按钮文字\n# excerpt_link: more\n# 文章卡片右下角常驻链接，不需要请设置为false\nshow_all_link: 'show all'\n# 数学公式\nmathjax: false\n# 是否在新窗口打开链接\nopen_in_new: false\n\n# 打赏\n# 打赏type设定：0-关闭打赏； 1-文章对应的md文件里有reward:true属性，才有打赏； 2-所有文章均有打赏\nreward_type: 0\n# # 打赏wording\n# reward_wording: '谢谢你请我吃糖果'\n# # 支二维码图片地址，跟你设置头像的方式一样。比如：/assets/img/alipay.jpg\n# alipay: \n# # 微信二维码图片地址\n# weixin: \n\n# 目录\n# 目录设定：0-不显示目录； 1-文章对应的md文件里有toc:true属性，才有目录； 2-所有文章均显示目录\ntoc: 1\n# 根据自己的习惯来设置，如果你的目录标题习惯有标号，置为true即可隐藏hexo重复的序号；否则置为false\ntoc_hide_index: true\n# 目录为空时的提示\ntoc_empty_wording: 'directery none exist'\n\n# 是否有快速回到顶部的按钮\ntop: true\n\n# Miscellaneous\nbaidu_analytics: ''\ngoogle_analytics: ''\nfavicon: /img/favicon.ico\n\n#你的头像url\navatar: /img/avatar.jpeg\n\n#是否开启分享\n# share_jia: true\n\n# #评论：1、多说；2、网易云跟帖；3、畅言；4、Disqus；5、Gitment\n# #不需要使用某项，直接设置值为false，或注释掉\n# #具体请参考wiki：https://github.com/litten/hexo-theme-yilia/wiki/\n\n# #1、多说\n# duoshuo: false\n\n# #2、网易云跟帖\n# wangyiyun: false\n\n# #3、畅言\n# changyan_appid: false\n# changyan_conf: false\n\n# #4、Disqus 在hexo根目录的config里也有disqus_shortname字段，优先使用yilia的\n# disqus: false\n\n# #5、Gitment\n# gitment_owner: false      #你的 GitHub ID\n# gitment_repo: ''          #存储评论的 repo\n# gitment_oauth:\n#   client_id: ''           #client ID\n#   client_secret: ''       #client secret\n\n# 样式定制 - 一般不需要修改，除非有很强的定制欲望…\nstyle:\n  # 头像上面的背景颜色\n  header: '#ece0cf'\n  # 右滑板块背景\n  slider: 'linear-gradient(45deg,#b4a698,#ece0cf)'\n\n# slider的设置\nslider:\n  # 是否默认展开tags板块\n  showTags: false\n\n# 智能菜单\n# 如不需要，将该对应项置为false\n# 比如\n#smart_menu:\n#  friends: false\nsmart_menu:\n  innerArchive: 'All articles'\n  # friends: '友链'\n  aboutme: 'About me'\n\n# friends:\n#   友情链接1: http://localhost:4000/\n#   友情链接2: http://localhost:4000/\n#   友情链接3: http://localhost:4000/\n#   友情链接4: http://localhost:4000/\n#   友情链接5: http://localhost:4000/\n#   友情链接6: http://localhost:4000/\n\naboutme: Stay hungry, stay fullish\n```\n\n## Advance customize\n\n### Stop visit litten.me:9005\n\nSometimes the user's client information is collected, see [here](https://github.com/litten/hexo-theme-yilia/issues/528) for details. \n\nStop reporting by clear the contents in `themes/yilia/source-src/js/report.js`\n\n### Limit display numbers on the main page\n\nSimply insert `<! -- more -->` to show only what comes before it while collapse the afterwards,  click on the article title to read it in full.\n\n### Easily add pics to blogs via hexo-renderer-marked plugin\n\n1.find `post_asset_folder ` in `_config.yml` file in the blog `root` directory, set to be true\n\n```yml\npost_asset_folder:true\n```\n\n2.Install plugin\n\n```shell\nnpm install hexo-renderer-marked --save\n```\n\n3.change `_config.yml` in blog `root` directory as\n\n```yml\npost_asset_folder: true\nmarked:\n  prependRoot: true\n  postAsset: true\n```\n\nthen img can be easily add with `![img description](img.png)` after add the image to the folder with the same name as the article in `/source/_posts/`\n\n4.change Typora pereference as\n\n<img src=\"typora setting.png\" alt=\"typora setting\" style=\"zoom:50%;\" />\n\nimg can drag into typro, yet blogname need to be deleted before deploying\n\n### Show number of articles and words on the left panel\n\n1.add wordcount plugin in terminal\n\n```shell\nnpm i --save hexo-wordcount\n```\n\n2.change `themes/yilia/layout/_partial/left-col.ejs` \n\nafter\n\n```html\n<nav class=\"header-menu\">\n  <ul>\n    <% for (var i in theme.menu){ %>\n      <li><a href=\"<%- url_for(theme.menu[i]) %>\"><%= i %></a></li>\n    <%}%>\n  </ul>\n</nav>\n```\n\nadd\n\n```html\n<span class=\"post-count\"><%=site.posts.length%> articles\n\t\t\t<span><%= totalcount(site, '0,0.0a') %></span> words</span>\n```\n\nadd style sheet in `themes/yilia/source/main.0cf68a.css`\n\n```\n.post-count{\n  font-size: 12px;\n  color: #696969;\n}\n```\n\n### Show number of visits in the footer\n\n[busuanzi](https://busuanzi.ibruce.info/) is in use, which is super easy to deploy\n\nchange `themes/yilia/layout/_partial/footer.ejs` as\n\n```html\n<footer id=\"footer\">\n  <div class=\"outer\">\n    <div id=\"footer-info\">\n    \t<div class=\"footer-left\">\n    \t\t<!-- total visits number -->\n          <% if (theme.busuanzi && theme.busuanzi.enable){ %>\n            <!-- busuanzi statistics -->\n            <span id=\"busuanzi_value_site_pv\"></span>&nbsp;visits in total\n            <script async src=\"//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js\"></script>\n          <% } %>\n        <!-- end -->\n    \t</div>\n      \t<div class=\"footer-right\">\n      \t\t&copy; <%= date(new Date(), 'YYYY') %> <%= config.author || config.title %>\n      \t</div>\n    </div>\n  </div>\n</footer>\n```\n\nand add \n\n```yml\nbusuanzi:\n  enable: true\n```\n\n### Add button of hiding the left panel\n\nRefer to  [hexo yilia主题添加隐藏左边栏目按钮](https://cqh-i.github.io/2019/08/07/hexo-yilia%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E9%9A%90%E8%97%8F%E5%B7%A6%E8%BE%B9%E6%A0%8F%E7%9B%AE%E6%8C%89%E9%92%AE/ )  \n\n1.add style list to file ` /themes/yilia/source/main.0cf68a.css `\n\n```css\n/*stylesheet for hide the left panel*/\n.mymenucontainer {\n\tdisplay:block;\n\tcursor:pointer;\n\tleft:0;\n\ttop:0;\n\twidth:35px;\n\theight:35px;\n\tz-index:9999;\n\tposition:fixed;\n}\n.bar1 {\n\twidth:35px;\n\theight:3px;\n\tborder-radius:3px;\n\tbackground-color:#8E6D51;\n\tmargin:6px 0;\n\ttransition:0.1s;\n\t-webkit-transform:rotate(-45deg) translate(-8px,8px);\n\ttransform:rotate(-45deg) translate(-8px,8px);\n}\n.bar2 {\n\twidth:35px;\n\theight:3px;\n\tborder-radius:3px;\n\tbackground-color:#8E6D51;\n\tmargin:6px 0;\n\ttransition:0.1s;\n\topacity:0;\n}\n.bar3 {\n\twidth:35px;\n\theight:3px;\n\tborder-radius:3px;\n\tbackground-color:#8E6D51;\n\tmargin:6px 0;\n\ttransition:0.1s;\n\t-webkit-transform:rotate(45deg) translate(-4px,-6px);\n\ttransform:rotate(45deg) translate(-4px,-6px);\n}\n.change .bar1 {\n\t-webkit-transform:rotate(0deg) translate(0px,0px);\n\ttransform:rotate(0deg) translate(0px,0px);\n}\n.change .bar2 {\n\topacity:1;\n}\n.change .bar3 {\n\t-webkit-transform:rotate(0deg) translate(0px,0px);\n\ttransform:rotate(0deg) translate(0px,0px);\n}\n/*stylesheet for hide the left panel end*/\n```\n\n2.go to ` /themes/yilia/layout/layout.ejs ` add before `  <div class=\"left-col\"  ` \n\n```html\n<div class=\"mymenucontainer\" onclick=\"myFunction(this)\">\n  <div class=\"bar1\"></div>\n  <div class=\"bar2\"></div>\n  <div class=\"bar3\"></div>\n</div>\n```\n\n3.add between ` </body> ` and ` </html> ` \n\n```js\n<script>\n    var hide = false;\n    function myFunction(x) {\n        x.classList.toggle(\"change\");\n        if(hide == false){\n            $(\".left-col\").css('display', 'none');\n            $(\".mid-col\").css(\"left\", 6);\n            $(\".tools-col\").css('display', 'none');\n            $(\".tools-col.hide\").css('display', 'none');\n            hide = true;\n        }else{\n            $(\".left-col\").css('display', '');\n            $(\".mid-col\").css(\"left\", 300);\n            $(\".tools-col\").css('display', '');\n            $(\".tools-col.hide\").css('display', '');\n            hide = false;\n        }\n    }\n</script>\n```\n\n\n\n\n\n### Beautiful contents navigation in articles\n\nDefault navigator is kind of ugly so found a more beautiful version, to use default version, simply change `toc: 2` in file `themes/yilia/_config.yml`\n\n1.add this block at the end of `themes/yilia/source/main.0cf68a.css`\n\n```css\n/* navigator */\n#container .show-toc-btn,#container .toc-article{display:block}\n.toc-article{z-index:100;background:#fff;border:1px solid #ccc;max-width:250px;min-width:150px;max-height:500px;overflow-y:auto;-webkit-box-shadow:5px 5px 2px #ccc;box-shadow:5px 5px 2px #ccc;font-size:12px;padding:10px;position:fixed;right:35px;top:129px}.toc-article .toc-close{font-weight:700;font-size:20px;cursor:pointer;float:right;color:#ccc}.toc-article .toc-close:hover{color:#000}.toc-article .toc{font-size:12px;padding:0;line-height:20px}.toc-article .toc .toc-number{color:#333}.toc-article .toc .toc-text:hover{text-decoration:underline;color:#2a6496}.toc-article li{list-style-type:none}.toc-article .toc-level-1{margin:4px 0}.toc-article .toc-child{}@-moz-keyframes cd-bounce-1{0%{opacity:0;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)}60%{opacity:1;-o-transform:scale(1.01);-webkit-transform:scale(1.01);-moz-transform:scale(1.01);-ms-transform:scale(1.01);transform:scale(1.01)}100%{-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)}}@-webkit-keyframes cd-bounce-1{0%{opacity:0;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)}60%{opacity:1;-o-transform:scale(1.01);-webkit-transform:scale(1.01);-moz-transform:scale(1.01);-ms-transform:scale(1.01);transform:scale(1.01)}100%{-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)}}@-o-keyframes cd-bounce-1{0%{opacity:0;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)}60%{opacity:1;-o-transform:scale(1.01);-webkit-transform:scale(1.01);-moz-transform:scale(1.01);-ms-transform:scale(1.01);transform:scale(1.01)}100%{-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)}}@keyframes cd-bounce-1{0%{opacity:0;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)}60%{opacity:1;-o-transform:scale(1.01);-webkit-transform:scale(1.01);-moz-transform:scale(1.01);-ms-transform:scale(1.01);transform:scale(1.01)}100%{-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)}}.show-toc-btn{display:none;z-index:10;width:30px;min-height:14px;overflow:hidden;padding:4px 6px 8px 5px;border:1px solid #ddd;border-right:none;position:fixed;right:40px;text-align:center;background-color:#f9f9f9}.show-toc-btn .btn-bg{margin-top:2px;display:block;width:16px;height:14px;background:url(http://7xtawy.com1.z0.glb.clouddn.com/show.png) no-repeat;-webkit-background-size:100%;-moz-background-size:100%;background-size:100%}.show-toc-btn .btn-text{color:#999;font-size:12px}.show-toc-btn:hover{cursor:pointer}.show-toc-btn:hover .btn-bg{background-position:0 -16px}.show-toc-btn:hover .btn-text{font-size:12px;color:#ea8010}\n.toc-article li ol, .toc-article li ul {\n    margin-left: 30px;\n}\n.toc-article ol, .toc-article ul {\n    margin: 10px 0;\n}\n```\n\n2.after `</header><% } %>` in file `themes/yilia/layout/_partial/article.ejs` add\n\n```html\n    <!-- navigator -->\n    <% if (!index && post.toc){ %>\n      <p class=\"show-toc-btn\" id=\"show-toc-btn\" onclick=\"showToc();\" style=\"display:none\">\n            <span class=\"btn-bg\"></span>\n            <span class=\"btn-text\">...</span>\n            </p>\n      <div id=\"toc-article\" class=\"toc-article\">\n          <span id=\"toc-close\" class=\"toc-close\" title=\"hide navigator\" onclick=\"showBtn();\">×</span>\n          <strong class=\"toc-title\">navigator</strong>\n            <%- toc(post.content) %>\n          </div>\n    <script type=\"text/javascript\">\n      function showToc(){\n          var toc_article = document.getElementById(\"toc-article\");\n          var show_toc_btn = document.getElementById(\"show-toc-btn\");\n          toc_article.setAttribute(\"style\",\"display:block\");\n          show_toc_btn.setAttribute(\"style\",\"display:none\");\n          };\n      function showBtn(){\n          var toc_article = document.getElementById(\"toc-article\");\n          var show_toc_btn = document.getElementById(\"show-toc-btn\");\n          toc_article.setAttribute(\"style\",\"display:none\");\n          show_toc_btn.setAttribute(\"style\",\"display:block\");\n          };\n    </script>\n        <% } %>\n    <!-- navigator end -->\n```\n\n3.add `toc:true` to the articles that need the navigator.\n\n### Add custormize header to articles\n\nwhen run `hexo new` to initiate a new blog, a defaul head would generate, change it by\n\nchange the `scaffolds/post.md` in the `root` directory\n\n```txt\n---\ntitle: {{ title }}\ndate: {{ date }}\nauthor: daydreamatnight\ntoc: true\ndeclare: true\ntags:\n---\n```\n\n#### more headers to choose when writing a blog\n\nbefore a blog, more paras can be chosen to add\n\n```txt\n--- \ntitle: #你的博客文章名 \ntoc: ture #toc \ndate: 2020-09-07 09:25:00 #文章时间 \nauthor: GavenLee #作者 \nimg: /source/images/xxx.jpg #图片 \ntop: true #是否顶置 \ncover: true #是否在引导页轮播 \ncoverImg: /images/1.jpg #轮播图片 \npassword: #阅读密码这里被加密 \nmathjax: false #mathjax \nsummary: 这是你自定义的文章摘要内容，如果这个属性有值，文章卡片摘要就显示这段文字，否则程序会自动截取文章的部分内容作为摘要 \ncategories: Markdown #分类 \ntags: #标签 \nabbrlink: HexoLearn #链接 \n---\n```\n\n### Disable auto wrap in code block\n\nlocate and delete `white-space:pre-wrap` in file `themes/yilia/source/main.0cf68a.css` \n\n### Add copy button to code block\n\n1.create a `clipboard_use.js` file in directory `themes/yilia/source` \n\n```js\n$(\".highlight\").wrap(\"<div class='code-wrapper' style='position:relative'></div>\");\n/*create copy button after page loaded*/\n!function (e, t, a) {\n    /* code */\n    var initCopyCode = function () {\n        var copyHtml = '';\n        copyHtml += '<button class=\"btn-copy\" data-clipboard-snippet=\"\">';\n        copyHtml += '  <i class=\"fa fa-clipboard\"></i><span>copy</span>';\n        copyHtml += '</button>';\n        $(\".highlight .code\").before(copyHtml);\n        var clipboard = new ClipboardJS('.btn-copy', {\n            target: function (trigger) {\n                return trigger.nextElementSibling;\n            }\n        });\n        clipboard.on('success', function (e) {\n            e.trigger.innerHTML = \"<i class='fa fa-check' style='color:green'></i><span style='color:green'>copy success</span>\"\n            setTimeout(function () {\n                e.trigger.innerHTML = \"<i class='fa fa-clipboard'></i><span>copy</span>\"\n            }, 1000)\n            e.clearSelection();\n        });\n        clipboard.on('error', function (e) {\n            e.trigger.innerHTML = \"<i class='fa fa-exclamation' style='color:red'></i><span style='color:red'>copy success</span>\"\n            setTimeout(function () {\n                e.trigger.innerHTML = \"<i class='fa fa-clipboard'></i><span>copy</span>\"\n            }, 1000)\n            e.clearSelection();\n        });\n    }\n    initCopyCode();\n}(window, document);\n```\n\n2.load .js file, edit `themes/yilia/layout/layout.ejs` file, add before `</body>`. \n\n```html\n<!-- copy button in code block-->\n<script type=\"text/javascript\" src=\"https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.js\"></script>\n<script type=\"text/javascript\" src=\"https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js\"></script>\n<script type=\"text/javascript\" src=\"/clipboard_use.js\"></script>\n```\n\n3.add stylesheet the end of `themes/yilia/source/main.0cf68a.css`\n\n```css\n/* code copy button */\n.btn-copy {\n  display: inline-block;\n  cursor: pointer;\n  background-color: #eee;\n  background-image: linear-gradient(#fcfcfc, #eee);\n  border: 1px solid #d5d5d5;\n  border-radius: 3px;\n  -webkit-user-select: none;\n  -moz-user-select: none;\n  -ms-user-select: none;\n  user-select: none;\n  -webkit-appearance: none;\n  font-size: 13px;\n  font-weight: 700;\n  line-height: 20px;\n  color: #333;\n  -webkit-transition: opacity .3s ease-in-out;\n  -o-transition: opacity .3s ease-in-out;\n  transition: opacity .3s ease-in-out;\n  padding: 2px 6px;\n  position: absolute;\n  right: 5px;\n  top: 5px;\n  opacity: 0;\n}\n.btn-copy span {\n  margin-left: 5px;\n}\n.highlight:hover .btn-copy {\n  opacity: 1;\n}\n/* code copy button end */\n```\n\n4.add copy button icon, edit `themes/yilia/layout/_partia/head.ejs` add before `</head>` \n\n```html\n<!-- copy button icon -->\n<link rel=\"stylesheet\" type=\"text/css\" href=\"//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css\">\n```\n\n### Allow search engines to index this Blog\n\n#### index Google to this Blog\n\ncheck if google can find you, enter `site:daydreamatnight.github.io` to see\n\n<img src=\"check google search.png\" alt=\"check google search\" style=\"zoom:50%;\" />\n\n##### Add url to goole search console\n\n1.open [google console](https://search.google.com/search-console/welcome) , add URL link of the blog (https://daydreamatnight.github.io), in the `URL prefix` block, click `CONTINUE`\n\n<img src=\"google search console.png\" alt=\"google console\" style=\"zoom:50%;\" />\n\n2.upload the html file to the blog `root` directory and deploy the website, then clicke verify.\n\n<img src=\"google console varification.png\" alt=\"google console varification\" style=\"zoom:50%;\" />\n\nlittle buggy here, see [**don’t upload** the file **using hexo** command](https://hilyy.xyz/how-to-allow-google-to-index-your-hexo-blog-website-google-search-console-verification-methods/)\n\n##### add sitemap for google\n\nadd sitemap for google and baidu together\n\n> A *sitemap* is a file where you provide information about the pages, videos, and other files on your site, and the relationships between them. Search engines like Google read this file to more intelligently crawl your site. A sitemap tells Google which pages and files you think are important in your site, and also provides valuable information about these files: for example, for pages, when the page was last updated, how often the page is changed, and any alternate language versions of a page.\n\n1.install sitemap plugins\t\n\n```shell\n$ npm install hexo-generator-sitemap --save\n$ npm install hexo-generator-baidu-sitemap --save\n```\n\n2.add to the `_config.yml` in the blog `root` \n\n```yml\n# hexo sitemap\nsitemap:\n  path: sitemap.xml\nbaidusitemap:\n  path: baidusitemap.xml\n```\n\n3.Deploy the blog, go to  https://daydreamatnight.github.io/sitemap.xml and https://daydreamatnight.github.io/baidusitemap.xml to see if sitemaps are uploaded\n\n4.Go to Google Search Console, in the left panel, click `Sitemaps`, enter your sitemap URL `sitemap.xml` \n\n<img src=\"can't fetch sitemap.png\" alt=\"can't fetch sitemap\" style=\"zoom:50%;\" />\n\nGooglebot won't download the sitemap immediately. Give it time. \n\n##### add robots.txt\n\n> A robots.txt file tells search engine crawlers which URLs the crawler can access on your site. This is used mainly to avoid overloading your site with requests; **it is not a mechanism for keeping a web page out of Google**. To keep a web page out of Google, [block indexing with `noindex`](https://developers.google.com/search/docs/advanced/crawling/block-indexing) or password-protect the page.\n>\n> A robots.txt file is used primarily to manage crawler traffic to your site, and *usually* to keep a file off Google, depending on the file type:\n\n```txt\nUser-agent: *\nAllow: /\nAllow: /archives/\nAllow: /tags/\nAllow: /categories/\nAllow: /about/\nAllow: /guestbook/\nAllow: /others/\n\n\nDisallow: /js/\nDisallow: /css/\nDisallow: /lib/\n\nSitemap: https://daydreamatnight.github.io/sitemap.xml\nSitemap: https://daydreamatnight.github.io/baidusitemap.xml\n```\n\ndeploy the blog and wait.\n\n##### check if sitemap is available\n\nAfter uploaded several updates, my sitemap still didn't fetched by google. So I went to check, it turns out my url setting in `_config.yml` is wrong.  So I changed it to be my home url. And check it with  [URL Inspection Tool](https://www.jcchouinard.com/url-inspection-tool/). \n\n1.Open [google search console](https://search.google.com/search-console), add the url of sitemap in the upper url inspecting box.\n\n<img src=\"Google%20sitemap%20inspect.png\" alt=\"Google sitemap inspect URL is not on Google \" style=\"zoom:22%;\" />\n\n<img src=\"Google%20sitemap%20inspect%202.png\" alt=\"Google sitemap inspect 2\" style=\"zoom:22%;\" />\n\nIt's normal it shows `URL is not on Google` because it shouldn't as a sitemap.\n\n2.click `live test` to check the availability.\n\n<img src=\"Google%20sitemap%20inspect%203.png\" alt=\"Google sitemap inspect 3\" style=\"zoom:75%;\" />\n\nIt should be available, then just wait.\n\n#### index Bing to this Blog\n\n1.go to [Bing webmaster](https://www.bing.com/webmasters/) and login\n\n2.connect with google webmaster.\n\n<img src=\"bing%20sitemap.png\" alt=\"bing sitemap\" style=\"zoom:75%;\" />\n\n  \n\n<img src=\"being%20sitemap%20connect%20google.png\" alt=\"being sitemap connect google\" style=\"zoom:75%;\" />\n\n  \n\n<img src=\"bing%20search%20console%20success.png\" alt=\"bing search console success\" style=\"zoom:75%;\" />\n\n#### index baidu to this Blog(not possibly working)\n\ngo to the [baidu search console](https://ziyuan.baidu.com/site/index) , \n\n<img src=\"baidu console.png\" alt=\"baidu console\" style=\"zoom:50%;\" />\n\nClick `添加网站` and input every thing, do similar thing\n\n<img src=\"baidu console varification.png\" alt=\"baidu console varification\" style=\"zoom:50%;\" />\n\nadd sitemap\n\n<img src=\"baidu console sitemap.png\" alt=\"baidu console sitemap\" style=\"zoom:50%;\" />\n\njust wait forever, this could take 2000 years, so give up\n\n### Add copyright statement\n\n1.open file `themes/yilia/layout/_partial/article.ejs` add before `<% if ((theme.reward_type === 2 || (theme.reward_type === 1 && post.reward)) && !index){ %>`\n\n```html\n<!-- add copyright statement -->\n<% if(theme.declare){%>\n    <%- partial('post/declare') %>\n<% } %>\n<!-- end -->\n```\n\n2.create new file `declare.ejs` under `themes/yilia/layout/_partial/post/` with:\n\n```html\n<!--add copyright statement https://github.com/JoeyBling/hexo-theme-yilia-plus/commit/c1215e132f6d5621c5fea83d3c4f7ccbcca074a3-->\n<%\n  var sUrl = url.replace(/index\\.html$/, '');\n  sUrl = /^(http:|https:)\\/\\//.test(sUrl) ? sUrl : 'https:' + sUrl;\n%>\n\n<!-- #copyright setting：0-close statement; 1-declare statement if declare: true in the article header; 2-always declare the copyright -->\n<% if ((theme.declare.declare_type === 2 || (theme.declare.declare_type === 1 && post.declare)) && !index){ %>\n  <div class=\"declare\">\n    <strong class=\"author\">author: </strong>\n    <% if(config.author != undefined){ %>\n      <%= config.author%>\n    <% }else{%>\n      <font color=\"red\">please add right \"author\" name in \"_config.yml\" in the blog root</font>\n    <%}%>\n    <br>\n    <strong class=\"create-time\">posting date: </strong>\n    <%- date(post.date, 'YYYY-MM-DD HH:MM:SS') %>\n    <br>\n    <strong class=\"update-time\">last update: </strong>\n    <%- date(post.updated, 'YYYY-MM-DD HH:MM:SS') %>\n    <br>\n    <strong class=\"article-titles\">article title: </strong>\n    <a href=\"<%= config.url %>/<%= post.path %>\" title=\"<%= post.title %>\" target=\"_blank\"><%= post.title %></a>\n    <br>\n    <strong class=\"article-url\">article link: </strong>\n    <a href=\"<%= config.url %>/<%= post.path %>\" title=\"<%= post.title %>\" target=\"_blank\"><%= config.url %>/<%= post.path %></a>\n    <br>\n    <strong class=\"copyright\">copyright:</strong>\n    This work is licensed under a\n    <a rel=\"license\" href=\"<%= theme.declare.licensee_url%>\" title=\"<%= theme.declare.licensee_alias %>\"><%= theme.declare.licensee_name%></a>\n    licience \n    <% if(theme.declare.licensee_img != undefined){ %>\n      <a rel=\"license\" href=\"<%= theme.declare.licensee_url%>\"><img alt=\"知识共享许可协议\" style=\"border-width:0\" src=\"<%= theme.declare.licensee_img%>\"/></a>\n    <% } %>\n  </div>\n<% } else {%>\n  <div class=\"declare\" hidden=\"hidden\"></div>\n<% } %>\n<!-- add copyright statement -->\n```\n\n3.add stylesheet the end of `themes/yilia/source/main.0cf68a.css` \n\n```css\n/*stylesheet for the delcare*/\n.declare {\n  background-color: #eaeaea;\n  margin-top: 2em;\n  border-left: 3px solid #ff1700;\n  padding: .5em 1em; \n}\n/*stylesheet for the delcare end*/\n```\n\n4.add at the end of `themes/yilia/_config.yml` file:\n\n```\ndeclare:\n  declare_type: 1\n  licensee_url: http://creativecommons.org/licenses/by-nc-sa/4.0/      \n  licensee_name: 'CC BY-NC-SA 4.0'                              \n  licensee_alias: 'CC BY-NC-SA 4.0'     \n  licensee_img: https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\n```\n\n### Add mind-map support\n\n```shell\nnpm install hexo-markmap\n```\n\nDetailed in its [Github](https://github.com/MaxChang3/hexo-markmap)\n\nExample:\n\n```\n{% markmap 300px %}\n- Testa\n  - test1\n  - test2\n- Testb\n  - test1\n  - test2\n{%endmarkmap%}\n```\n\n### Add Latex math support\n\nChange the renderer to the more powerful pandoc:\n\n1.Install pandoc on macOS:\n\n```\ncopybrew install pandoc\n```\n\n2.in the blog root directory uninstall the default renderer then install the pandoc renderer:\n\n```\ncopynpm uninstall hexo-renderer-marked --save\nnpm install hexo-renderer-pandoc --save\n```\n\n3.install the hexo math plugin\n\n```\ncopynpm install hexo-math --save\n```\n\n4.add these lines to the hexo `_config` file\n\n```\ncopymarkdown:\n  plugins:\n    - markdown-it-footnote\n    - markdown-it-sup\n    - markdown-it-sub\n    - markdown-it-abbr\n    - markdown-it-emoji\n    - hexo-math\n```\n\n5.add these lines to the theme `_config` file\n\n```\ncopy# MathJax Support\nmathjax:\n  enable: true\n  per_page: true\n```\n\n6.rebuild the to blog see changes\n\n7.Examples: $this_{is}an\\frac{inline}{equation}$\n$$\n\\begin{equation}\n    \\mathbf{K}_\\mathbf{1}=\\frac{1}{\\Delta r}\\ \\left[\\begin{matrix}\\begin{matrix}-1&1\\\\-1&1\\\\\\end{matrix}&\\ &\\ \\\\\\begin{matrix}\\ &\\ddots\\\\\\end{matrix}&\\begin{matrix}\\ddots&\\ \\\\\\end{matrix}&\\ \\\\\\ &-1\\ &1\\\\\\end{matrix}\\right],\\ \\ {\\ \\mathbf{K}}_\\mathbf{2}=\\frac{1}{\\Delta r}\\ \\left[\\begin{matrix}\\begin{matrix}-1&1\\\\\\ &\\ddots\\\\\\end{matrix}&\\begin{matrix}\\\\\\ddots\\\\\\end{matrix}&\\ \\\\\\begin{matrix}\\ &\\ \\\\\\end{matrix}&-1&1\\ \\\\\\ &-1\\ &1\\\\\\end{matrix}\\right]\n    \\label{K2}\n\\end{equation}\n$$\n\n### The last snapshot\n\nOk, never spend time on a no-longer maintained project. Here's the last figure of it.\n\n<img src=\"Last snapshot.png\" alt=\"Last snapshot\" style=\"zoom:80%;\" />\n\n## Reference\n\nhttps://flatironschool.com/blog/the-benefits-of-blogging-how-and-why-to-keep-a-technical-blog/\n\nhttps://weblog.masukomi.org/2015/10/18/static-vs-dynamic-blogging/\n\nhttps://www.cnblogs.com/aoguai/p/11781505.html\n\nhttps://www.kblog.top/post/30452.html\n\nhttps://wkzqn.gitee.io/2020/02/16/typora%E7%BC%96%E5%86%99hexo%E5%8D%9A%E5%AE%A2%E6%97%B6%E7%9A%84%E5%9B%BE%E7%89%87%E6%98%BE%E7%A4%BA/\n\nhttps://segmentfault.com/a/1190000009478837#articleHeader5\n\nhttps://hilyy.xyz/how-to-allow-google-to-index-your-hexo-blog-website-google-search-console-verification-methods/\n\nhttps://busuanzi.ibruce.info/\n\nhttps://creativecommons.org/choose/results-one?license_code=by-nc-sa&amp;jurisdiction=&amp;version=4.0&amp;lang=en\n\nhttps://www.jcchouinard.com/sitemap-could-not-be-read-couldnt-fetch-in-google-search-console/\n\nhttps://cqh-i.github.io/2019/08/07/hexo-yilia%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E9%9A%90%E8%97%8F%E5%B7%A6%E8%BE%B9%E6%A0%8F%E7%9B%AE%E6%8C%89%E9%92%AE/","source":"_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia.md","raw":"---\ntitle: Build and configure a personal blog via hexo and yilia on github\ndate: 2022-02-22 16:15:03\ntoc: true\ndeclare: true\ntags: \n  - hexo\n  - blog\n---\n\n> Technical blog has a hundred benefits and no harm\n>\n> This blog records the process of building and customizing this personal blog from 0 to 1\n\n> Unfortunately, the yilia theme has been no longer updating and it is too buggy right now. I switch to other theme.\n\n<!-- more -->\n\n## Preliminary\n\n### Why personal blog\n\n> Keeping a technical blog can be **a great way of documenting your growth as a developer**. This documentation can be particularly useful on a professional level. All software companies want to hire smart, thoughtful, communicative developers who can easily assimilate into a team, and who are ready to both teach and learn\n\n### Static vs dynamic blog\n\nthere are 2 types of mainstream personal blog: static and dynamic.\n\nStatic is recommended considering its simplicity, 0 maintenance and 0 safety worry. \n\n|              | Static blog                                                  | Dynamic blog                                                 |\n| :----------- | :----------------------------------------------------------- | :----------------------------------------------------------- |\n| Price        | low, 0 cost when the traffic is relatively low               | High, server is needed, cloud server of  high performance is usually very expensive. |\n| Features     | Limited, only third-party services can be used to complete certain \"dynamic\" functions, such as comments | Rich, in WordPress for example, basically any kind of plugins can be found. Featuers such as auto-resizing, media players, multiple authors, scheduled posts, user analysis can be easily realize. |\n| Speed        | Fast                                                         | Slow                                                         |\n| Maintainance | 0                                                            | Need to care about the sever                                 |\n| Markdown     | Supported yet it's the only choice                           | not supported                                                |\n| Geeky        | YES                                                          | No                                                           |\n\nAnd I also chose static because I'm geeky (poor of money) and results-driven (lazy to spend time on maintaining).\n\n## Build a static blog via hexo\n\n### Set environment\n\n1.check machine information: macOS on M1 MacBook\n\n2.Install Nodejs, including node and npm\n\nopen  https://nodejs.org/en/download/ and click download\n\n<img src=\"node js install.png\" alt=\"node js install\" style=\"zoom:50%;\" />\n\n3.Install Git\n\nAready installed\n\n4.Open terminal, check node, npm and git versions\n\n```shell\n$ npm -v\n$ node -v\n$ git --version\n8.3.1\nv16.14.0\ngit version 2.32.0 (Apple Git-132)\n```\n\n### Initialize blog\n\n1.install hexo via npm\n\n```shell\n$ sudo npm install -g hexo-cli\n$ hexo -v\nINFO  Validating config\nhexo: 6.0.0\nhexo-cli: 4.3.0\nos: darwin 21.2.0 12.1\n\nnode: 16.14.0\nv8: 9.4.146.24-node.20\nuv: 1.43.0\nzlib: 1.2.11\nbrotli: 1.0.9\nares: 1.18.1\nmodules: 93\nnghttp2: 1.45.1\nnapi: 8\nllhttp: 6.0.4\nopenssl: 1.1.1m+quic\ncldr: 40.0\nicu: 70.1\ntz: 2021a3\nunicode: 14.0\nngtcp2: 0.1.0-DEV\nnghttp3: 0.1.0-DEV\n```\n\n2.create a new folder in terminal and initialize the blog\n\n```shell\n$ cd ~/Documents/\n$ makedir self_blog\n$ cd self_blog/\n$ hexo init\nINFO  Cloning hexo-starter https://github.com/hexojs/hexo-starter.gitINFO  Install dependencies⸨#########⠂⠂⠂⠂⠂⠂⠂⠂⠂⸩ ⠹ idealTree:hexo-front-matter: timing idealTree:node_modules/hexo-front-matter Completed in 212msINFO  Start blogging with Hexo! \n```\n\n3.view the blog on localhost, s for start\n\n```shell\n$ hexo s\nINFO  Validating config\nINFO  Start processing\nINFO  Hexo is running at http://localhost:4000/ . Press Ctrl+C to stop.\n```\n\n\n### Write first blog\n\n1.write a new blog, n for new\n\n```shell\n$ hexo n 'Hello ShouRou'\nINFO  Validating config\nINFO  Created: ~/Documents/self_blog/source/_posts/Hello-ShouRou.md\n```\n\nthe blog can be written on any editor, Typora in use.\n\n```shell\nopen ~/Documents/self_blog/source/_posts/Hello-ShouRou.md\n```\n\n2.clean cache(not necessary)\n\n```shell\n$ hexo clean\n```\n\n3.generate the blog, g for generate\n\n```shell\n$ hexo g\nINFO  Validating config\nINFO  Start processing\nINFO  Files loaded in 61 ms\n(node:10719) Warning: Accessing non-existent property 'lineno' of module exports inside circular dependency\n(Use `node --trace-warnings ...` to show where the warning was created)\n(node:10719) Warning: Accessing non-existent property 'column' of module exports inside circular dependency\n(node:10719) Warning: Accessing non-existent property 'filename' of module exports inside circular dependency\n(node:10719) Warning: Accessing non-existent property 'lineno' of module exports inside circular dependency\n(node:10719) Warning: Accessing non-existent property 'column' of module exports inside circular dependency\n(node:10719) Warning: Accessing non-existent property 'filename' of module exports inside circular dependency\nINFO  Generated: archives/2022/index.html\nINFO  Generated: archives/index.html\nINFO  Generated: js/script.js\nINFO  Generated: fancybox/jquery.fancybox.min.css\nINFO  Generated: index.html\nINFO  Generated: css/style.css\nINFO  Generated: css/fonts/fontawesome-webfont.woff2\nINFO  Generated: fancybox/jquery.fancybox.min.js\nINFO  Generated: js/jquery-3.4.1.min.js\nINFO  Generated: archives/2022/02/index.html\nINFO  Generated: css/fonts/FontAwesome.otf\nINFO  Generated: css/fonts/fontawesome-webfont.woff\nINFO  Generated: css/fonts/fontawesome-webfont.eot\nINFO  Generated: css/fonts/fontawesome-webfont.ttf\nINFO  Generated: css/images/banner.jpg\nINFO  Generated: 2022/02/22/hello-world/index.html\nINFO  Generated: css/fonts/fontawesome-webfont.svg\nINFO  Generated: 2022/02/22/Hello-ShouRou/index.html\nINFO  18 files generated in 161 ms\n```\n\n### Deploy to remote (GitHub)\n\n1.Create a new repository with the name of $username.github.io\n\n<img src=\"github page.png\" alt=\"github page\" style=\"zoom:50%;\" />\n\nuse default setting\n\n2.open terminal, install plugin of deploying to git\n\n```shell\n$ npm install --save hexo-deployer-git\n```\n\n3.Open the `_config.yml` file in the blog `root` directory, add these lines afterwards\n\n```yml\ndeploy:\n type: git\n repo: git@github.com:DaydreamAtNight/DaydreamAtNight.github.io.git\n branch: master\n```\n\n4.Go to the blog `root`, deploy the blog to remote, d for deploy\n\n```shell\n$ hexo clean\n$ hexo g\n$ hexo d\n```\n\nOpen https://daydreamatnight.github.io/ to see if it works\n\n## Change theme to yilia\n\ndefault theme of hexo is called landscape and it's not beautiful enough to most of the people. Yilia is a fast, simple, elegant and popular theme. Thought it has not been updated since Nov 2017, it still a good choice for fresh bloggers.\n\n### Download and deploy yilia\n\n1.go to the blog `root`\n\n``` shell\n$ git clone https://github.com/litten/hexo-theme-yilia theme/yilia\n```\n\n2.eidt the `_config.yml` file, add\n\n```yml\ntheme: yilia\n```\n\n3.clean and deploy hexo\n\n```shell\n$ hexo clean\n$ hexo g\n$ hexo d\n```\n\n### Basic customize yillia\n\n#### Activate `aboutme` ‘left slider’ button\n\n1.go to terminal run\n\n```shell\n$ npm i hexo-generator-json-content --save\n```\n\n2.go to the blog `root` directory, add these lines to the `_config.yml` file\n\n```yml\njsonContent:\n    meta: false\n    pages: false\n    posts:\n      title: true\n      date: true\n      path: true\n      text: false\n      raw: false\n      content: false\n      slug: false\n      updated: false\n      comments: false\n      link: false\n      permalink: false\n      excerpt: false\n      categories: false\n      tags: true\n```\n\n#### Customize avatar\n\nput the avatar file in directory  `themes/yilia/source/img`\n\n> do not add to the public repository directly, or the img get cleaned every time running `hexo clean` , need to upload to the same dir again after this command.\n\n#### Set favicon (icon on the tab of website)\n\nput the favicon img in directory `themes/yilia/source/img`\n\n[Bitbug](https://www.bitbug.net/) is a way of converting image into .ico file.\n\n#### Other configuration\n\nSet file of yillia is in `themes/yilia/_config.yml` as:\n\n```yml\n# Header\nauthor: Ryan LI\nsubtitle: 'Daydreaming at night'\nmenu:\n  main: /\n  archives: /archives/index.html\n  learn: /tags/learn/\n\n# SubNav\nsubnav:\n  github: \"https://github.com/DaydreamAtNight\"\n  # weibo: \"#\"\n  # rss: \"#\"\n  # zhihu: \"#\"\n  #qq: \"#\"\n  #weixin: \"#\"\n  #jianshu: \"#\"\n  #douban: \"#\"\n  #segmentfault: \"#\"\n  #bilibili: \"#\"\n  #acfun: \"#\"\n  mail: \"mailto:lishoushou2019@gmail.com\"\n  #facebook: \"#\"\n  #google: \"#\"\n  #twitter: \"#\"\n  #linkedin: \"#\"\n\nrss: /atom.xml\n\n# 是否需要修改 root 路径\n# 如果您的网站存放在子目录中，例如 http://yoursite.com/blog，\n# 请将您的 url 设为 http://yoursite.com/blog 并把 root 设为 /blog/。\nroot: /\n\n# Content\n\n# 文章太长，截断按钮文字\n# excerpt_link: more\n# 文章卡片右下角常驻链接，不需要请设置为false\nshow_all_link: 'show all'\n# 数学公式\nmathjax: false\n# 是否在新窗口打开链接\nopen_in_new: false\n\n# 打赏\n# 打赏type设定：0-关闭打赏； 1-文章对应的md文件里有reward:true属性，才有打赏； 2-所有文章均有打赏\nreward_type: 0\n# # 打赏wording\n# reward_wording: '谢谢你请我吃糖果'\n# # 支二维码图片地址，跟你设置头像的方式一样。比如：/assets/img/alipay.jpg\n# alipay: \n# # 微信二维码图片地址\n# weixin: \n\n# 目录\n# 目录设定：0-不显示目录； 1-文章对应的md文件里有toc:true属性，才有目录； 2-所有文章均显示目录\ntoc: 1\n# 根据自己的习惯来设置，如果你的目录标题习惯有标号，置为true即可隐藏hexo重复的序号；否则置为false\ntoc_hide_index: true\n# 目录为空时的提示\ntoc_empty_wording: 'directery none exist'\n\n# 是否有快速回到顶部的按钮\ntop: true\n\n# Miscellaneous\nbaidu_analytics: ''\ngoogle_analytics: ''\nfavicon: /img/favicon.ico\n\n#你的头像url\navatar: /img/avatar.jpeg\n\n#是否开启分享\n# share_jia: true\n\n# #评论：1、多说；2、网易云跟帖；3、畅言；4、Disqus；5、Gitment\n# #不需要使用某项，直接设置值为false，或注释掉\n# #具体请参考wiki：https://github.com/litten/hexo-theme-yilia/wiki/\n\n# #1、多说\n# duoshuo: false\n\n# #2、网易云跟帖\n# wangyiyun: false\n\n# #3、畅言\n# changyan_appid: false\n# changyan_conf: false\n\n# #4、Disqus 在hexo根目录的config里也有disqus_shortname字段，优先使用yilia的\n# disqus: false\n\n# #5、Gitment\n# gitment_owner: false      #你的 GitHub ID\n# gitment_repo: ''          #存储评论的 repo\n# gitment_oauth:\n#   client_id: ''           #client ID\n#   client_secret: ''       #client secret\n\n# 样式定制 - 一般不需要修改，除非有很强的定制欲望…\nstyle:\n  # 头像上面的背景颜色\n  header: '#ece0cf'\n  # 右滑板块背景\n  slider: 'linear-gradient(45deg,#b4a698,#ece0cf)'\n\n# slider的设置\nslider:\n  # 是否默认展开tags板块\n  showTags: false\n\n# 智能菜单\n# 如不需要，将该对应项置为false\n# 比如\n#smart_menu:\n#  friends: false\nsmart_menu:\n  innerArchive: 'All articles'\n  # friends: '友链'\n  aboutme: 'About me'\n\n# friends:\n#   友情链接1: http://localhost:4000/\n#   友情链接2: http://localhost:4000/\n#   友情链接3: http://localhost:4000/\n#   友情链接4: http://localhost:4000/\n#   友情链接5: http://localhost:4000/\n#   友情链接6: http://localhost:4000/\n\naboutme: Stay hungry, stay fullish\n```\n\n## Advance customize\n\n### Stop visit litten.me:9005\n\nSometimes the user's client information is collected, see [here](https://github.com/litten/hexo-theme-yilia/issues/528) for details. \n\nStop reporting by clear the contents in `themes/yilia/source-src/js/report.js`\n\n### Limit display numbers on the main page\n\nSimply insert `<! -- more -->` to show only what comes before it while collapse the afterwards,  click on the article title to read it in full.\n\n### Easily add pics to blogs via hexo-renderer-marked plugin\n\n1.find `post_asset_folder ` in `_config.yml` file in the blog `root` directory, set to be true\n\n```yml\npost_asset_folder:true\n```\n\n2.Install plugin\n\n```shell\nnpm install hexo-renderer-marked --save\n```\n\n3.change `_config.yml` in blog `root` directory as\n\n```yml\npost_asset_folder: true\nmarked:\n  prependRoot: true\n  postAsset: true\n```\n\nthen img can be easily add with `![img description](img.png)` after add the image to the folder with the same name as the article in `/source/_posts/`\n\n4.change Typora pereference as\n\n<img src=\"typora setting.png\" alt=\"typora setting\" style=\"zoom:50%;\" />\n\nimg can drag into typro, yet blogname need to be deleted before deploying\n\n### Show number of articles and words on the left panel\n\n1.add wordcount plugin in terminal\n\n```shell\nnpm i --save hexo-wordcount\n```\n\n2.change `themes/yilia/layout/_partial/left-col.ejs` \n\nafter\n\n```html\n<nav class=\"header-menu\">\n  <ul>\n    <% for (var i in theme.menu){ %>\n      <li><a href=\"<%- url_for(theme.menu[i]) %>\"><%= i %></a></li>\n    <%}%>\n  </ul>\n</nav>\n```\n\nadd\n\n```html\n<span class=\"post-count\"><%=site.posts.length%> articles\n\t\t\t<span><%= totalcount(site, '0,0.0a') %></span> words</span>\n```\n\nadd style sheet in `themes/yilia/source/main.0cf68a.css`\n\n```\n.post-count{\n  font-size: 12px;\n  color: #696969;\n}\n```\n\n### Show number of visits in the footer\n\n[busuanzi](https://busuanzi.ibruce.info/) is in use, which is super easy to deploy\n\nchange `themes/yilia/layout/_partial/footer.ejs` as\n\n```html\n<footer id=\"footer\">\n  <div class=\"outer\">\n    <div id=\"footer-info\">\n    \t<div class=\"footer-left\">\n    \t\t<!-- total visits number -->\n          <% if (theme.busuanzi && theme.busuanzi.enable){ %>\n            <!-- busuanzi statistics -->\n            <span id=\"busuanzi_value_site_pv\"></span>&nbsp;visits in total\n            <script async src=\"//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js\"></script>\n          <% } %>\n        <!-- end -->\n    \t</div>\n      \t<div class=\"footer-right\">\n      \t\t&copy; <%= date(new Date(), 'YYYY') %> <%= config.author || config.title %>\n      \t</div>\n    </div>\n  </div>\n</footer>\n```\n\nand add \n\n```yml\nbusuanzi:\n  enable: true\n```\n\n### Add button of hiding the left panel\n\nRefer to  [hexo yilia主题添加隐藏左边栏目按钮](https://cqh-i.github.io/2019/08/07/hexo-yilia%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E9%9A%90%E8%97%8F%E5%B7%A6%E8%BE%B9%E6%A0%8F%E7%9B%AE%E6%8C%89%E9%92%AE/ )  \n\n1.add style list to file ` /themes/yilia/source/main.0cf68a.css `\n\n```css\n/*stylesheet for hide the left panel*/\n.mymenucontainer {\n\tdisplay:block;\n\tcursor:pointer;\n\tleft:0;\n\ttop:0;\n\twidth:35px;\n\theight:35px;\n\tz-index:9999;\n\tposition:fixed;\n}\n.bar1 {\n\twidth:35px;\n\theight:3px;\n\tborder-radius:3px;\n\tbackground-color:#8E6D51;\n\tmargin:6px 0;\n\ttransition:0.1s;\n\t-webkit-transform:rotate(-45deg) translate(-8px,8px);\n\ttransform:rotate(-45deg) translate(-8px,8px);\n}\n.bar2 {\n\twidth:35px;\n\theight:3px;\n\tborder-radius:3px;\n\tbackground-color:#8E6D51;\n\tmargin:6px 0;\n\ttransition:0.1s;\n\topacity:0;\n}\n.bar3 {\n\twidth:35px;\n\theight:3px;\n\tborder-radius:3px;\n\tbackground-color:#8E6D51;\n\tmargin:6px 0;\n\ttransition:0.1s;\n\t-webkit-transform:rotate(45deg) translate(-4px,-6px);\n\ttransform:rotate(45deg) translate(-4px,-6px);\n}\n.change .bar1 {\n\t-webkit-transform:rotate(0deg) translate(0px,0px);\n\ttransform:rotate(0deg) translate(0px,0px);\n}\n.change .bar2 {\n\topacity:1;\n}\n.change .bar3 {\n\t-webkit-transform:rotate(0deg) translate(0px,0px);\n\ttransform:rotate(0deg) translate(0px,0px);\n}\n/*stylesheet for hide the left panel end*/\n```\n\n2.go to ` /themes/yilia/layout/layout.ejs ` add before `  <div class=\"left-col\"  ` \n\n```html\n<div class=\"mymenucontainer\" onclick=\"myFunction(this)\">\n  <div class=\"bar1\"></div>\n  <div class=\"bar2\"></div>\n  <div class=\"bar3\"></div>\n</div>\n```\n\n3.add between ` </body> ` and ` </html> ` \n\n```js\n<script>\n    var hide = false;\n    function myFunction(x) {\n        x.classList.toggle(\"change\");\n        if(hide == false){\n            $(\".left-col\").css('display', 'none');\n            $(\".mid-col\").css(\"left\", 6);\n            $(\".tools-col\").css('display', 'none');\n            $(\".tools-col.hide\").css('display', 'none');\n            hide = true;\n        }else{\n            $(\".left-col\").css('display', '');\n            $(\".mid-col\").css(\"left\", 300);\n            $(\".tools-col\").css('display', '');\n            $(\".tools-col.hide\").css('display', '');\n            hide = false;\n        }\n    }\n</script>\n```\n\n\n\n\n\n### Beautiful contents navigation in articles\n\nDefault navigator is kind of ugly so found a more beautiful version, to use default version, simply change `toc: 2` in file `themes/yilia/_config.yml`\n\n1.add this block at the end of `themes/yilia/source/main.0cf68a.css`\n\n```css\n/* navigator */\n#container .show-toc-btn,#container .toc-article{display:block}\n.toc-article{z-index:100;background:#fff;border:1px solid #ccc;max-width:250px;min-width:150px;max-height:500px;overflow-y:auto;-webkit-box-shadow:5px 5px 2px #ccc;box-shadow:5px 5px 2px #ccc;font-size:12px;padding:10px;position:fixed;right:35px;top:129px}.toc-article .toc-close{font-weight:700;font-size:20px;cursor:pointer;float:right;color:#ccc}.toc-article .toc-close:hover{color:#000}.toc-article .toc{font-size:12px;padding:0;line-height:20px}.toc-article .toc .toc-number{color:#333}.toc-article .toc .toc-text:hover{text-decoration:underline;color:#2a6496}.toc-article li{list-style-type:none}.toc-article .toc-level-1{margin:4px 0}.toc-article .toc-child{}@-moz-keyframes cd-bounce-1{0%{opacity:0;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)}60%{opacity:1;-o-transform:scale(1.01);-webkit-transform:scale(1.01);-moz-transform:scale(1.01);-ms-transform:scale(1.01);transform:scale(1.01)}100%{-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)}}@-webkit-keyframes cd-bounce-1{0%{opacity:0;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)}60%{opacity:1;-o-transform:scale(1.01);-webkit-transform:scale(1.01);-moz-transform:scale(1.01);-ms-transform:scale(1.01);transform:scale(1.01)}100%{-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)}}@-o-keyframes cd-bounce-1{0%{opacity:0;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)}60%{opacity:1;-o-transform:scale(1.01);-webkit-transform:scale(1.01);-moz-transform:scale(1.01);-ms-transform:scale(1.01);transform:scale(1.01)}100%{-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)}}@keyframes cd-bounce-1{0%{opacity:0;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)}60%{opacity:1;-o-transform:scale(1.01);-webkit-transform:scale(1.01);-moz-transform:scale(1.01);-ms-transform:scale(1.01);transform:scale(1.01)}100%{-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)}}.show-toc-btn{display:none;z-index:10;width:30px;min-height:14px;overflow:hidden;padding:4px 6px 8px 5px;border:1px solid #ddd;border-right:none;position:fixed;right:40px;text-align:center;background-color:#f9f9f9}.show-toc-btn .btn-bg{margin-top:2px;display:block;width:16px;height:14px;background:url(http://7xtawy.com1.z0.glb.clouddn.com/show.png) no-repeat;-webkit-background-size:100%;-moz-background-size:100%;background-size:100%}.show-toc-btn .btn-text{color:#999;font-size:12px}.show-toc-btn:hover{cursor:pointer}.show-toc-btn:hover .btn-bg{background-position:0 -16px}.show-toc-btn:hover .btn-text{font-size:12px;color:#ea8010}\n.toc-article li ol, .toc-article li ul {\n    margin-left: 30px;\n}\n.toc-article ol, .toc-article ul {\n    margin: 10px 0;\n}\n```\n\n2.after `</header><% } %>` in file `themes/yilia/layout/_partial/article.ejs` add\n\n```html\n    <!-- navigator -->\n    <% if (!index && post.toc){ %>\n      <p class=\"show-toc-btn\" id=\"show-toc-btn\" onclick=\"showToc();\" style=\"display:none\">\n            <span class=\"btn-bg\"></span>\n            <span class=\"btn-text\">...</span>\n            </p>\n      <div id=\"toc-article\" class=\"toc-article\">\n          <span id=\"toc-close\" class=\"toc-close\" title=\"hide navigator\" onclick=\"showBtn();\">×</span>\n          <strong class=\"toc-title\">navigator</strong>\n            <%- toc(post.content) %>\n          </div>\n    <script type=\"text/javascript\">\n      function showToc(){\n          var toc_article = document.getElementById(\"toc-article\");\n          var show_toc_btn = document.getElementById(\"show-toc-btn\");\n          toc_article.setAttribute(\"style\",\"display:block\");\n          show_toc_btn.setAttribute(\"style\",\"display:none\");\n          };\n      function showBtn(){\n          var toc_article = document.getElementById(\"toc-article\");\n          var show_toc_btn = document.getElementById(\"show-toc-btn\");\n          toc_article.setAttribute(\"style\",\"display:none\");\n          show_toc_btn.setAttribute(\"style\",\"display:block\");\n          };\n    </script>\n        <% } %>\n    <!-- navigator end -->\n```\n\n3.add `toc:true` to the articles that need the navigator.\n\n### Add custormize header to articles\n\nwhen run `hexo new` to initiate a new blog, a defaul head would generate, change it by\n\nchange the `scaffolds/post.md` in the `root` directory\n\n```txt\n---\ntitle: {{ title }}\ndate: {{ date }}\nauthor: daydreamatnight\ntoc: true\ndeclare: true\ntags:\n---\n```\n\n#### more headers to choose when writing a blog\n\nbefore a blog, more paras can be chosen to add\n\n```txt\n--- \ntitle: #你的博客文章名 \ntoc: ture #toc \ndate: 2020-09-07 09:25:00 #文章时间 \nauthor: GavenLee #作者 \nimg: /source/images/xxx.jpg #图片 \ntop: true #是否顶置 \ncover: true #是否在引导页轮播 \ncoverImg: /images/1.jpg #轮播图片 \npassword: #阅读密码这里被加密 \nmathjax: false #mathjax \nsummary: 这是你自定义的文章摘要内容，如果这个属性有值，文章卡片摘要就显示这段文字，否则程序会自动截取文章的部分内容作为摘要 \ncategories: Markdown #分类 \ntags: #标签 \nabbrlink: HexoLearn #链接 \n---\n```\n\n### Disable auto wrap in code block\n\nlocate and delete `white-space:pre-wrap` in file `themes/yilia/source/main.0cf68a.css` \n\n### Add copy button to code block\n\n1.create a `clipboard_use.js` file in directory `themes/yilia/source` \n\n```js\n$(\".highlight\").wrap(\"<div class='code-wrapper' style='position:relative'></div>\");\n/*create copy button after page loaded*/\n!function (e, t, a) {\n    /* code */\n    var initCopyCode = function () {\n        var copyHtml = '';\n        copyHtml += '<button class=\"btn-copy\" data-clipboard-snippet=\"\">';\n        copyHtml += '  <i class=\"fa fa-clipboard\"></i><span>copy</span>';\n        copyHtml += '</button>';\n        $(\".highlight .code\").before(copyHtml);\n        var clipboard = new ClipboardJS('.btn-copy', {\n            target: function (trigger) {\n                return trigger.nextElementSibling;\n            }\n        });\n        clipboard.on('success', function (e) {\n            e.trigger.innerHTML = \"<i class='fa fa-check' style='color:green'></i><span style='color:green'>copy success</span>\"\n            setTimeout(function () {\n                e.trigger.innerHTML = \"<i class='fa fa-clipboard'></i><span>copy</span>\"\n            }, 1000)\n            e.clearSelection();\n        });\n        clipboard.on('error', function (e) {\n            e.trigger.innerHTML = \"<i class='fa fa-exclamation' style='color:red'></i><span style='color:red'>copy success</span>\"\n            setTimeout(function () {\n                e.trigger.innerHTML = \"<i class='fa fa-clipboard'></i><span>copy</span>\"\n            }, 1000)\n            e.clearSelection();\n        });\n    }\n    initCopyCode();\n}(window, document);\n```\n\n2.load .js file, edit `themes/yilia/layout/layout.ejs` file, add before `</body>`. \n\n```html\n<!-- copy button in code block-->\n<script type=\"text/javascript\" src=\"https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.js\"></script>\n<script type=\"text/javascript\" src=\"https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js\"></script>\n<script type=\"text/javascript\" src=\"/clipboard_use.js\"></script>\n```\n\n3.add stylesheet the end of `themes/yilia/source/main.0cf68a.css`\n\n```css\n/* code copy button */\n.btn-copy {\n  display: inline-block;\n  cursor: pointer;\n  background-color: #eee;\n  background-image: linear-gradient(#fcfcfc, #eee);\n  border: 1px solid #d5d5d5;\n  border-radius: 3px;\n  -webkit-user-select: none;\n  -moz-user-select: none;\n  -ms-user-select: none;\n  user-select: none;\n  -webkit-appearance: none;\n  font-size: 13px;\n  font-weight: 700;\n  line-height: 20px;\n  color: #333;\n  -webkit-transition: opacity .3s ease-in-out;\n  -o-transition: opacity .3s ease-in-out;\n  transition: opacity .3s ease-in-out;\n  padding: 2px 6px;\n  position: absolute;\n  right: 5px;\n  top: 5px;\n  opacity: 0;\n}\n.btn-copy span {\n  margin-left: 5px;\n}\n.highlight:hover .btn-copy {\n  opacity: 1;\n}\n/* code copy button end */\n```\n\n4.add copy button icon, edit `themes/yilia/layout/_partia/head.ejs` add before `</head>` \n\n```html\n<!-- copy button icon -->\n<link rel=\"stylesheet\" type=\"text/css\" href=\"//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css\">\n```\n\n### Allow search engines to index this Blog\n\n#### index Google to this Blog\n\ncheck if google can find you, enter `site:daydreamatnight.github.io` to see\n\n<img src=\"check google search.png\" alt=\"check google search\" style=\"zoom:50%;\" />\n\n##### Add url to goole search console\n\n1.open [google console](https://search.google.com/search-console/welcome) , add URL link of the blog (https://daydreamatnight.github.io), in the `URL prefix` block, click `CONTINUE`\n\n<img src=\"google search console.png\" alt=\"google console\" style=\"zoom:50%;\" />\n\n2.upload the html file to the blog `root` directory and deploy the website, then clicke verify.\n\n<img src=\"google console varification.png\" alt=\"google console varification\" style=\"zoom:50%;\" />\n\nlittle buggy here, see [**don’t upload** the file **using hexo** command](https://hilyy.xyz/how-to-allow-google-to-index-your-hexo-blog-website-google-search-console-verification-methods/)\n\n##### add sitemap for google\n\nadd sitemap for google and baidu together\n\n> A *sitemap* is a file where you provide information about the pages, videos, and other files on your site, and the relationships between them. Search engines like Google read this file to more intelligently crawl your site. A sitemap tells Google which pages and files you think are important in your site, and also provides valuable information about these files: for example, for pages, when the page was last updated, how often the page is changed, and any alternate language versions of a page.\n\n1.install sitemap plugins\t\n\n```shell\n$ npm install hexo-generator-sitemap --save\n$ npm install hexo-generator-baidu-sitemap --save\n```\n\n2.add to the `_config.yml` in the blog `root` \n\n```yml\n# hexo sitemap\nsitemap:\n  path: sitemap.xml\nbaidusitemap:\n  path: baidusitemap.xml\n```\n\n3.Deploy the blog, go to  https://daydreamatnight.github.io/sitemap.xml and https://daydreamatnight.github.io/baidusitemap.xml to see if sitemaps are uploaded\n\n4.Go to Google Search Console, in the left panel, click `Sitemaps`, enter your sitemap URL `sitemap.xml` \n\n<img src=\"can't fetch sitemap.png\" alt=\"can't fetch sitemap\" style=\"zoom:50%;\" />\n\nGooglebot won't download the sitemap immediately. Give it time. \n\n##### add robots.txt\n\n> A robots.txt file tells search engine crawlers which URLs the crawler can access on your site. This is used mainly to avoid overloading your site with requests; **it is not a mechanism for keeping a web page out of Google**. To keep a web page out of Google, [block indexing with `noindex`](https://developers.google.com/search/docs/advanced/crawling/block-indexing) or password-protect the page.\n>\n> A robots.txt file is used primarily to manage crawler traffic to your site, and *usually* to keep a file off Google, depending on the file type:\n\n```txt\nUser-agent: *\nAllow: /\nAllow: /archives/\nAllow: /tags/\nAllow: /categories/\nAllow: /about/\nAllow: /guestbook/\nAllow: /others/\n\n\nDisallow: /js/\nDisallow: /css/\nDisallow: /lib/\n\nSitemap: https://daydreamatnight.github.io/sitemap.xml\nSitemap: https://daydreamatnight.github.io/baidusitemap.xml\n```\n\ndeploy the blog and wait.\n\n##### check if sitemap is available\n\nAfter uploaded several updates, my sitemap still didn't fetched by google. So I went to check, it turns out my url setting in `_config.yml` is wrong.  So I changed it to be my home url. And check it with  [URL Inspection Tool](https://www.jcchouinard.com/url-inspection-tool/). \n\n1.Open [google search console](https://search.google.com/search-console), add the url of sitemap in the upper url inspecting box.\n\n<img src=\"Google%20sitemap%20inspect.png\" alt=\"Google sitemap inspect URL is not on Google \" style=\"zoom:22%;\" />\n\n<img src=\"Google%20sitemap%20inspect%202.png\" alt=\"Google sitemap inspect 2\" style=\"zoom:22%;\" />\n\nIt's normal it shows `URL is not on Google` because it shouldn't as a sitemap.\n\n2.click `live test` to check the availability.\n\n<img src=\"Google%20sitemap%20inspect%203.png\" alt=\"Google sitemap inspect 3\" style=\"zoom:75%;\" />\n\nIt should be available, then just wait.\n\n#### index Bing to this Blog\n\n1.go to [Bing webmaster](https://www.bing.com/webmasters/) and login\n\n2.connect with google webmaster.\n\n<img src=\"bing%20sitemap.png\" alt=\"bing sitemap\" style=\"zoom:75%;\" />\n\n  \n\n<img src=\"being%20sitemap%20connect%20google.png\" alt=\"being sitemap connect google\" style=\"zoom:75%;\" />\n\n  \n\n<img src=\"bing%20search%20console%20success.png\" alt=\"bing search console success\" style=\"zoom:75%;\" />\n\n#### index baidu to this Blog(not possibly working)\n\ngo to the [baidu search console](https://ziyuan.baidu.com/site/index) , \n\n<img src=\"baidu console.png\" alt=\"baidu console\" style=\"zoom:50%;\" />\n\nClick `添加网站` and input every thing, do similar thing\n\n<img src=\"baidu console varification.png\" alt=\"baidu console varification\" style=\"zoom:50%;\" />\n\nadd sitemap\n\n<img src=\"baidu console sitemap.png\" alt=\"baidu console sitemap\" style=\"zoom:50%;\" />\n\njust wait forever, this could take 2000 years, so give up\n\n### Add copyright statement\n\n1.open file `themes/yilia/layout/_partial/article.ejs` add before `<% if ((theme.reward_type === 2 || (theme.reward_type === 1 && post.reward)) && !index){ %>`\n\n```html\n<!-- add copyright statement -->\n<% if(theme.declare){%>\n    <%- partial('post/declare') %>\n<% } %>\n<!-- end -->\n```\n\n2.create new file `declare.ejs` under `themes/yilia/layout/_partial/post/` with:\n\n```html\n<!--add copyright statement https://github.com/JoeyBling/hexo-theme-yilia-plus/commit/c1215e132f6d5621c5fea83d3c4f7ccbcca074a3-->\n<%\n  var sUrl = url.replace(/index\\.html$/, '');\n  sUrl = /^(http:|https:)\\/\\//.test(sUrl) ? sUrl : 'https:' + sUrl;\n%>\n\n<!-- #copyright setting：0-close statement; 1-declare statement if declare: true in the article header; 2-always declare the copyright -->\n<% if ((theme.declare.declare_type === 2 || (theme.declare.declare_type === 1 && post.declare)) && !index){ %>\n  <div class=\"declare\">\n    <strong class=\"author\">author: </strong>\n    <% if(config.author != undefined){ %>\n      <%= config.author%>\n    <% }else{%>\n      <font color=\"red\">please add right \"author\" name in \"_config.yml\" in the blog root</font>\n    <%}%>\n    <br>\n    <strong class=\"create-time\">posting date: </strong>\n    <%- date(post.date, 'YYYY-MM-DD HH:MM:SS') %>\n    <br>\n    <strong class=\"update-time\">last update: </strong>\n    <%- date(post.updated, 'YYYY-MM-DD HH:MM:SS') %>\n    <br>\n    <strong class=\"article-titles\">article title: </strong>\n    <a href=\"<%= config.url %>/<%= post.path %>\" title=\"<%= post.title %>\" target=\"_blank\"><%= post.title %></a>\n    <br>\n    <strong class=\"article-url\">article link: </strong>\n    <a href=\"<%= config.url %>/<%= post.path %>\" title=\"<%= post.title %>\" target=\"_blank\"><%= config.url %>/<%= post.path %></a>\n    <br>\n    <strong class=\"copyright\">copyright:</strong>\n    This work is licensed under a\n    <a rel=\"license\" href=\"<%= theme.declare.licensee_url%>\" title=\"<%= theme.declare.licensee_alias %>\"><%= theme.declare.licensee_name%></a>\n    licience \n    <% if(theme.declare.licensee_img != undefined){ %>\n      <a rel=\"license\" href=\"<%= theme.declare.licensee_url%>\"><img alt=\"知识共享许可协议\" style=\"border-width:0\" src=\"<%= theme.declare.licensee_img%>\"/></a>\n    <% } %>\n  </div>\n<% } else {%>\n  <div class=\"declare\" hidden=\"hidden\"></div>\n<% } %>\n<!-- add copyright statement -->\n```\n\n3.add stylesheet the end of `themes/yilia/source/main.0cf68a.css` \n\n```css\n/*stylesheet for the delcare*/\n.declare {\n  background-color: #eaeaea;\n  margin-top: 2em;\n  border-left: 3px solid #ff1700;\n  padding: .5em 1em; \n}\n/*stylesheet for the delcare end*/\n```\n\n4.add at the end of `themes/yilia/_config.yml` file:\n\n```\ndeclare:\n  declare_type: 1\n  licensee_url: http://creativecommons.org/licenses/by-nc-sa/4.0/      \n  licensee_name: 'CC BY-NC-SA 4.0'                              \n  licensee_alias: 'CC BY-NC-SA 4.0'     \n  licensee_img: https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\n```\n\n### Add mind-map support\n\n```shell\nnpm install hexo-markmap\n```\n\nDetailed in its [Github](https://github.com/MaxChang3/hexo-markmap)\n\nExample:\n\n```\n{% markmap 300px %}\n- Testa\n  - test1\n  - test2\n- Testb\n  - test1\n  - test2\n{%endmarkmap%}\n```\n\n### Add Latex math support\n\nChange the renderer to the more powerful pandoc:\n\n1.Install pandoc on macOS:\n\n```\ncopybrew install pandoc\n```\n\n2.in the blog root directory uninstall the default renderer then install the pandoc renderer:\n\n```\ncopynpm uninstall hexo-renderer-marked --save\nnpm install hexo-renderer-pandoc --save\n```\n\n3.install the hexo math plugin\n\n```\ncopynpm install hexo-math --save\n```\n\n4.add these lines to the hexo `_config` file\n\n```\ncopymarkdown:\n  plugins:\n    - markdown-it-footnote\n    - markdown-it-sup\n    - markdown-it-sub\n    - markdown-it-abbr\n    - markdown-it-emoji\n    - hexo-math\n```\n\n5.add these lines to the theme `_config` file\n\n```\ncopy# MathJax Support\nmathjax:\n  enable: true\n  per_page: true\n```\n\n6.rebuild the to blog see changes\n\n7.Examples: $this_{is}an\\frac{inline}{equation}$\n$$\n\\begin{equation}\n    \\mathbf{K}_\\mathbf{1}=\\frac{1}{\\Delta r}\\ \\left[\\begin{matrix}\\begin{matrix}-1&1\\\\-1&1\\\\\\end{matrix}&\\ &\\ \\\\\\begin{matrix}\\ &\\ddots\\\\\\end{matrix}&\\begin{matrix}\\ddots&\\ \\\\\\end{matrix}&\\ \\\\\\ &-1\\ &1\\\\\\end{matrix}\\right],\\ \\ {\\ \\mathbf{K}}_\\mathbf{2}=\\frac{1}{\\Delta r}\\ \\left[\\begin{matrix}\\begin{matrix}-1&1\\\\\\ &\\ddots\\\\\\end{matrix}&\\begin{matrix}\\\\\\ddots\\\\\\end{matrix}&\\ \\\\\\begin{matrix}\\ &\\ \\\\\\end{matrix}&-1&1\\ \\\\\\ &-1\\ &1\\\\\\end{matrix}\\right]\n    \\label{K2}\n\\end{equation}\n$$\n\n### The last snapshot\n\nOk, never spend time on a no-longer maintained project. Here's the last figure of it.\n\n<img src=\"Last snapshot.png\" alt=\"Last snapshot\" style=\"zoom:80%;\" />\n\n## Reference\n\nhttps://flatironschool.com/blog/the-benefits-of-blogging-how-and-why-to-keep-a-technical-blog/\n\nhttps://weblog.masukomi.org/2015/10/18/static-vs-dynamic-blogging/\n\nhttps://www.cnblogs.com/aoguai/p/11781505.html\n\nhttps://www.kblog.top/post/30452.html\n\nhttps://wkzqn.gitee.io/2020/02/16/typora%E7%BC%96%E5%86%99hexo%E5%8D%9A%E5%AE%A2%E6%97%B6%E7%9A%84%E5%9B%BE%E7%89%87%E6%98%BE%E7%A4%BA/\n\nhttps://segmentfault.com/a/1190000009478837#articleHeader5\n\nhttps://hilyy.xyz/how-to-allow-google-to-index-your-hexo-blog-website-google-search-console-verification-methods/\n\nhttps://busuanzi.ibruce.info/\n\nhttps://creativecommons.org/choose/results-one?license_code=by-nc-sa&amp;jurisdiction=&amp;version=4.0&amp;lang=en\n\nhttps://www.jcchouinard.com/sitemap-could-not-be-read-couldnt-fetch-in-google-search-console/\n\nhttps://cqh-i.github.io/2019/08/07/hexo-yilia%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E9%9A%90%E8%97%8F%E5%B7%A6%E8%BE%B9%E6%A0%8F%E7%9B%AE%E6%8C%89%E9%92%AE/","slug":"Build-and-configure-a-personal-blog-via-hexo-and-yilia","published":1,"updated":"2022-04-30T08:15:16.219Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl2n5f4080006p9yb6qu44826","content":"<blockquote>\n<p>Technical blog has a hundred benefits and no harm</p>\n<p>This blog records the process of building and customizing this personal blog from 0 to 1</p>\n</blockquote>\n<blockquote>\n<p>Unfortunately, the yilia theme has been no longer updating and it is too buggy right now. I switch to other theme.</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"preliminary\">Preliminary</h2>\n<h3 id=\"why-personal-blog\">Why personal blog</h3>\n<blockquote>\n<p>Keeping a technical blog can be <strong>a great way of documenting your growth as a developer</strong>. This documentation can be particularly useful on a professional level. All software companies want to hire smart, thoughtful, communicative developers who can easily assimilate into a team, and who are ready to both teach and learn</p>\n</blockquote>\n<h3 id=\"static-vs-dynamic-blog\">Static vs dynamic blog</h3>\n<p>there are 2 types of mainstream personal blog: static and dynamic.</p>\n<p>Static is recommended considering its simplicity, 0 maintenance and 0 safety worry.</p>\n<table>\n<colgroup>\n<col style=\"width: 9%\" />\n<col style=\"width: 45%\" />\n<col style=\"width: 45%\" />\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\"></th>\n<th style=\"text-align: left;\">Static blog</th>\n<th style=\"text-align: left;\">Dynamic blog</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Price</td>\n<td style=\"text-align: left;\">low, 0 cost when the traffic is relatively low</td>\n<td style=\"text-align: left;\">High, server is needed, cloud server of high performance is usually very expensive.</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Features</td>\n<td style=\"text-align: left;\">Limited, only third-party services can be used to complete certain \"dynamic\" functions, such as comments</td>\n<td style=\"text-align: left;\">Rich, in WordPress for example, basically any kind of plugins can be found. Featuers such as auto-resizing, media players, multiple authors, scheduled posts, user analysis can be easily realize.</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Speed</td>\n<td style=\"text-align: left;\">Fast</td>\n<td style=\"text-align: left;\">Slow</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Maintainance</td>\n<td style=\"text-align: left;\">0</td>\n<td style=\"text-align: left;\">Need to care about the sever</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Markdown</td>\n<td style=\"text-align: left;\">Supported yet it's the only choice</td>\n<td style=\"text-align: left;\">not supported</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Geeky</td>\n<td style=\"text-align: left;\">YES</td>\n<td style=\"text-align: left;\">No</td>\n</tr>\n</tbody>\n</table>\n<p>And I also chose static because I'm geeky (poor of money) and results-driven (lazy to spend time on maintaining).</p>\n<h2 id=\"build-a-static-blog-via-hexo\">Build a static blog via hexo</h2>\n<h3 id=\"set-environment\">Set environment</h3>\n<p>1.check machine information: macOS on M1 MacBook</p>\n<p>2.Install Nodejs, including node and npm</p>\n<p>open https://nodejs.org/en/download/ and click download</p>\n<p><img src=\"node js install.png\" srcset=\"/img/loading.gif\" lazyload alt=\"node js install\" style=\"zoom:50%;\" /></p>\n<p>3.Install Git</p>\n<p>Aready installed</p>\n<p>4.Open terminal, check node, npm and git versions</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs shell\"><span class=\"hljs-meta\">$ </span><span class=\"language-bash\">npm -v</span>\n<span class=\"hljs-meta\">$ </span><span class=\"language-bash\">node -v</span>\n<span class=\"hljs-meta\">$ </span><span class=\"language-bash\">git --version</span>\n8.3.1\nv16.14.0\ngit version 2.32.0 (Apple Git-132)</code></pre></div>\n<h3 id=\"initialize-blog\">Initialize blog</h3>\n<p>1.install hexo via npm</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs shell\"><span class=\"hljs-meta\">$ </span><span class=\"language-bash\">sudo npm install -g hexo-cli</span>\n<span class=\"hljs-meta\">$ </span><span class=\"language-bash\">hexo -v</span>\nINFO  Validating config\nhexo: 6.0.0\nhexo-cli: 4.3.0\nos: darwin 21.2.0 12.1\n\nnode: 16.14.0\nv8: 9.4.146.24-node.20\nuv: 1.43.0\nzlib: 1.2.11\nbrotli: 1.0.9\nares: 1.18.1\nmodules: 93\nnghttp2: 1.45.1\nnapi: 8\nllhttp: 6.0.4\nopenssl: 1.1.1m+quic\ncldr: 40.0\nicu: 70.1\ntz: 2021a3\nunicode: 14.0\nngtcp2: 0.1.0-DEV\nnghttp3: 0.1.0-DEV</code></pre></div>\n<p>2.create a new folder in terminal and initialize the blog</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs shell\"><span class=\"hljs-meta\">$ </span><span class=\"language-bash\"><span class=\"hljs-built_in\">cd</span> ~/Documents/</span>\n<span class=\"hljs-meta\">$ </span><span class=\"language-bash\">makedir self_blog</span>\n<span class=\"hljs-meta\">$ </span><span class=\"language-bash\"><span class=\"hljs-built_in\">cd</span> self_blog/</span>\n<span class=\"hljs-meta\">$ </span><span class=\"language-bash\">hexo init</span>\nINFO  Cloning hexo-starter https://github.com/hexojs/hexo-starter.gitINFO  Install dependencies⸨#########⠂⠂⠂⠂⠂⠂⠂⠂⠂⸩ ⠹ idealTree:hexo-front-matter: timing idealTree:node_modules/hexo-front-matter Completed in 212msINFO  Start blogging with Hexo!</code></pre></div>\n<p>3.view the blog on localhost, s for start</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs shell\"><span class=\"hljs-meta\">$ </span><span class=\"language-bash\">hexo s</span>\nINFO  Validating config\nINFO  Start processing\nINFO  Hexo is running at http://localhost:4000/ . Press Ctrl+C to stop.</code></pre></div>\n<h3 id=\"write-first-blog\">Write first blog</h3>\n<p>1.write a new blog, n for new</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs shell\"><span class=\"hljs-meta\">$ </span><span class=\"language-bash\">hexo n <span class=\"hljs-string\">&#x27;Hello ShouRou&#x27;</span></span>\nINFO  Validating config\nINFO  Created: ~/Documents/self_blog/source/_posts/Hello-ShouRou.md</code></pre></div>\n<p>the blog can be written on any editor, Typora in use.</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs shell\">open ~/Documents/self_blog/source/_posts/Hello-ShouRou.md</code></pre></div>\n<p>2.clean cache(not necessary)</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs shell\"><span class=\"hljs-meta\">$ </span><span class=\"language-bash\">hexo clean</span></code></pre></div>\n<p>3.generate the blog, g for generate</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs shell\"><span class=\"hljs-meta\">$ </span><span class=\"language-bash\">hexo g</span>\nINFO  Validating config\nINFO  Start processing\nINFO  Files loaded in 61 ms\n(node:10719) Warning: Accessing non-existent property &#x27;lineno&#x27; of module exports inside circular dependency\n(Use `node --trace-warnings ...` to show where the warning was created)\n(node:10719) Warning: Accessing non-existent property &#x27;column&#x27; of module exports inside circular dependency\n(node:10719) Warning: Accessing non-existent property &#x27;filename&#x27; of module exports inside circular dependency\n(node:10719) Warning: Accessing non-existent property &#x27;lineno&#x27; of module exports inside circular dependency\n(node:10719) Warning: Accessing non-existent property &#x27;column&#x27; of module exports inside circular dependency\n(node:10719) Warning: Accessing non-existent property &#x27;filename&#x27; of module exports inside circular dependency\nINFO  Generated: archives/2022/index.html\nINFO  Generated: archives/index.html\nINFO  Generated: js/script.js\nINFO  Generated: fancybox/jquery.fancybox.min.css\nINFO  Generated: index.html\nINFO  Generated: css/style.css\nINFO  Generated: css/fonts/fontawesome-webfont.woff2\nINFO  Generated: fancybox/jquery.fancybox.min.js\nINFO  Generated: js/jquery-3.4.1.min.js\nINFO  Generated: archives/2022/02/index.html\nINFO  Generated: css/fonts/FontAwesome.otf\nINFO  Generated: css/fonts/fontawesome-webfont.woff\nINFO  Generated: css/fonts/fontawesome-webfont.eot\nINFO  Generated: css/fonts/fontawesome-webfont.ttf\nINFO  Generated: css/images/banner.jpg\nINFO  Generated: 2022/02/22/hello-world/index.html\nINFO  Generated: css/fonts/fontawesome-webfont.svg\nINFO  Generated: 2022/02/22/Hello-ShouRou/index.html\nINFO  18 files generated in 161 ms</code></pre></div>\n<h3 id=\"deploy-to-remote-github\">Deploy to remote (GitHub)</h3>\n<p>1.Create a new repository with the name of $username.github.io</p>\n<p><img src=\"github page.png\" srcset=\"/img/loading.gif\" lazyload alt=\"github page\" style=\"zoom:50%;\" /></p>\n<p>use default setting</p>\n<p>2.open terminal, install plugin of deploying to git</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs shell\"><span class=\"hljs-meta\">$ </span><span class=\"language-bash\">npm install --save hexo-deployer-git</span></code></pre></div>\n<p>3.Open the <code>_config.yml</code> file in the blog <code>root</code> directory, add these lines afterwards</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs yml\"><span class=\"hljs-attr\">deploy:</span>\n <span class=\"hljs-attr\">type:</span> <span class=\"hljs-string\">git</span>\n <span class=\"hljs-attr\">repo:</span> <span class=\"hljs-string\">git@github.com:DaydreamAtNight/DaydreamAtNight.github.io.git</span>\n <span class=\"hljs-attr\">branch:</span> <span class=\"hljs-string\">master</span></code></pre></div>\n<p>4.Go to the blog <code>root</code>, deploy the blog to remote, d for deploy</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs shell\"><span class=\"hljs-meta\">$ </span><span class=\"language-bash\">hexo clean</span>\n<span class=\"hljs-meta\">$ </span><span class=\"language-bash\">hexo g</span>\n<span class=\"hljs-meta\">$ </span><span class=\"language-bash\">hexo d</span></code></pre></div>\n<p>Open https://daydreamatnight.github.io/ to see if it works</p>\n<h2 id=\"change-theme-to-yilia\">Change theme to yilia</h2>\n<p>default theme of hexo is called landscape and it's not beautiful enough to most of the people. Yilia is a fast, simple, elegant and popular theme. Thought it has not been updated since Nov 2017, it still a good choice for fresh bloggers.</p>\n<h3 id=\"download-and-deploy-yilia\">Download and deploy yilia</h3>\n<p>1.go to the blog <code>root</code></p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs shell\"><span class=\"hljs-meta\">$ </span><span class=\"language-bash\">git <span class=\"hljs-built_in\">clone</span> https://github.com/litten/hexo-theme-yilia theme/yilia</span></code></pre></div>\n<p>2.eidt the <code>_config.yml</code> file, add</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs yml\"><span class=\"hljs-attr\">theme:</span> <span class=\"hljs-string\">yilia</span></code></pre></div>\n<p>3.clean and deploy hexo</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs shell\"><span class=\"hljs-meta\">$ </span><span class=\"language-bash\">hexo clean</span>\n<span class=\"hljs-meta\">$ </span><span class=\"language-bash\">hexo g</span>\n<span class=\"hljs-meta\">$ </span><span class=\"language-bash\">hexo d</span></code></pre></div>\n<h3 id=\"basic-customize-yillia\">Basic customize yillia</h3>\n<h4 id=\"activate-aboutme-left-slider-button\">Activate <code>aboutme</code> ‘left slider’ button</h4>\n<p>1.go to terminal run</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs shell\"><span class=\"hljs-meta\">$ </span><span class=\"language-bash\">npm i hexo-generator-json-content --save</span></code></pre></div>\n<p>2.go to the blog <code>root</code> directory, add these lines to the <code>_config.yml</code> file</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs yml\"><span class=\"hljs-attr\">jsonContent:</span>\n    <span class=\"hljs-attr\">meta:</span> <span class=\"hljs-literal\">false</span>\n    <span class=\"hljs-attr\">pages:</span> <span class=\"hljs-literal\">false</span>\n    <span class=\"hljs-attr\">posts:</span>\n      <span class=\"hljs-attr\">title:</span> <span class=\"hljs-literal\">true</span>\n      <span class=\"hljs-attr\">date:</span> <span class=\"hljs-literal\">true</span>\n      <span class=\"hljs-attr\">path:</span> <span class=\"hljs-literal\">true</span>\n      <span class=\"hljs-attr\">text:</span> <span class=\"hljs-literal\">false</span>\n      <span class=\"hljs-attr\">raw:</span> <span class=\"hljs-literal\">false</span>\n      <span class=\"hljs-attr\">content:</span> <span class=\"hljs-literal\">false</span>\n      <span class=\"hljs-attr\">slug:</span> <span class=\"hljs-literal\">false</span>\n      <span class=\"hljs-attr\">updated:</span> <span class=\"hljs-literal\">false</span>\n      <span class=\"hljs-attr\">comments:</span> <span class=\"hljs-literal\">false</span>\n      <span class=\"hljs-attr\">link:</span> <span class=\"hljs-literal\">false</span>\n      <span class=\"hljs-attr\">permalink:</span> <span class=\"hljs-literal\">false</span>\n      <span class=\"hljs-attr\">excerpt:</span> <span class=\"hljs-literal\">false</span>\n      <span class=\"hljs-attr\">categories:</span> <span class=\"hljs-literal\">false</span>\n      <span class=\"hljs-attr\">tags:</span> <span class=\"hljs-literal\">true</span></code></pre></div>\n<h4 id=\"customize-avatar\">Customize avatar</h4>\n<p>put the avatar file in directory <code>themes/yilia/source/img</code></p>\n<blockquote>\n<p>do not add to the public repository directly, or the img get cleaned every time running <code>hexo clean</code> , need to upload to the same dir again after this command.</p>\n</blockquote>\n<h4 id=\"set-favicon-icon-on-the-tab-of-website\">Set favicon (icon on the tab of website)</h4>\n<p>put the favicon img in directory <code>themes/yilia/source/img</code></p>\n<p><a href=\"https://www.bitbug.net/\">Bitbug</a> is a way of converting image into .ico file.</p>\n<h4 id=\"other-configuration\">Other configuration</h4>\n<p>Set file of yillia is in <code>themes/yilia/_config.yml</code> as:</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs yml\"><span class=\"hljs-comment\"># Header</span>\n<span class=\"hljs-attr\">author:</span> <span class=\"hljs-string\">Ryan</span> <span class=\"hljs-string\">LI</span>\n<span class=\"hljs-attr\">subtitle:</span> <span class=\"hljs-string\">&#x27;Daydreaming at night&#x27;</span>\n<span class=\"hljs-attr\">menu:</span>\n  <span class=\"hljs-attr\">main:</span> <span class=\"hljs-string\">/</span>\n  <span class=\"hljs-attr\">archives:</span> <span class=\"hljs-string\">/archives/index.html</span>\n  <span class=\"hljs-attr\">learn:</span> <span class=\"hljs-string\">/tags/learn/</span>\n\n<span class=\"hljs-comment\"># SubNav</span>\n<span class=\"hljs-attr\">subnav:</span>\n  <span class=\"hljs-attr\">github:</span> <span class=\"hljs-string\">&quot;https://github.com/DaydreamAtNight&quot;</span>\n  <span class=\"hljs-comment\"># weibo: &quot;#&quot;</span>\n  <span class=\"hljs-comment\"># rss: &quot;#&quot;</span>\n  <span class=\"hljs-comment\"># zhihu: &quot;#&quot;</span>\n  <span class=\"hljs-comment\">#qq: &quot;#&quot;</span>\n  <span class=\"hljs-comment\">#weixin: &quot;#&quot;</span>\n  <span class=\"hljs-comment\">#jianshu: &quot;#&quot;</span>\n  <span class=\"hljs-comment\">#douban: &quot;#&quot;</span>\n  <span class=\"hljs-comment\">#segmentfault: &quot;#&quot;</span>\n  <span class=\"hljs-comment\">#bilibili: &quot;#&quot;</span>\n  <span class=\"hljs-comment\">#acfun: &quot;#&quot;</span>\n  <span class=\"hljs-attr\">mail:</span> <span class=\"hljs-string\">&quot;mailto:lishoushou2019@gmail.com&quot;</span>\n  <span class=\"hljs-comment\">#facebook: &quot;#&quot;</span>\n  <span class=\"hljs-comment\">#google: &quot;#&quot;</span>\n  <span class=\"hljs-comment\">#twitter: &quot;#&quot;</span>\n  <span class=\"hljs-comment\">#linkedin: &quot;#&quot;</span>\n\n<span class=\"hljs-attr\">rss:</span> <span class=\"hljs-string\">/atom.xml</span>\n\n<span class=\"hljs-comment\"># 是否需要修改 root 路径</span>\n<span class=\"hljs-comment\"># 如果您的网站存放在子目录中，例如 http://yoursite.com/blog，</span>\n<span class=\"hljs-comment\"># 请将您的 url 设为 http://yoursite.com/blog 并把 root 设为 /blog/。</span>\n<span class=\"hljs-attr\">root:</span> <span class=\"hljs-string\">/</span>\n\n<span class=\"hljs-comment\"># Content</span>\n\n<span class=\"hljs-comment\"># 文章太长，截断按钮文字</span>\n<span class=\"hljs-comment\"># excerpt_link: more</span>\n<span class=\"hljs-comment\"># 文章卡片右下角常驻链接，不需要请设置为false</span>\n<span class=\"hljs-attr\">show_all_link:</span> <span class=\"hljs-string\">&#x27;show all&#x27;</span>\n<span class=\"hljs-comment\"># 数学公式</span>\n<span class=\"hljs-attr\">mathjax:</span> <span class=\"hljs-literal\">false</span>\n<span class=\"hljs-comment\"># 是否在新窗口打开链接</span>\n<span class=\"hljs-attr\">open_in_new:</span> <span class=\"hljs-literal\">false</span>\n\n<span class=\"hljs-comment\"># 打赏</span>\n<span class=\"hljs-comment\"># 打赏type设定：0-关闭打赏； 1-文章对应的md文件里有reward:true属性，才有打赏； 2-所有文章均有打赏</span>\n<span class=\"hljs-attr\">reward_type:</span> <span class=\"hljs-number\">0</span>\n<span class=\"hljs-comment\"># # 打赏wording</span>\n<span class=\"hljs-comment\"># reward_wording: &#x27;谢谢你请我吃糖果&#x27;</span>\n<span class=\"hljs-comment\"># # 支二维码图片地址，跟你设置头像的方式一样。比如：/assets/img/alipay.jpg</span>\n<span class=\"hljs-comment\"># alipay: </span>\n<span class=\"hljs-comment\"># # 微信二维码图片地址</span>\n<span class=\"hljs-comment\"># weixin: </span>\n\n<span class=\"hljs-comment\"># 目录</span>\n<span class=\"hljs-comment\"># 目录设定：0-不显示目录； 1-文章对应的md文件里有toc:true属性，才有目录； 2-所有文章均显示目录</span>\n<span class=\"hljs-attr\">toc:</span> <span class=\"hljs-number\">1</span>\n<span class=\"hljs-comment\"># 根据自己的习惯来设置，如果你的目录标题习惯有标号，置为true即可隐藏hexo重复的序号；否则置为false</span>\n<span class=\"hljs-attr\">toc_hide_index:</span> <span class=\"hljs-literal\">true</span>\n<span class=\"hljs-comment\"># 目录为空时的提示</span>\n<span class=\"hljs-attr\">toc_empty_wording:</span> <span class=\"hljs-string\">&#x27;directery none exist&#x27;</span>\n\n<span class=\"hljs-comment\"># 是否有快速回到顶部的按钮</span>\n<span class=\"hljs-attr\">top:</span> <span class=\"hljs-literal\">true</span>\n\n<span class=\"hljs-comment\"># Miscellaneous</span>\n<span class=\"hljs-attr\">baidu_analytics:</span> <span class=\"hljs-string\">&#x27;&#x27;</span>\n<span class=\"hljs-attr\">google_analytics:</span> <span class=\"hljs-string\">&#x27;&#x27;</span>\n<span class=\"hljs-attr\">favicon:</span> <span class=\"hljs-string\">/img/favicon.ico</span>\n\n<span class=\"hljs-comment\">#你的头像url</span>\n<span class=\"hljs-attr\">avatar:</span> <span class=\"hljs-string\">/img/avatar.jpeg</span>\n\n<span class=\"hljs-comment\">#是否开启分享</span>\n<span class=\"hljs-comment\"># share_jia: true</span>\n\n<span class=\"hljs-comment\"># #评论：1、多说；2、网易云跟帖；3、畅言；4、Disqus；5、Gitment</span>\n<span class=\"hljs-comment\"># #不需要使用某项，直接设置值为false，或注释掉</span>\n<span class=\"hljs-comment\"># #具体请参考wiki：https://github.com/litten/hexo-theme-yilia/wiki/</span>\n\n<span class=\"hljs-comment\"># #1、多说</span>\n<span class=\"hljs-comment\"># duoshuo: false</span>\n\n<span class=\"hljs-comment\"># #2、网易云跟帖</span>\n<span class=\"hljs-comment\"># wangyiyun: false</span>\n\n<span class=\"hljs-comment\"># #3、畅言</span>\n<span class=\"hljs-comment\"># changyan_appid: false</span>\n<span class=\"hljs-comment\"># changyan_conf: false</span>\n\n<span class=\"hljs-comment\"># #4、Disqus 在hexo根目录的config里也有disqus_shortname字段，优先使用yilia的</span>\n<span class=\"hljs-comment\"># disqus: false</span>\n\n<span class=\"hljs-comment\"># #5、Gitment</span>\n<span class=\"hljs-comment\"># gitment_owner: false      #你的 GitHub ID</span>\n<span class=\"hljs-comment\"># gitment_repo: &#x27;&#x27;          #存储评论的 repo</span>\n<span class=\"hljs-comment\"># gitment_oauth:</span>\n<span class=\"hljs-comment\">#   client_id: &#x27;&#x27;           #client ID</span>\n<span class=\"hljs-comment\">#   client_secret: &#x27;&#x27;       #client secret</span>\n\n<span class=\"hljs-comment\"># 样式定制 - 一般不需要修改，除非有很强的定制欲望…</span>\n<span class=\"hljs-attr\">style:</span>\n  <span class=\"hljs-comment\"># 头像上面的背景颜色</span>\n  <span class=\"hljs-attr\">header:</span> <span class=\"hljs-string\">&#x27;#ece0cf&#x27;</span>\n  <span class=\"hljs-comment\"># 右滑板块背景</span>\n  <span class=\"hljs-attr\">slider:</span> <span class=\"hljs-string\">&#x27;linear-gradient(45deg,#b4a698,#ece0cf)&#x27;</span>\n\n<span class=\"hljs-comment\"># slider的设置</span>\n<span class=\"hljs-attr\">slider:</span>\n  <span class=\"hljs-comment\"># 是否默认展开tags板块</span>\n  <span class=\"hljs-attr\">showTags:</span> <span class=\"hljs-literal\">false</span>\n\n<span class=\"hljs-comment\"># 智能菜单</span>\n<span class=\"hljs-comment\"># 如不需要，将该对应项置为false</span>\n<span class=\"hljs-comment\"># 比如</span>\n<span class=\"hljs-comment\">#smart_menu:</span>\n<span class=\"hljs-comment\">#  friends: false</span>\n<span class=\"hljs-attr\">smart_menu:</span>\n  <span class=\"hljs-attr\">innerArchive:</span> <span class=\"hljs-string\">&#x27;All articles&#x27;</span>\n  <span class=\"hljs-comment\"># friends: &#x27;友链&#x27;</span>\n  <span class=\"hljs-attr\">aboutme:</span> <span class=\"hljs-string\">&#x27;About me&#x27;</span>\n\n<span class=\"hljs-comment\"># friends:</span>\n<span class=\"hljs-comment\">#   友情链接1: http://localhost:4000/</span>\n<span class=\"hljs-comment\">#   友情链接2: http://localhost:4000/</span>\n<span class=\"hljs-comment\">#   友情链接3: http://localhost:4000/</span>\n<span class=\"hljs-comment\">#   友情链接4: http://localhost:4000/</span>\n<span class=\"hljs-comment\">#   友情链接5: http://localhost:4000/</span>\n<span class=\"hljs-comment\">#   友情链接6: http://localhost:4000/</span>\n\n<span class=\"hljs-attr\">aboutme:</span> <span class=\"hljs-string\">Stay</span> <span class=\"hljs-string\">hungry,</span> <span class=\"hljs-string\">stay</span> <span class=\"hljs-string\">fullish</span></code></pre></div>\n<h2 id=\"advance-customize\">Advance customize</h2>\n<h3 id=\"stop-visit-litten.me9005\">Stop visit litten.me:9005</h3>\n<p>Sometimes the user's client information is collected, see <a href=\"https://github.com/litten/hexo-theme-yilia/issues/528\">here</a> for details.</p>\n<p>Stop reporting by clear the contents in <code>themes/yilia/source-src/js/report.js</code></p>\n<h3 id=\"limit-display-numbers-on-the-main-page\">Limit display numbers on the main page</h3>\n<p>Simply insert <code>&lt;! -- more --&gt;</code> to show only what comes before it while collapse the afterwards, click on the article title to read it in full.</p>\n<h3 id=\"easily-add-pics-to-blogs-via-hexo-renderer-marked-plugin\">Easily add pics to blogs via hexo-renderer-marked plugin</h3>\n<p>1.find <code>post_asset_folder</code> in <code>_config.yml</code> file in the blog <code>root</code> directory, set to be true</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs yml\"><span class=\"hljs-string\">post_asset_folder:true</span></code></pre></div>\n<p>2.Install plugin</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs shell\">npm install hexo-renderer-marked --save</code></pre></div>\n<p>3.change <code>_config.yml</code> in blog <code>root</code> directory as</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs yml\"><span class=\"hljs-attr\">post_asset_folder:</span> <span class=\"hljs-literal\">true</span>\n<span class=\"hljs-attr\">marked:</span>\n  <span class=\"hljs-attr\">prependRoot:</span> <span class=\"hljs-literal\">true</span>\n  <span class=\"hljs-attr\">postAsset:</span> <span class=\"hljs-literal\">true</span></code></pre></div>\n<p>then img can be easily add with <code>![img description](img.png)</code> after add the image to the folder with the same name as the article in <code>/source/_posts/</code></p>\n<p>4.change Typora pereference as</p>\n<p><img src=\"typora setting.png\" srcset=\"/img/loading.gif\" lazyload alt=\"typora setting\" style=\"zoom:50%;\" /></p>\n<p>img can drag into typro, yet blogname need to be deleted before deploying</p>\n<h3 id=\"show-number-of-articles-and-words-on-the-left-panel\">Show number of articles and words on the left panel</h3>\n<p>1.add wordcount plugin in terminal</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs shell\">npm i --save hexo-wordcount</code></pre></div>\n<p>2.change <code>themes/yilia/layout/_partial/left-col.ejs</code></p>\n<p>after</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs html\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">nav</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;header-menu&quot;</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">ul</span>&gt;</span>\n    &lt;% for (var i in theme.menu)&#123; %&gt;\n      <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">li</span>&gt;</span><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">a</span> <span class=\"hljs-attr\">href</span>=<span class=\"hljs-string\">&quot;&lt;%- url_for(theme.menu[i]) %&gt;&quot;</span>&gt;</span>&lt;%= i %&gt;<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">a</span>&gt;</span><span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">li</span>&gt;</span>\n    &lt;%&#125;%&gt;\n  <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">ul</span>&gt;</span>\n<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">nav</span>&gt;</span></code></pre></div>\n<p>add</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs html\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">span</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;post-count&quot;</span>&gt;</span>&lt;%=site.posts.length%&gt; articles\n\t\t\t<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">span</span>&gt;</span>&lt;%= totalcount(site, &#x27;0,0.0a&#x27;) %&gt;<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">span</span>&gt;</span> words<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">span</span>&gt;</span></code></pre></div>\n<p>add style sheet in <code>themes/yilia/source/main.0cf68a.css</code></p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs css\"><span class=\"hljs-selector-class\">.post-count</span>&#123;\n  <span class=\"hljs-attribute\">font-size</span>: <span class=\"hljs-number\">12px</span>;\n  <span class=\"hljs-attribute\">color</span>: <span class=\"hljs-number\">#696969</span>;\n&#125;</code></pre></div>\n<h3 id=\"show-number-of-visits-in-the-footer\">Show number of visits in the footer</h3>\n<p><a href=\"https://busuanzi.ibruce.info/\">busuanzi</a> is in use, which is super easy to deploy</p>\n<p>change <code>themes/yilia/layout/_partial/footer.ejs</code> as</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs html\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">footer</span> <span class=\"hljs-attr\">id</span>=<span class=\"hljs-string\">&quot;footer&quot;</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;outer&quot;</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">id</span>=<span class=\"hljs-string\">&quot;footer-info&quot;</span>&gt;</span>\n    \t<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;footer-left&quot;</span>&gt;</span>\n    \t\t<span class=\"hljs-comment\">&lt;!-- total visits number --&gt;</span>\n          &lt;% if (theme.busuanzi &amp;&amp; theme.busuanzi.enable)&#123; %&gt;\n            <span class=\"hljs-comment\">&lt;!-- busuanzi statistics --&gt;</span>\n            <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">span</span> <span class=\"hljs-attr\">id</span>=<span class=\"hljs-string\">&quot;busuanzi_value_site_pv&quot;</span>&gt;</span><span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">span</span>&gt;</span><span class=\"hljs-symbol\">&amp;nbsp;</span>visits in total\n            <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">script</span> <span class=\"hljs-attr\">async</span> <span class=\"hljs-attr\">src</span>=<span class=\"hljs-string\">&quot;//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js&quot;</span>&gt;</span><span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">script</span>&gt;</span>\n          &lt;% &#125; %&gt;\n        <span class=\"hljs-comment\">&lt;!-- end --&gt;</span>\n    \t<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">div</span>&gt;</span>\n      \t<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;footer-right&quot;</span>&gt;</span>\n      \t\t<span class=\"hljs-symbol\">&amp;copy;</span> &lt;%= date(new Date(), &#x27;YYYY&#x27;) %&gt; &lt;%= config.author || config.title %&gt;\n      \t<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">div</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">div</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">div</span>&gt;</span>\n<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">footer</span>&gt;</span></code></pre></div>\n<p>and add</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs yml\"><span class=\"hljs-attr\">busuanzi:</span>\n  <span class=\"hljs-attr\">enable:</span> <span class=\"hljs-literal\">true</span></code></pre></div>\n<h3 id=\"add-button-of-hiding-the-left-panel\">Add button of hiding the left panel</h3>\n<p>Refer to <a href=\"https://cqh-i.github.io/2019/08/07/hexo-yilia%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E9%9A%90%E8%97%8F%E5%B7%A6%E8%BE%B9%E6%A0%8F%E7%9B%AE%E6%8C%89%E9%92%AE/\">hexo yilia主题添加隐藏左边栏目按钮</a></p>\n<p>1.add style list to file <code>/themes/yilia/source/main.0cf68a.css</code></p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs css\"><span class=\"hljs-comment\">/*stylesheet for hide the left panel*/</span>\n<span class=\"hljs-selector-class\">.mymenucontainer</span> &#123;\n\t<span class=\"hljs-attribute\">display</span>:block;\n\t<span class=\"hljs-attribute\">cursor</span>:pointer;\n\t<span class=\"hljs-attribute\">left</span>:<span class=\"hljs-number\">0</span>;\n\t<span class=\"hljs-attribute\">top</span>:<span class=\"hljs-number\">0</span>;\n\t<span class=\"hljs-attribute\">width</span>:<span class=\"hljs-number\">35px</span>;\n\t<span class=\"hljs-attribute\">height</span>:<span class=\"hljs-number\">35px</span>;\n\t<span class=\"hljs-attribute\">z-index</span>:<span class=\"hljs-number\">9999</span>;\n\t<span class=\"hljs-attribute\">position</span>:fixed;\n&#125;\n<span class=\"hljs-selector-class\">.bar1</span> &#123;\n\t<span class=\"hljs-attribute\">width</span>:<span class=\"hljs-number\">35px</span>;\n\t<span class=\"hljs-attribute\">height</span>:<span class=\"hljs-number\">3px</span>;\n\t<span class=\"hljs-attribute\">border-radius</span>:<span class=\"hljs-number\">3px</span>;\n\t<span class=\"hljs-attribute\">background-color</span>:<span class=\"hljs-number\">#8E6D51</span>;\n\t<span class=\"hljs-attribute\">margin</span>:<span class=\"hljs-number\">6px</span> <span class=\"hljs-number\">0</span>;\n\t<span class=\"hljs-attribute\">transition</span>:<span class=\"hljs-number\">0.1s</span>;\n\t-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">rotate</span>(-<span class=\"hljs-number\">45deg</span>) <span class=\"hljs-built_in\">translate</span>(-<span class=\"hljs-number\">8px</span>,<span class=\"hljs-number\">8px</span>);\n\t<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">rotate</span>(-<span class=\"hljs-number\">45deg</span>) <span class=\"hljs-built_in\">translate</span>(-<span class=\"hljs-number\">8px</span>,<span class=\"hljs-number\">8px</span>);\n&#125;\n<span class=\"hljs-selector-class\">.bar2</span> &#123;\n\t<span class=\"hljs-attribute\">width</span>:<span class=\"hljs-number\">35px</span>;\n\t<span class=\"hljs-attribute\">height</span>:<span class=\"hljs-number\">3px</span>;\n\t<span class=\"hljs-attribute\">border-radius</span>:<span class=\"hljs-number\">3px</span>;\n\t<span class=\"hljs-attribute\">background-color</span>:<span class=\"hljs-number\">#8E6D51</span>;\n\t<span class=\"hljs-attribute\">margin</span>:<span class=\"hljs-number\">6px</span> <span class=\"hljs-number\">0</span>;\n\t<span class=\"hljs-attribute\">transition</span>:<span class=\"hljs-number\">0.1s</span>;\n\t<span class=\"hljs-attribute\">opacity</span>:<span class=\"hljs-number\">0</span>;\n&#125;\n<span class=\"hljs-selector-class\">.bar3</span> &#123;\n\t<span class=\"hljs-attribute\">width</span>:<span class=\"hljs-number\">35px</span>;\n\t<span class=\"hljs-attribute\">height</span>:<span class=\"hljs-number\">3px</span>;\n\t<span class=\"hljs-attribute\">border-radius</span>:<span class=\"hljs-number\">3px</span>;\n\t<span class=\"hljs-attribute\">background-color</span>:<span class=\"hljs-number\">#8E6D51</span>;\n\t<span class=\"hljs-attribute\">margin</span>:<span class=\"hljs-number\">6px</span> <span class=\"hljs-number\">0</span>;\n\t<span class=\"hljs-attribute\">transition</span>:<span class=\"hljs-number\">0.1s</span>;\n\t-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">rotate</span>(<span class=\"hljs-number\">45deg</span>) <span class=\"hljs-built_in\">translate</span>(-<span class=\"hljs-number\">4px</span>,-<span class=\"hljs-number\">6px</span>);\n\t<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">rotate</span>(<span class=\"hljs-number\">45deg</span>) <span class=\"hljs-built_in\">translate</span>(-<span class=\"hljs-number\">4px</span>,-<span class=\"hljs-number\">6px</span>);\n&#125;\n<span class=\"hljs-selector-class\">.change</span> <span class=\"hljs-selector-class\">.bar1</span> &#123;\n\t-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">rotate</span>(<span class=\"hljs-number\">0deg</span>) <span class=\"hljs-built_in\">translate</span>(<span class=\"hljs-number\">0px</span>,<span class=\"hljs-number\">0px</span>);\n\t<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">rotate</span>(<span class=\"hljs-number\">0deg</span>) <span class=\"hljs-built_in\">translate</span>(<span class=\"hljs-number\">0px</span>,<span class=\"hljs-number\">0px</span>);\n&#125;\n<span class=\"hljs-selector-class\">.change</span> <span class=\"hljs-selector-class\">.bar2</span> &#123;\n\t<span class=\"hljs-attribute\">opacity</span>:<span class=\"hljs-number\">1</span>;\n&#125;\n<span class=\"hljs-selector-class\">.change</span> <span class=\"hljs-selector-class\">.bar3</span> &#123;\n\t-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">rotate</span>(<span class=\"hljs-number\">0deg</span>) <span class=\"hljs-built_in\">translate</span>(<span class=\"hljs-number\">0px</span>,<span class=\"hljs-number\">0px</span>);\n\t<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">rotate</span>(<span class=\"hljs-number\">0deg</span>) <span class=\"hljs-built_in\">translate</span>(<span class=\"hljs-number\">0px</span>,<span class=\"hljs-number\">0px</span>);\n&#125;\n<span class=\"hljs-comment\">/*stylesheet for hide the left panel end*/</span></code></pre></div>\n<p>2.go to <code>/themes/yilia/layout/layout.ejs</code> add before <code>&lt;div class=\"left-col\"</code></p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs html\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;mymenucontainer&quot;</span> <span class=\"hljs-attr\">onclick</span>=<span class=\"hljs-string\">&quot;myFunction(this)&quot;</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;bar1&quot;</span>&gt;</span><span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">div</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;bar2&quot;</span>&gt;</span><span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">div</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;bar3&quot;</span>&gt;</span><span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">div</span>&gt;</span>\n<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">div</span>&gt;</span></code></pre></div>\n<p>3.add between <code>&lt;/body&gt;</code> and <code>&lt;/html&gt;</code></p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs js\">&lt;script&gt;\n    <span class=\"hljs-keyword\">var</span> hide = <span class=\"hljs-literal\">false</span>;\n    <span class=\"hljs-keyword\">function</span> <span class=\"hljs-title function_\">myFunction</span>(<span class=\"hljs-params\">x</span>) &#123;\n        x.<span class=\"hljs-property\">classList</span>.<span class=\"hljs-title function_\">toggle</span>(<span class=\"hljs-string\">&quot;change&quot;</span>);\n        <span class=\"hljs-keyword\">if</span>(hide == <span class=\"hljs-literal\">false</span>)&#123;\n            $(<span class=\"hljs-string\">&quot;.left-col&quot;</span>).<span class=\"hljs-title function_\">css</span>(<span class=\"hljs-string\">&#x27;display&#x27;</span>, <span class=\"hljs-string\">&#x27;none&#x27;</span>);\n            $(<span class=\"hljs-string\">&quot;.mid-col&quot;</span>).<span class=\"hljs-title function_\">css</span>(<span class=\"hljs-string\">&quot;left&quot;</span>, <span class=\"hljs-number\">6</span>);\n            $(<span class=\"hljs-string\">&quot;.tools-col&quot;</span>).<span class=\"hljs-title function_\">css</span>(<span class=\"hljs-string\">&#x27;display&#x27;</span>, <span class=\"hljs-string\">&#x27;none&#x27;</span>);\n            $(<span class=\"hljs-string\">&quot;.tools-col.hide&quot;</span>).<span class=\"hljs-title function_\">css</span>(<span class=\"hljs-string\">&#x27;display&#x27;</span>, <span class=\"hljs-string\">&#x27;none&#x27;</span>);\n            hide = <span class=\"hljs-literal\">true</span>;\n        &#125;<span class=\"hljs-keyword\">else</span>&#123;\n            $(<span class=\"hljs-string\">&quot;.left-col&quot;</span>).<span class=\"hljs-title function_\">css</span>(<span class=\"hljs-string\">&#x27;display&#x27;</span>, <span class=\"hljs-string\">&#x27;&#x27;</span>);\n            $(<span class=\"hljs-string\">&quot;.mid-col&quot;</span>).<span class=\"hljs-title function_\">css</span>(<span class=\"hljs-string\">&quot;left&quot;</span>, <span class=\"hljs-number\">300</span>);\n            $(<span class=\"hljs-string\">&quot;.tools-col&quot;</span>).<span class=\"hljs-title function_\">css</span>(<span class=\"hljs-string\">&#x27;display&#x27;</span>, <span class=\"hljs-string\">&#x27;&#x27;</span>);\n            $(<span class=\"hljs-string\">&quot;.tools-col.hide&quot;</span>).<span class=\"hljs-title function_\">css</span>(<span class=\"hljs-string\">&#x27;display&#x27;</span>, <span class=\"hljs-string\">&#x27;&#x27;</span>);\n            hide = <span class=\"hljs-literal\">false</span>;\n        &#125;\n    &#125;\n&lt;/script&gt;</code></pre></div>\n<h3 id=\"beautiful-contents-navigation-in-articles\">Beautiful contents navigation in articles</h3>\n<p>Default navigator is kind of ugly so found a more beautiful version, to use default version, simply change <code>toc: 2</code> in file <code>themes/yilia/_config.yml</code></p>\n<p>1.add this block at the end of <code>themes/yilia/source/main.0cf68a.css</code></p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs css\"><span class=\"hljs-comment\">/* navigator */</span>\n<span class=\"hljs-selector-id\">#container</span> <span class=\"hljs-selector-class\">.show-toc-btn</span>,<span class=\"hljs-selector-id\">#container</span> <span class=\"hljs-selector-class\">.toc-article</span>&#123;<span class=\"hljs-attribute\">display</span>:block&#125;\n<span class=\"hljs-selector-class\">.toc-article</span>&#123;<span class=\"hljs-attribute\">z-index</span>:<span class=\"hljs-number\">100</span>;<span class=\"hljs-attribute\">background</span>:<span class=\"hljs-number\">#fff</span>;<span class=\"hljs-attribute\">border</span>:<span class=\"hljs-number\">1px</span> solid <span class=\"hljs-number\">#ccc</span>;<span class=\"hljs-attribute\">max-width</span>:<span class=\"hljs-number\">250px</span>;<span class=\"hljs-attribute\">min-width</span>:<span class=\"hljs-number\">150px</span>;<span class=\"hljs-attribute\">max-height</span>:<span class=\"hljs-number\">500px</span>;<span class=\"hljs-attribute\">overflow-y</span>:auto;-webkit-<span class=\"hljs-attribute\">box-shadow</span>:<span class=\"hljs-number\">5px</span> <span class=\"hljs-number\">5px</span> <span class=\"hljs-number\">2px</span> <span class=\"hljs-number\">#ccc</span>;<span class=\"hljs-attribute\">box-shadow</span>:<span class=\"hljs-number\">5px</span> <span class=\"hljs-number\">5px</span> <span class=\"hljs-number\">2px</span> <span class=\"hljs-number\">#ccc</span>;<span class=\"hljs-attribute\">font-size</span>:<span class=\"hljs-number\">12px</span>;<span class=\"hljs-attribute\">padding</span>:<span class=\"hljs-number\">10px</span>;<span class=\"hljs-attribute\">position</span>:fixed;<span class=\"hljs-attribute\">right</span>:<span class=\"hljs-number\">35px</span>;<span class=\"hljs-attribute\">top</span>:<span class=\"hljs-number\">129px</span>&#125;<span class=\"hljs-selector-class\">.toc-article</span> <span class=\"hljs-selector-class\">.toc-close</span>&#123;<span class=\"hljs-attribute\">font-weight</span>:<span class=\"hljs-number\">700</span>;<span class=\"hljs-attribute\">font-size</span>:<span class=\"hljs-number\">20px</span>;<span class=\"hljs-attribute\">cursor</span>:pointer;<span class=\"hljs-attribute\">float</span><span class=\"hljs-selector-pseudo\">:right</span>;<span class=\"hljs-attribute\">color</span>:<span class=\"hljs-number\">#ccc</span>&#125;<span class=\"hljs-selector-class\">.toc-article</span> <span class=\"hljs-selector-class\">.toc-close</span><span class=\"hljs-selector-pseudo\">:hover</span>&#123;<span class=\"hljs-attribute\">color</span>:<span class=\"hljs-number\">#000</span>&#125;<span class=\"hljs-selector-class\">.toc-article</span> <span class=\"hljs-selector-class\">.toc</span>&#123;<span class=\"hljs-attribute\">font-size</span>:<span class=\"hljs-number\">12px</span>;<span class=\"hljs-attribute\">padding</span>:<span class=\"hljs-number\">0</span>;<span class=\"hljs-attribute\">line-height</span>:<span class=\"hljs-number\">20px</span>&#125;<span class=\"hljs-selector-class\">.toc-article</span> <span class=\"hljs-selector-class\">.toc</span> <span class=\"hljs-selector-class\">.toc-number</span>&#123;<span class=\"hljs-attribute\">color</span>:<span class=\"hljs-number\">#333</span>&#125;<span class=\"hljs-selector-class\">.toc-article</span> <span class=\"hljs-selector-class\">.toc</span> <span class=\"hljs-selector-class\">.toc-text</span><span class=\"hljs-selector-pseudo\">:hover</span>&#123;<span class=\"hljs-attribute\">text-decoration</span>:underline;<span class=\"hljs-attribute\">color</span>:<span class=\"hljs-number\">#2a6496</span>&#125;<span class=\"hljs-selector-class\">.toc-article</span> <span class=\"hljs-selector-tag\">li</span>&#123;<span class=\"hljs-attribute\">list-style-type</span>:none&#125;<span class=\"hljs-selector-class\">.toc-article</span> <span class=\"hljs-selector-class\">.toc-level-1</span>&#123;<span class=\"hljs-attribute\">margin</span>:<span class=\"hljs-number\">4px</span> <span class=\"hljs-number\">0</span>&#125;<span class=\"hljs-selector-class\">.toc-article</span> <span class=\"hljs-selector-class\">.toc-child</span>&#123;&#125;<span class=\"hljs-keyword\">@-moz-keyframes</span> cd-bounce-<span class=\"hljs-number\">1</span>&#123;<span class=\"hljs-number\">0%</span>&#123;<span class=\"hljs-attribute\">opacity</span>:<span class=\"hljs-number\">0</span>;-o-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-moz-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-ms-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>)&#125;<span class=\"hljs-number\">60%</span>&#123;<span class=\"hljs-attribute\">opacity</span>:<span class=\"hljs-number\">1</span>;-o-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);-moz-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);-ms-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>)&#125;<span class=\"hljs-number\">100%</span>&#123;-o-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-moz-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-ms-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>)&#125;&#125;<span class=\"hljs-keyword\">@-webkit-keyframes</span> cd-bounce-<span class=\"hljs-number\">1</span>&#123;<span class=\"hljs-number\">0%</span>&#123;<span class=\"hljs-attribute\">opacity</span>:<span class=\"hljs-number\">0</span>;-o-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-moz-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-ms-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>)&#125;<span class=\"hljs-number\">60%</span>&#123;<span class=\"hljs-attribute\">opacity</span>:<span class=\"hljs-number\">1</span>;-o-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);-moz-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);-ms-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>)&#125;<span class=\"hljs-number\">100%</span>&#123;-o-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-moz-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-ms-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>)&#125;&#125;<span class=\"hljs-keyword\">@-o-keyframes</span> cd-bounce-<span class=\"hljs-number\">1</span>&#123;<span class=\"hljs-number\">0%</span>&#123;<span class=\"hljs-attribute\">opacity</span>:<span class=\"hljs-number\">0</span>;-o-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-moz-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-ms-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>)&#125;<span class=\"hljs-number\">60%</span>&#123;<span class=\"hljs-attribute\">opacity</span>:<span class=\"hljs-number\">1</span>;-o-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);-moz-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);-ms-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>)&#125;<span class=\"hljs-number\">100%</span>&#123;-o-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-moz-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-ms-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>)&#125;&#125;<span class=\"hljs-keyword\">@keyframes</span> cd-bounce-<span class=\"hljs-number\">1</span>&#123;<span class=\"hljs-number\">0%</span>&#123;<span class=\"hljs-attribute\">opacity</span>:<span class=\"hljs-number\">0</span>;-o-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-moz-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-ms-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>)&#125;<span class=\"hljs-number\">60%</span>&#123;<span class=\"hljs-attribute\">opacity</span>:<span class=\"hljs-number\">1</span>;-o-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);-moz-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);-ms-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>)&#125;<span class=\"hljs-number\">100%</span>&#123;-o-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-moz-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-ms-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>)&#125;&#125;<span class=\"hljs-selector-class\">.show-toc-btn</span>&#123;<span class=\"hljs-attribute\">display</span>:none;<span class=\"hljs-attribute\">z-index</span>:<span class=\"hljs-number\">10</span>;<span class=\"hljs-attribute\">width</span>:<span class=\"hljs-number\">30px</span>;<span class=\"hljs-attribute\">min-height</span>:<span class=\"hljs-number\">14px</span>;<span class=\"hljs-attribute\">overflow</span>:hidden;<span class=\"hljs-attribute\">padding</span>:<span class=\"hljs-number\">4px</span> <span class=\"hljs-number\">6px</span> <span class=\"hljs-number\">8px</span> <span class=\"hljs-number\">5px</span>;<span class=\"hljs-attribute\">border</span>:<span class=\"hljs-number\">1px</span> solid <span class=\"hljs-number\">#ddd</span>;<span class=\"hljs-attribute\">border-right</span>:none;<span class=\"hljs-attribute\">position</span>:fixed;<span class=\"hljs-attribute\">right</span>:<span class=\"hljs-number\">40px</span>;<span class=\"hljs-attribute\">text-align</span>:center;<span class=\"hljs-attribute\">background-color</span>:<span class=\"hljs-number\">#f9f9f9</span>&#125;<span class=\"hljs-selector-class\">.show-toc-btn</span> <span class=\"hljs-selector-class\">.btn-bg</span>&#123;<span class=\"hljs-attribute\">margin-top</span>:<span class=\"hljs-number\">2px</span>;<span class=\"hljs-attribute\">display</span>:block;<span class=\"hljs-attribute\">width</span>:<span class=\"hljs-number\">16px</span>;<span class=\"hljs-attribute\">height</span>:<span class=\"hljs-number\">14px</span>;<span class=\"hljs-attribute\">background</span>:<span class=\"hljs-built_in\">url</span>(<span class=\"hljs-string\">http://7xtawy.com1.z0.glb.clouddn.com/show.png</span>) no-repeat;-webkit-<span class=\"hljs-attribute\">background-size</span>:<span class=\"hljs-number\">100%</span>;-moz-<span class=\"hljs-attribute\">background-size</span>:<span class=\"hljs-number\">100%</span>;<span class=\"hljs-attribute\">background-size</span>:<span class=\"hljs-number\">100%</span>&#125;<span class=\"hljs-selector-class\">.show-toc-btn</span> <span class=\"hljs-selector-class\">.btn-text</span>&#123;<span class=\"hljs-attribute\">color</span>:<span class=\"hljs-number\">#999</span>;<span class=\"hljs-attribute\">font-size</span>:<span class=\"hljs-number\">12px</span>&#125;<span class=\"hljs-selector-class\">.show-toc-btn</span><span class=\"hljs-selector-pseudo\">:hover</span>&#123;<span class=\"hljs-attribute\">cursor</span>:pointer&#125;<span class=\"hljs-selector-class\">.show-toc-btn</span><span class=\"hljs-selector-pseudo\">:hover</span> <span class=\"hljs-selector-class\">.btn-bg</span>&#123;<span class=\"hljs-attribute\">background-position</span>:<span class=\"hljs-number\">0</span> -<span class=\"hljs-number\">16px</span>&#125;<span class=\"hljs-selector-class\">.show-toc-btn</span><span class=\"hljs-selector-pseudo\">:hover</span> <span class=\"hljs-selector-class\">.btn-text</span>&#123;<span class=\"hljs-attribute\">font-size</span>:<span class=\"hljs-number\">12px</span>;<span class=\"hljs-attribute\">color</span>:<span class=\"hljs-number\">#ea8010</span>&#125;\n<span class=\"hljs-selector-class\">.toc-article</span> <span class=\"hljs-selector-tag\">li</span> <span class=\"hljs-selector-tag\">ol</span>, <span class=\"hljs-selector-class\">.toc-article</span> <span class=\"hljs-selector-tag\">li</span> <span class=\"hljs-selector-tag\">ul</span> &#123;\n    <span class=\"hljs-attribute\">margin-left</span>: <span class=\"hljs-number\">30px</span>;\n&#125;\n<span class=\"hljs-selector-class\">.toc-article</span> <span class=\"hljs-selector-tag\">ol</span>, <span class=\"hljs-selector-class\">.toc-article</span> <span class=\"hljs-selector-tag\">ul</span> &#123;\n    <span class=\"hljs-attribute\">margin</span>: <span class=\"hljs-number\">10px</span> <span class=\"hljs-number\">0</span>;\n&#125;</code></pre></div>\n<p>2.after <code>&lt;/header&gt;&lt;% &#125; %&gt;</code> in file <code>themes/yilia/layout/_partial/article.ejs</code> add</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs html\"><span class=\"hljs-comment\">&lt;!-- navigator --&gt;</span>\n&lt;% if (!index &amp;&amp; post.toc)&#123; %&gt;\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">p</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;show-toc-btn&quot;</span> <span class=\"hljs-attr\">id</span>=<span class=\"hljs-string\">&quot;show-toc-btn&quot;</span> <span class=\"hljs-attr\">onclick</span>=<span class=\"hljs-string\">&quot;showToc();&quot;</span> <span class=\"hljs-attr\">style</span>=<span class=\"hljs-string\">&quot;display:none&quot;</span>&gt;</span>\n        <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">span</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;btn-bg&quot;</span>&gt;</span><span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">span</span>&gt;</span>\n        <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">span</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;btn-text&quot;</span>&gt;</span>...<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">span</span>&gt;</span>\n        <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">p</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">id</span>=<span class=\"hljs-string\">&quot;toc-article&quot;</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;toc-article&quot;</span>&gt;</span>\n      <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">span</span> <span class=\"hljs-attr\">id</span>=<span class=\"hljs-string\">&quot;toc-close&quot;</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;toc-close&quot;</span> <span class=\"hljs-attr\">title</span>=<span class=\"hljs-string\">&quot;hide navigator&quot;</span> <span class=\"hljs-attr\">onclick</span>=<span class=\"hljs-string\">&quot;showBtn();&quot;</span>&gt;</span>×<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">span</span>&gt;</span>\n      <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">strong</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;toc-title&quot;</span>&gt;</span>navigator<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">strong</span>&gt;</span>\n        &lt;%- toc(post.content) %&gt;\n      <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">div</span>&gt;</span>\n<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">script</span> <span class=\"hljs-attr\">type</span>=<span class=\"hljs-string\">&quot;text/javascript&quot;</span>&gt;</span><span class=\"language-javascript\"></span>\n<span class=\"language-javascript\">  <span class=\"hljs-keyword\">function</span> <span class=\"hljs-title function_\">showToc</span>(<span class=\"hljs-params\"></span>)&#123;</span>\n<span class=\"language-javascript\">      <span class=\"hljs-keyword\">var</span> toc_article = <span class=\"hljs-variable language_\">document</span>.<span class=\"hljs-title function_\">getElementById</span>(<span class=\"hljs-string\">&quot;toc-article&quot;</span>);</span>\n<span class=\"language-javascript\">      <span class=\"hljs-keyword\">var</span> show_toc_btn = <span class=\"hljs-variable language_\">document</span>.<span class=\"hljs-title function_\">getElementById</span>(<span class=\"hljs-string\">&quot;show-toc-btn&quot;</span>);</span>\n<span class=\"language-javascript\">      toc_article.<span class=\"hljs-title function_\">setAttribute</span>(<span class=\"hljs-string\">&quot;style&quot;</span>,<span class=\"hljs-string\">&quot;display:block&quot;</span>);</span>\n<span class=\"language-javascript\">      show_toc_btn.<span class=\"hljs-title function_\">setAttribute</span>(<span class=\"hljs-string\">&quot;style&quot;</span>,<span class=\"hljs-string\">&quot;display:none&quot;</span>);</span>\n<span class=\"language-javascript\">      &#125;;</span>\n<span class=\"language-javascript\">  <span class=\"hljs-keyword\">function</span> <span class=\"hljs-title function_\">showBtn</span>(<span class=\"hljs-params\"></span>)&#123;</span>\n<span class=\"language-javascript\">      <span class=\"hljs-keyword\">var</span> toc_article = <span class=\"hljs-variable language_\">document</span>.<span class=\"hljs-title function_\">getElementById</span>(<span class=\"hljs-string\">&quot;toc-article&quot;</span>);</span>\n<span class=\"language-javascript\">      <span class=\"hljs-keyword\">var</span> show_toc_btn = <span class=\"hljs-variable language_\">document</span>.<span class=\"hljs-title function_\">getElementById</span>(<span class=\"hljs-string\">&quot;show-toc-btn&quot;</span>);</span>\n<span class=\"language-javascript\">      toc_article.<span class=\"hljs-title function_\">setAttribute</span>(<span class=\"hljs-string\">&quot;style&quot;</span>,<span class=\"hljs-string\">&quot;display:none&quot;</span>);</span>\n<span class=\"language-javascript\">      show_toc_btn.<span class=\"hljs-title function_\">setAttribute</span>(<span class=\"hljs-string\">&quot;style&quot;</span>,<span class=\"hljs-string\">&quot;display:block&quot;</span>);</span>\n<span class=\"language-javascript\">      &#125;;</span>\n<span class=\"language-javascript\"></span><span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">script</span>&gt;</span>\n    &lt;% &#125; %&gt;\n<span class=\"hljs-comment\">&lt;!-- navigator end --&gt;</span></code></pre></div>\n<p>3.add <code>toc:true</code> to the articles that need the navigator.</p>\n<h3 id=\"add-custormize-header-to-articles\">Add custormize header to articles</h3>\n<p>when run <code>hexo new</code> to initiate a new blog, a defaul head would generate, change it by</p>\n<p>change the <code>scaffolds/post.md</code> in the <code>root</code> directory</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs txt\">---\ntitle: &#123;&#123; title &#125;&#125;\ndate: &#123;&#123; date &#125;&#125;\nauthor: daydreamatnight\ntoc: true\ndeclare: true\ntags:\n---</code></pre></div>\n<h4 id=\"more-headers-to-choose-when-writing-a-blog\">more headers to choose when writing a blog</h4>\n<p>before a blog, more paras can be chosen to add</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs txt\">--- \ntitle: #你的博客文章名 \ntoc: ture #toc \ndate: 2020-09-07 09:25:00 #文章时间 \nauthor: GavenLee #作者 \nimg: /source/images/xxx.jpg #图片 \ntop: true #是否顶置 \ncover: true #是否在引导页轮播 \ncoverImg: /images/1.jpg #轮播图片 \npassword: #阅读密码这里被加密 \nmathjax: false #mathjax \nsummary: 这是你自定义的文章摘要内容，如果这个属性有值，文章卡片摘要就显示这段文字，否则程序会自动截取文章的部分内容作为摘要 \ncategories: Markdown #分类 \ntags: #标签 \nabbrlink: HexoLearn #链接 \n---</code></pre></div>\n<h3 id=\"disable-auto-wrap-in-code-block\">Disable auto wrap in code block</h3>\n<p>locate and delete <code>white-space:pre-wrap</code> in file <code>themes/yilia/source/main.0cf68a.css</code></p>\n<h3 id=\"add-copy-button-to-code-block\">Add copy button to code block</h3>\n<p>1.create a <code>clipboard_use.js</code> file in directory <code>themes/yilia/source</code></p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs js\">$(<span class=\"hljs-string\">&quot;.highlight&quot;</span>).<span class=\"hljs-title function_\">wrap</span>(<span class=\"hljs-string\">&quot;&lt;div class=&#x27;code-wrapper&#x27; style=&#x27;position:relative&#x27;&gt;&lt;/div&gt;&quot;</span>);\n<span class=\"hljs-comment\">/*create copy button after page loaded*/</span>\n!<span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">e, t, a</span>) &#123;\n    <span class=\"hljs-comment\">/* code */</span>\n    <span class=\"hljs-keyword\">var</span> initCopyCode = <span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\"></span>) &#123;\n        <span class=\"hljs-keyword\">var</span> copyHtml = <span class=\"hljs-string\">&#x27;&#x27;</span>;\n        copyHtml += <span class=\"hljs-string\">&#x27;&lt;button class=&quot;btn-copy&quot; data-clipboard-snippet=&quot;&quot;&gt;&#x27;</span>;\n        copyHtml += <span class=\"hljs-string\">&#x27;  &lt;i class=&quot;fa fa-clipboard&quot;&gt;&lt;/i&gt;&lt;span&gt;copy&lt;/span&gt;&#x27;</span>;\n        copyHtml += <span class=\"hljs-string\">&#x27;&lt;/button&gt;&#x27;</span>;\n        $(<span class=\"hljs-string\">&quot;.highlight .code&quot;</span>).<span class=\"hljs-title function_\">before</span>(copyHtml);\n        <span class=\"hljs-keyword\">var</span> clipboard = <span class=\"hljs-keyword\">new</span> <span class=\"hljs-title class_\">ClipboardJS</span>(<span class=\"hljs-string\">&#x27;.btn-copy&#x27;</span>, &#123;\n            <span class=\"hljs-attr\">target</span>: <span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">trigger</span>) &#123;\n                <span class=\"hljs-keyword\">return</span> trigger.<span class=\"hljs-property\">nextElementSibling</span>;\n            &#125;\n        &#125;);\n        clipboard.<span class=\"hljs-title function_\">on</span>(<span class=\"hljs-string\">&#x27;success&#x27;</span>, <span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">e</span>) &#123;\n            e.<span class=\"hljs-property\">trigger</span>.<span class=\"hljs-property\">innerHTML</span> = <span class=\"hljs-string\">&quot;&lt;i class=&#x27;fa fa-check&#x27; style=&#x27;color:green&#x27;&gt;&lt;/i&gt;&lt;span style=&#x27;color:green&#x27;&gt;copy success&lt;/span&gt;&quot;</span>\n            <span class=\"hljs-built_in\">setTimeout</span>(<span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\"></span>) &#123;\n                e.<span class=\"hljs-property\">trigger</span>.<span class=\"hljs-property\">innerHTML</span> = <span class=\"hljs-string\">&quot;&lt;i class=&#x27;fa fa-clipboard&#x27;&gt;&lt;/i&gt;&lt;span&gt;copy&lt;/span&gt;&quot;</span>\n            &#125;, <span class=\"hljs-number\">1000</span>)\n            e.<span class=\"hljs-title function_\">clearSelection</span>();\n        &#125;);\n        clipboard.<span class=\"hljs-title function_\">on</span>(<span class=\"hljs-string\">&#x27;error&#x27;</span>, <span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">e</span>) &#123;\n            e.<span class=\"hljs-property\">trigger</span>.<span class=\"hljs-property\">innerHTML</span> = <span class=\"hljs-string\">&quot;&lt;i class=&#x27;fa fa-exclamation&#x27; style=&#x27;color:red&#x27;&gt;&lt;/i&gt;&lt;span style=&#x27;color:red&#x27;&gt;copy success&lt;/span&gt;&quot;</span>\n            <span class=\"hljs-built_in\">setTimeout</span>(<span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\"></span>) &#123;\n                e.<span class=\"hljs-property\">trigger</span>.<span class=\"hljs-property\">innerHTML</span> = <span class=\"hljs-string\">&quot;&lt;i class=&#x27;fa fa-clipboard&#x27;&gt;&lt;/i&gt;&lt;span&gt;copy&lt;/span&gt;&quot;</span>\n            &#125;, <span class=\"hljs-number\">1000</span>)\n            e.<span class=\"hljs-title function_\">clearSelection</span>();\n        &#125;);\n    &#125;\n    <span class=\"hljs-title function_\">initCopyCode</span>();\n&#125;(<span class=\"hljs-variable language_\">window</span>, <span class=\"hljs-variable language_\">document</span>);</code></pre></div>\n<p>2.load .js file, edit <code>themes/yilia/layout/layout.ejs</code> file, add before <code>&lt;/body&gt;</code>.</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs html\"><span class=\"hljs-comment\">&lt;!-- copy button in code block--&gt;</span>\n<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">script</span> <span class=\"hljs-attr\">type</span>=<span class=\"hljs-string\">&quot;text/javascript&quot;</span> <span class=\"hljs-attr\">src</span>=<span class=\"hljs-string\">&quot;https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.js&quot;</span>&gt;</span><span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">script</span>&gt;</span>\n<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">script</span> <span class=\"hljs-attr\">type</span>=<span class=\"hljs-string\">&quot;text/javascript&quot;</span> <span class=\"hljs-attr\">src</span>=<span class=\"hljs-string\">&quot;https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js&quot;</span>&gt;</span><span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">script</span>&gt;</span>\n<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">script</span> <span class=\"hljs-attr\">type</span>=<span class=\"hljs-string\">&quot;text/javascript&quot;</span> <span class=\"hljs-attr\">src</span>=<span class=\"hljs-string\">&quot;/clipboard_use.js&quot;</span>&gt;</span><span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">script</span>&gt;</span></code></pre></div>\n<p>3.add stylesheet the end of <code>themes/yilia/source/main.0cf68a.css</code></p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs css\"><span class=\"hljs-comment\">/* code copy button */</span>\n<span class=\"hljs-selector-class\">.btn-copy</span> &#123;\n  <span class=\"hljs-attribute\">display</span>: inline-block;\n  <span class=\"hljs-attribute\">cursor</span>: pointer;\n  <span class=\"hljs-attribute\">background-color</span>: <span class=\"hljs-number\">#eee</span>;\n  <span class=\"hljs-attribute\">background-image</span>: <span class=\"hljs-built_in\">linear-gradient</span>(<span class=\"hljs-number\">#fcfcfc</span>, <span class=\"hljs-number\">#eee</span>);\n  <span class=\"hljs-attribute\">border</span>: <span class=\"hljs-number\">1px</span> solid <span class=\"hljs-number\">#d5d5d5</span>;\n  <span class=\"hljs-attribute\">border-radius</span>: <span class=\"hljs-number\">3px</span>;\n  -webkit-user-select: none;\n  -moz-user-select: none;\n  -ms-user-select: none;\n  user-select: none;\n  -webkit-appearance: none;\n  <span class=\"hljs-attribute\">font-size</span>: <span class=\"hljs-number\">13px</span>;\n  <span class=\"hljs-attribute\">font-weight</span>: <span class=\"hljs-number\">700</span>;\n  <span class=\"hljs-attribute\">line-height</span>: <span class=\"hljs-number\">20px</span>;\n  <span class=\"hljs-attribute\">color</span>: <span class=\"hljs-number\">#333</span>;\n  -webkit-<span class=\"hljs-attribute\">transition</span>: opacity .<span class=\"hljs-number\">3s</span> ease-in-out;\n  -o-<span class=\"hljs-attribute\">transition</span>: opacity .<span class=\"hljs-number\">3s</span> ease-in-out;\n  <span class=\"hljs-attribute\">transition</span>: opacity .<span class=\"hljs-number\">3s</span> ease-in-out;\n  <span class=\"hljs-attribute\">padding</span>: <span class=\"hljs-number\">2px</span> <span class=\"hljs-number\">6px</span>;\n  <span class=\"hljs-attribute\">position</span>: absolute;\n  <span class=\"hljs-attribute\">right</span>: <span class=\"hljs-number\">5px</span>;\n  <span class=\"hljs-attribute\">top</span>: <span class=\"hljs-number\">5px</span>;\n  <span class=\"hljs-attribute\">opacity</span>: <span class=\"hljs-number\">0</span>;\n&#125;\n<span class=\"hljs-selector-class\">.btn-copy</span> <span class=\"hljs-selector-tag\">span</span> &#123;\n  <span class=\"hljs-attribute\">margin-left</span>: <span class=\"hljs-number\">5px</span>;\n&#125;\n<span class=\"hljs-selector-class\">.highlight</span><span class=\"hljs-selector-pseudo\">:hover</span> <span class=\"hljs-selector-class\">.btn-copy</span> &#123;\n  <span class=\"hljs-attribute\">opacity</span>: <span class=\"hljs-number\">1</span>;\n&#125;\n<span class=\"hljs-comment\">/* code copy button end */</span></code></pre></div>\n<p>4.add copy button icon, edit <code>themes/yilia/layout/_partia/head.ejs</code> add before <code>&lt;/head&gt;</code></p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs html\"><span class=\"hljs-comment\">&lt;!-- copy button icon --&gt;</span>\n<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">link</span> <span class=\"hljs-attr\">rel</span>=<span class=\"hljs-string\">&quot;stylesheet&quot;</span> <span class=\"hljs-attr\">type</span>=<span class=\"hljs-string\">&quot;text/css&quot;</span> <span class=\"hljs-attr\">href</span>=<span class=\"hljs-string\">&quot;//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css&quot;</span>&gt;</span></code></pre></div>\n<h3 id=\"allow-search-engines-to-index-this-blog\">Allow search engines to index this Blog</h3>\n<h4 id=\"index-google-to-this-blog\">index Google to this Blog</h4>\n<p>check if google can find you, enter <code>site:daydreamatnight.github.io</code> to see</p>\n<p><img src=\"check google search.png\" srcset=\"/img/loading.gif\" lazyload alt=\"check google search\" style=\"zoom:50%;\" /></p>\n<h5 id=\"add-url-to-goole-search-console\">Add url to goole search console</h5>\n<p>1.open <a href=\"https://search.google.com/search-console/welcome\">google console</a> , add URL link of the blog (https://daydreamatnight.github.io), in the <code>URL prefix</code> block, click <code>CONTINUE</code></p>\n<p><img src=\"google search console.png\" srcset=\"/img/loading.gif\" lazyload alt=\"google console\" style=\"zoom:50%;\" /></p>\n<p>2.upload the html file to the blog <code>root</code> directory and deploy the website, then clicke verify.</p>\n<p><img src=\"google console varification.png\" srcset=\"/img/loading.gif\" lazyload alt=\"google console varification\" style=\"zoom:50%;\" /></p>\n<p>little buggy here, see <a href=\"https://hilyy.xyz/how-to-allow-google-to-index-your-hexo-blog-website-google-search-console-verification-methods/\"><strong>don’t upload</strong> the file <strong>using hexo</strong> command</a></p>\n<h5 id=\"add-sitemap-for-google\">add sitemap for google</h5>\n<p>add sitemap for google and baidu together</p>\n<blockquote>\n<p>A <em>sitemap</em> is a file where you provide information about the pages, videos, and other files on your site, and the relationships between them. Search engines like Google read this file to more intelligently crawl your site. A sitemap tells Google which pages and files you think are important in your site, and also provides valuable information about these files: for example, for pages, when the page was last updated, how often the page is changed, and any alternate language versions of a page.</p>\n</blockquote>\n<p>1.install sitemap plugins</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs shell\"><span class=\"hljs-meta\">$ </span><span class=\"language-bash\">npm install hexo-generator-sitemap --save</span>\n<span class=\"hljs-meta\">$ </span><span class=\"language-bash\">npm install hexo-generator-baidu-sitemap --save</span></code></pre></div>\n<p>2.add to the <code>_config.yml</code> in the blog <code>root</code></p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs yml\"><span class=\"hljs-comment\"># hexo sitemap</span>\n<span class=\"hljs-attr\">sitemap:</span>\n  <span class=\"hljs-attr\">path:</span> <span class=\"hljs-string\">sitemap.xml</span>\n<span class=\"hljs-attr\">baidusitemap:</span>\n  <span class=\"hljs-attr\">path:</span> <span class=\"hljs-string\">baidusitemap.xml</span></code></pre></div>\n<p>3.Deploy the blog, go to https://daydreamatnight.github.io/sitemap.xml and https://daydreamatnight.github.io/baidusitemap.xml to see if sitemaps are uploaded</p>\n<p>4.Go to Google Search Console, in the left panel, click <code>Sitemaps</code>, enter your sitemap URL <code>sitemap.xml</code></p>\n<p><img src=\"can't fetch sitemap.png\" srcset=\"/img/loading.gif\" lazyload alt=\"can't fetch sitemap\" style=\"zoom:50%;\" /></p>\n<p>Googlebot won't download the sitemap immediately. Give it time.</p>\n<h5 id=\"add-robots.txt\">add robots.txt</h5>\n<blockquote>\n<p>A robots.txt file tells search engine crawlers which URLs the crawler can access on your site. This is used mainly to avoid overloading your site with requests; <strong>it is not a mechanism for keeping a web page out of Google</strong>. To keep a web page out of Google, <a href=\"https://developers.google.com/search/docs/advanced/crawling/block-indexing\">block indexing with <code>noindex</code></a> or password-protect the page.</p>\n<p>A robots.txt file is used primarily to manage crawler traffic to your site, and <em>usually</em> to keep a file off Google, depending on the file type:</p>\n</blockquote>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs txt\">User-agent: *\nAllow: /\nAllow: /archives/\nAllow: /tags/\nAllow: /categories/\nAllow: /about/\nAllow: /guestbook/\nAllow: /others/\n\n\nDisallow: /js/\nDisallow: /css/\nDisallow: /lib/\n\nSitemap: https://daydreamatnight.github.io/sitemap.xml\nSitemap: https://daydreamatnight.github.io/baidusitemap.xml</code></pre></div>\n<p>deploy the blog and wait.</p>\n<h5 id=\"check-if-sitemap-is-available\">check if sitemap is available</h5>\n<p>After uploaded several updates, my sitemap still didn't fetched by google. So I went to check, it turns out my url setting in <code>_config.yml</code> is wrong. So I changed it to be my home url. And check it with <a href=\"https://www.jcchouinard.com/url-inspection-tool/\">URL Inspection Tool</a>.</p>\n<p>1.Open <a href=\"https://search.google.com/search-console\">google search console</a>, add the url of sitemap in the upper url inspecting box.</p>\n<p><img src=\"Google%20sitemap%20inspect.png\" srcset=\"/img/loading.gif\" lazyload alt=\"Google sitemap inspect URL is not on Google \" style=\"zoom:22%;\" /></p>\n<p><img src=\"Google%20sitemap%20inspect%202.png\" srcset=\"/img/loading.gif\" lazyload alt=\"Google sitemap inspect 2\" style=\"zoom:22%;\" /></p>\n<p>It's normal it shows <code>URL is not on Google</code> because it shouldn't as a sitemap.</p>\n<p>2.click <code>live test</code> to check the availability.</p>\n<p><img src=\"Google%20sitemap%20inspect%203.png\" srcset=\"/img/loading.gif\" lazyload alt=\"Google sitemap inspect 3\" style=\"zoom:75%;\" /></p>\n<p>It should be available, then just wait.</p>\n<h4 id=\"index-bing-to-this-blog\">index Bing to this Blog</h4>\n<p>1.go to <a href=\"https://www.bing.com/webmasters/\">Bing webmaster</a> and login</p>\n<p>2.connect with google webmaster.</p>\n<p><img src=\"bing%20sitemap.png\" srcset=\"/img/loading.gif\" lazyload alt=\"bing sitemap\" style=\"zoom:75%;\" /></p>\n<p><img src=\"being%20sitemap%20connect%20google.png\" srcset=\"/img/loading.gif\" lazyload alt=\"being sitemap connect google\" style=\"zoom:75%;\" /></p>\n<p><img src=\"bing%20search%20console%20success.png\" srcset=\"/img/loading.gif\" lazyload alt=\"bing search console success\" style=\"zoom:75%;\" /></p>\n<h4 id=\"index-baidu-to-this-blognot-possibly-working\">index baidu to this Blog(not possibly working)</h4>\n<p>go to the <a href=\"https://ziyuan.baidu.com/site/index\">baidu search console</a> ,</p>\n<p><img src=\"baidu console.png\" srcset=\"/img/loading.gif\" lazyload alt=\"baidu console\" style=\"zoom:50%;\" /></p>\n<p>Click <code>添加网站</code> and input every thing, do similar thing</p>\n<p><img src=\"baidu console varification.png\" srcset=\"/img/loading.gif\" lazyload alt=\"baidu console varification\" style=\"zoom:50%;\" /></p>\n<p>add sitemap</p>\n<p><img src=\"baidu console sitemap.png\" srcset=\"/img/loading.gif\" lazyload alt=\"baidu console sitemap\" style=\"zoom:50%;\" /></p>\n<p>just wait forever, this could take 2000 years, so give up</p>\n<h3 id=\"add-copyright-statement\">Add copyright statement</h3>\n<p>1.open file <code>themes/yilia/layout/_partial/article.ejs</code> add before <code>&lt;% if ((theme.reward_type === 2 || (theme.reward_type === 1 &amp;&amp; post.reward)) &amp;&amp; !index)&#123; %&gt;</code></p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs html\"><span class=\"hljs-comment\">&lt;!-- add copyright statement --&gt;</span>\n&lt;% if(theme.declare)&#123;%&gt;\n    &lt;%- partial(&#x27;post/declare&#x27;) %&gt;\n&lt;% &#125; %&gt;\n<span class=\"hljs-comment\">&lt;!-- end --&gt;</span></code></pre></div>\n<p>2.create new file <code>declare.ejs</code> under <code>themes/yilia/layout/_partial/post/</code> with:</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs html\"><span class=\"hljs-comment\">&lt;!--add copyright statement https://github.com/JoeyBling/hexo-theme-yilia-plus/commit/c1215e132f6d5621c5fea83d3c4f7ccbcca074a3--&gt;</span>\n&lt;%\n  var sUrl = url.replace(/index\\.html$/, &#x27;&#x27;);\n  sUrl = /^(http:|https:)\\/\\//.test(sUrl) ? sUrl : &#x27;https:&#x27; + sUrl;\n%&gt;\n\n<span class=\"hljs-comment\">&lt;!-- #copyright setting：0-close statement; 1-declare statement if declare: true in the article header; 2-always declare the copyright --&gt;</span>\n&lt;% if ((theme.declare.declare_type === 2 || (theme.declare.declare_type === 1 &amp;&amp; post.declare)) &amp;&amp; !index)&#123; %&gt;\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;declare&quot;</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">strong</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;author&quot;</span>&gt;</span>author: <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">strong</span>&gt;</span>\n    &lt;% if(config.author != undefined)&#123; %&gt;\n      &lt;%= config.author%&gt;\n    &lt;% &#125;else&#123;%&gt;\n      <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">font</span> <span class=\"hljs-attr\">color</span>=<span class=\"hljs-string\">&quot;red&quot;</span>&gt;</span>please add right &quot;author&quot; name in &quot;_config.yml&quot; in the blog root<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">font</span>&gt;</span>\n    &lt;%&#125;%&gt;\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">br</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">strong</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;create-time&quot;</span>&gt;</span>posting date: <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">strong</span>&gt;</span>\n    &lt;%- date(post.date, &#x27;YYYY-MM-DD HH:MM:SS&#x27;) %&gt;\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">br</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">strong</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;update-time&quot;</span>&gt;</span>last update: <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">strong</span>&gt;</span>\n    &lt;%- date(post.updated, &#x27;YYYY-MM-DD HH:MM:SS&#x27;) %&gt;\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">br</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">strong</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;article-titles&quot;</span>&gt;</span>article title: <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">strong</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">a</span> <span class=\"hljs-attr\">href</span>=<span class=\"hljs-string\">&quot;&lt;%= config.url %&gt;/&lt;%= post.path %&gt;&quot;</span> <span class=\"hljs-attr\">title</span>=<span class=\"hljs-string\">&quot;&lt;%= post.title %&gt;&quot;</span> <span class=\"hljs-attr\">target</span>=<span class=\"hljs-string\">&quot;_blank&quot;</span>&gt;</span>&lt;%= post.title %&gt;<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">a</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">br</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">strong</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;article-url&quot;</span>&gt;</span>article link: <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">strong</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">a</span> <span class=\"hljs-attr\">href</span>=<span class=\"hljs-string\">&quot;&lt;%= config.url %&gt;/&lt;%= post.path %&gt;&quot;</span> <span class=\"hljs-attr\">title</span>=<span class=\"hljs-string\">&quot;&lt;%= post.title %&gt;&quot;</span> <span class=\"hljs-attr\">target</span>=<span class=\"hljs-string\">&quot;_blank&quot;</span>&gt;</span>&lt;%= config.url %&gt;/&lt;%= post.path %&gt;<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">a</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">br</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">strong</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;copyright&quot;</span>&gt;</span>copyright:<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">strong</span>&gt;</span>\n    This work is licensed under a\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">a</span> <span class=\"hljs-attr\">rel</span>=<span class=\"hljs-string\">&quot;license&quot;</span> <span class=\"hljs-attr\">href</span>=<span class=\"hljs-string\">&quot;&lt;%= theme.declare.licensee_url%&gt;&quot;</span> <span class=\"hljs-attr\">title</span>=<span class=\"hljs-string\">&quot;&lt;%= theme.declare.licensee_alias %&gt;&quot;</span>&gt;</span>&lt;%= theme.declare.licensee_name%&gt;<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">a</span>&gt;</span>\n    licience \n    &lt;% if(theme.declare.licensee_img != undefined)&#123; %&gt;\n      <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">a</span> <span class=\"hljs-attr\">rel</span>=<span class=\"hljs-string\">&quot;license&quot;</span> <span class=\"hljs-attr\">href</span>=<span class=\"hljs-string\">&quot;&lt;%= theme.declare.licensee_url%&gt;&quot;</span>&gt;</span><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">img</span> <span class=\"hljs-attr\">alt</span>=<span class=\"hljs-string\">&quot;知识共享许可协议&quot;</span> <span class=\"hljs-attr\">style</span>=<span class=\"hljs-string\">&quot;border-width:0&quot;</span> <span class=\"hljs-attr\">src</span>=<span class=\"hljs-string\">&quot;&lt;%= theme.declare.licensee_img%&gt;&quot;</span>/&gt;</span><span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">a</span>&gt;</span>\n    &lt;% &#125; %&gt;\n  <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">div</span>&gt;</span>\n&lt;% &#125; else &#123;%&gt;\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;declare&quot;</span> <span class=\"hljs-attr\">hidden</span>=<span class=\"hljs-string\">&quot;hidden&quot;</span>&gt;</span><span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">div</span>&gt;</span>\n&lt;% &#125; %&gt;\n<span class=\"hljs-comment\">&lt;!-- add copyright statement --&gt;</span></code></pre></div>\n<p>3.add stylesheet the end of <code>themes/yilia/source/main.0cf68a.css</code></p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs css\"><span class=\"hljs-comment\">/*stylesheet for the delcare*/</span>\n<span class=\"hljs-selector-class\">.declare</span> &#123;\n  <span class=\"hljs-attribute\">background-color</span>: <span class=\"hljs-number\">#eaeaea</span>;\n  <span class=\"hljs-attribute\">margin-top</span>: <span class=\"hljs-number\">2em</span>;\n  <span class=\"hljs-attribute\">border-left</span>: <span class=\"hljs-number\">3px</span> solid <span class=\"hljs-number\">#ff1700</span>;\n  <span class=\"hljs-attribute\">padding</span>: .<span class=\"hljs-number\">5em</span> <span class=\"hljs-number\">1em</span>; \n&#125;\n<span class=\"hljs-comment\">/*stylesheet for the delcare end*/</span></code></pre></div>\n<p>4.add at the end of <code>themes/yilia/_config.yml</code> file:</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs awk\">declare:\n  declare_type: <span class=\"hljs-number\">1</span>\n  licensee_url: http:<span class=\"hljs-regexp\">//</span>creativecommons.org<span class=\"hljs-regexp\">/licenses/</span>by-nc-sa<span class=\"hljs-regexp\">/4.0/</span>      \n  licensee_name: <span class=\"hljs-string\">&#x27;CC BY-NC-SA 4.0&#x27;</span>                              \n  licensee_alias: <span class=\"hljs-string\">&#x27;CC BY-NC-SA 4.0&#x27;</span>     \n  licensee_img: https:<span class=\"hljs-regexp\">//i</span>.creativecommons.org<span class=\"hljs-regexp\">/l/</span>by-nc-sa<span class=\"hljs-regexp\">/4.0/</span><span class=\"hljs-number\">80</span>x15.png</code></pre></div>\n<h3 id=\"add-mind-map-support\">Add mind-map support</h3>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs shell\">npm install hexo-markmap</code></pre></div>\n<p>Detailed in its <a href=\"https://github.com/MaxChang3/hexo-markmap\">Github</a></p>\n<p>Example:</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs markdown\">&#123;% markmap 300px %&#125;\n<span class=\"hljs-bullet\">-</span> Testa\n<span class=\"hljs-bullet\">  -</span> test1\n<span class=\"hljs-bullet\">  -</span> test2\n<span class=\"hljs-bullet\">-</span> Testb\n<span class=\"hljs-bullet\">  -</span> test1\n<span class=\"hljs-bullet\">  -</span> test2\n&#123;%endmarkmap%&#125;</code></pre></div>\n<h3 id=\"add-latex-math-support\">Add Latex math support</h3>\n<p>Change the renderer to the more powerful pandoc:</p>\n<p>1.Install pandoc on macOS:</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs cmake\">copybrew <span class=\"hljs-keyword\">install</span> pandoc</code></pre></div>\n<p>2.in the blog root directory uninstall the default renderer then install the pandoc renderer:</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs ada\">copynpm uninstall hexo-renderer-marked <span class=\"hljs-comment\">--save</span>\nnpm install hexo-renderer-pandoc <span class=\"hljs-comment\">--save</span></code></pre></div>\n<p>3.install the hexo math plugin</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs cmake\">copynpm <span class=\"hljs-keyword\">install</span> hexo-<span class=\"hljs-keyword\">math</span> --save</code></pre></div>\n<p>4.add these lines to the hexo <code>_config</code> file</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs nestedtext\"><span class=\"hljs-attribute\">copymarkdown</span><span class=\"hljs-punctuation\">:</span>\n  <span class=\"hljs-attribute\">plugins</span><span class=\"hljs-punctuation\">:</span>\n    <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">markdown-it-footnote</span>\n    <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">markdown-it-sup</span>\n    <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">markdown-it-sub</span>\n    <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">markdown-it-abbr</span>\n    <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">markdown-it-emoji</span>\n    <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">hexo-math</span></code></pre></div>\n<p>5.add these lines to the theme <code>_config</code> file</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs yaml\"><span class=\"hljs-string\">copy#</span> <span class=\"hljs-string\">MathJax</span> <span class=\"hljs-string\">Support</span>\n<span class=\"hljs-attr\">mathjax:</span>\n  <span class=\"hljs-attr\">enable:</span> <span class=\"hljs-literal\">true</span>\n  <span class=\"hljs-attr\">per_page:</span> <span class=\"hljs-literal\">true</span></code></pre></div>\n<p>6.rebuild the to blog see changes</p>\n<p>7.Examples: <span class=\"math inline\">\\(this_{is}an\\frac{inline}{equation}\\)</span> <span class=\"math display\">\\[\n\\begin{equation}\n    \\mathbf{K}_\\mathbf{1}=\\frac{1}{\\Delta r}\\ \\left[\\begin{matrix}\\begin{matrix}-1&amp;1\\\\-1&amp;1\\\\\\end{matrix}&amp;\\ &amp;\\ \\\\\\begin{matrix}\\ &amp;\\ddots\\\\\\end{matrix}&amp;\\begin{matrix}\\ddots&amp;\\ \\\\\\end{matrix}&amp;\\ \\\\\\ &amp;-1\\ &amp;1\\\\\\end{matrix}\\right],\\ \\ {\\ \\mathbf{K}}_\\mathbf{2}=\\frac{1}{\\Delta r}\\ \\left[\\begin{matrix}\\begin{matrix}-1&amp;1\\\\\\ &amp;\\ddots\\\\\\end{matrix}&amp;\\begin{matrix}\\\\\\ddots\\\\\\end{matrix}&amp;\\ \\\\\\begin{matrix}\\ &amp;\\ \\\\\\end{matrix}&amp;-1&amp;1\\ \\\\\\ &amp;-1\\ &amp;1\\\\\\end{matrix}\\right]\n    \\label{K2}\n\\end{equation}\n\\]</span></p>\n<h3 id=\"the-last-snapshot\">The last snapshot</h3>\n<p>Ok, never spend time on a no-longer maintained project. Here's the last figure of it.</p>\n<p><img src=\"Last snapshot.png\" srcset=\"/img/loading.gif\" lazyload alt=\"Last snapshot\" style=\"zoom:80%;\" /></p>\n<h2 id=\"reference\">Reference</h2>\n<p>https://flatironschool.com/blog/the-benefits-of-blogging-how-and-why-to-keep-a-technical-blog/</p>\n<p>https://weblog.masukomi.org/2015/10/18/static-vs-dynamic-blogging/</p>\n<p>https://www.cnblogs.com/aoguai/p/11781505.html</p>\n<p>https://www.kblog.top/post/30452.html</p>\n<p>https://wkzqn.gitee.io/2020/02/16/typora%E7%BC%96%E5%86%99hexo%E5%8D%9A%E5%AE%A2%E6%97%B6%E7%9A%84%E5%9B%BE%E7%89%87%E6%98%BE%E7%A4%BA/</p>\n<p>https://segmentfault.com/a/1190000009478837#articleHeader5</p>\n<p>https://hilyy.xyz/how-to-allow-google-to-index-your-hexo-blog-website-google-search-console-verification-methods/</p>\n<p>https://busuanzi.ibruce.info/</p>\n<p>https://creativecommons.org/choose/results-one?license_code=by-nc-sa&amp;jurisdiction=&amp;version=4.0&amp;lang=en</p>\n<p>https://www.jcchouinard.com/sitemap-could-not-be-read-couldnt-fetch-in-google-search-console/</p>\n<p>https://cqh-i.github.io/2019/08/07/hexo-yilia%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E9%9A%90%E8%97%8F%E5%B7%A6%E8%BE%B9%E6%A0%8F%E7%9B%AE%E6%8C%89%E9%92%AE/</p>\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>Technical blog has a hundred benefits and no harm</p>\n<p>This blog records the process of building and customizing this personal blog from 0 to 1</p>\n</blockquote>\n<blockquote>\n<p>Unfortunately, the yilia theme has been no longer updating and it is too buggy right now. I switch to other theme.</p>\n</blockquote>","more":"<h2 id=\"preliminary\">Preliminary</h2>\n<h3 id=\"why-personal-blog\">Why personal blog</h3>\n<blockquote>\n<p>Keeping a technical blog can be <strong>a great way of documenting your growth as a developer</strong>. This documentation can be particularly useful on a professional level. All software companies want to hire smart, thoughtful, communicative developers who can easily assimilate into a team, and who are ready to both teach and learn</p>\n</blockquote>\n<h3 id=\"static-vs-dynamic-blog\">Static vs dynamic blog</h3>\n<p>there are 2 types of mainstream personal blog: static and dynamic.</p>\n<p>Static is recommended considering its simplicity, 0 maintenance and 0 safety worry.</p>\n<table>\n<colgroup>\n<col style=\"width: 9%\" />\n<col style=\"width: 45%\" />\n<col style=\"width: 45%\" />\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\"></th>\n<th style=\"text-align: left;\">Static blog</th>\n<th style=\"text-align: left;\">Dynamic blog</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Price</td>\n<td style=\"text-align: left;\">low, 0 cost when the traffic is relatively low</td>\n<td style=\"text-align: left;\">High, server is needed, cloud server of high performance is usually very expensive.</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Features</td>\n<td style=\"text-align: left;\">Limited, only third-party services can be used to complete certain \"dynamic\" functions, such as comments</td>\n<td style=\"text-align: left;\">Rich, in WordPress for example, basically any kind of plugins can be found. Featuers such as auto-resizing, media players, multiple authors, scheduled posts, user analysis can be easily realize.</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Speed</td>\n<td style=\"text-align: left;\">Fast</td>\n<td style=\"text-align: left;\">Slow</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Maintainance</td>\n<td style=\"text-align: left;\">0</td>\n<td style=\"text-align: left;\">Need to care about the sever</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Markdown</td>\n<td style=\"text-align: left;\">Supported yet it's the only choice</td>\n<td style=\"text-align: left;\">not supported</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Geeky</td>\n<td style=\"text-align: left;\">YES</td>\n<td style=\"text-align: left;\">No</td>\n</tr>\n</tbody>\n</table>\n<p>And I also chose static because I'm geeky (poor of money) and results-driven (lazy to spend time on maintaining).</p>\n<h2 id=\"build-a-static-blog-via-hexo\">Build a static blog via hexo</h2>\n<h3 id=\"set-environment\">Set environment</h3>\n<p>1.check machine information: macOS on M1 MacBook</p>\n<p>2.Install Nodejs, including node and npm</p>\n<p>open https://nodejs.org/en/download/ and click download</p>\n<p><img src=\"node js install.png\" alt=\"node js install\" style=\"zoom:50%;\" /></p>\n<p>3.Install Git</p>\n<p>Aready installed</p>\n<p>4.Open terminal, check node, npm and git versions</p>\n<pre><code class=\"hljs shell\"><span class=\"hljs-meta\">$ </span><span class=\"language-bash\">npm -v</span>\n<span class=\"hljs-meta\">$ </span><span class=\"language-bash\">node -v</span>\n<span class=\"hljs-meta\">$ </span><span class=\"language-bash\">git --version</span>\n8.3.1\nv16.14.0\ngit version 2.32.0 (Apple Git-132)</code></pre>\n<h3 id=\"initialize-blog\">Initialize blog</h3>\n<p>1.install hexo via npm</p>\n<pre><code class=\"hljs shell\"><span class=\"hljs-meta\">$ </span><span class=\"language-bash\">sudo npm install -g hexo-cli</span>\n<span class=\"hljs-meta\">$ </span><span class=\"language-bash\">hexo -v</span>\nINFO  Validating config\nhexo: 6.0.0\nhexo-cli: 4.3.0\nos: darwin 21.2.0 12.1\n\nnode: 16.14.0\nv8: 9.4.146.24-node.20\nuv: 1.43.0\nzlib: 1.2.11\nbrotli: 1.0.9\nares: 1.18.1\nmodules: 93\nnghttp2: 1.45.1\nnapi: 8\nllhttp: 6.0.4\nopenssl: 1.1.1m+quic\ncldr: 40.0\nicu: 70.1\ntz: 2021a3\nunicode: 14.0\nngtcp2: 0.1.0-DEV\nnghttp3: 0.1.0-DEV</code></pre>\n<p>2.create a new folder in terminal and initialize the blog</p>\n<pre><code class=\"hljs shell\"><span class=\"hljs-meta\">$ </span><span class=\"language-bash\"><span class=\"hljs-built_in\">cd</span> ~/Documents/</span>\n<span class=\"hljs-meta\">$ </span><span class=\"language-bash\">makedir self_blog</span>\n<span class=\"hljs-meta\">$ </span><span class=\"language-bash\"><span class=\"hljs-built_in\">cd</span> self_blog/</span>\n<span class=\"hljs-meta\">$ </span><span class=\"language-bash\">hexo init</span>\nINFO  Cloning hexo-starter https://github.com/hexojs/hexo-starter.gitINFO  Install dependencies⸨#########⠂⠂⠂⠂⠂⠂⠂⠂⠂⸩ ⠹ idealTree:hexo-front-matter: timing idealTree:node_modules/hexo-front-matter Completed in 212msINFO  Start blogging with Hexo!</code></pre>\n<p>3.view the blog on localhost, s for start</p>\n<pre><code class=\"hljs shell\"><span class=\"hljs-meta\">$ </span><span class=\"language-bash\">hexo s</span>\nINFO  Validating config\nINFO  Start processing\nINFO  Hexo is running at http://localhost:4000/ . Press Ctrl+C to stop.</code></pre>\n<h3 id=\"write-first-blog\">Write first blog</h3>\n<p>1.write a new blog, n for new</p>\n<pre><code class=\"hljs shell\"><span class=\"hljs-meta\">$ </span><span class=\"language-bash\">hexo n <span class=\"hljs-string\">&#x27;Hello ShouRou&#x27;</span></span>\nINFO  Validating config\nINFO  Created: ~/Documents/self_blog/source/_posts/Hello-ShouRou.md</code></pre>\n<p>the blog can be written on any editor, Typora in use.</p>\n<pre><code class=\"hljs shell\">open ~/Documents/self_blog/source/_posts/Hello-ShouRou.md</code></pre>\n<p>2.clean cache(not necessary)</p>\n<pre><code class=\"hljs shell\"><span class=\"hljs-meta\">$ </span><span class=\"language-bash\">hexo clean</span></code></pre>\n<p>3.generate the blog, g for generate</p>\n<pre><code class=\"hljs shell\"><span class=\"hljs-meta\">$ </span><span class=\"language-bash\">hexo g</span>\nINFO  Validating config\nINFO  Start processing\nINFO  Files loaded in 61 ms\n(node:10719) Warning: Accessing non-existent property &#x27;lineno&#x27; of module exports inside circular dependency\n(Use `node --trace-warnings ...` to show where the warning was created)\n(node:10719) Warning: Accessing non-existent property &#x27;column&#x27; of module exports inside circular dependency\n(node:10719) Warning: Accessing non-existent property &#x27;filename&#x27; of module exports inside circular dependency\n(node:10719) Warning: Accessing non-existent property &#x27;lineno&#x27; of module exports inside circular dependency\n(node:10719) Warning: Accessing non-existent property &#x27;column&#x27; of module exports inside circular dependency\n(node:10719) Warning: Accessing non-existent property &#x27;filename&#x27; of module exports inside circular dependency\nINFO  Generated: archives/2022/index.html\nINFO  Generated: archives/index.html\nINFO  Generated: js/script.js\nINFO  Generated: fancybox/jquery.fancybox.min.css\nINFO  Generated: index.html\nINFO  Generated: css/style.css\nINFO  Generated: css/fonts/fontawesome-webfont.woff2\nINFO  Generated: fancybox/jquery.fancybox.min.js\nINFO  Generated: js/jquery-3.4.1.min.js\nINFO  Generated: archives/2022/02/index.html\nINFO  Generated: css/fonts/FontAwesome.otf\nINFO  Generated: css/fonts/fontawesome-webfont.woff\nINFO  Generated: css/fonts/fontawesome-webfont.eot\nINFO  Generated: css/fonts/fontawesome-webfont.ttf\nINFO  Generated: css/images/banner.jpg\nINFO  Generated: 2022/02/22/hello-world/index.html\nINFO  Generated: css/fonts/fontawesome-webfont.svg\nINFO  Generated: 2022/02/22/Hello-ShouRou/index.html\nINFO  18 files generated in 161 ms</code></pre>\n<h3 id=\"deploy-to-remote-github\">Deploy to remote (GitHub)</h3>\n<p>1.Create a new repository with the name of $username.github.io</p>\n<p><img src=\"github page.png\" alt=\"github page\" style=\"zoom:50%;\" /></p>\n<p>use default setting</p>\n<p>2.open terminal, install plugin of deploying to git</p>\n<pre><code class=\"hljs shell\"><span class=\"hljs-meta\">$ </span><span class=\"language-bash\">npm install --save hexo-deployer-git</span></code></pre>\n<p>3.Open the <code>_config.yml</code> file in the blog <code>root</code> directory, add these lines afterwards</p>\n<pre><code class=\"hljs yml\"><span class=\"hljs-attr\">deploy:</span>\n <span class=\"hljs-attr\">type:</span> <span class=\"hljs-string\">git</span>\n <span class=\"hljs-attr\">repo:</span> <span class=\"hljs-string\">git@github.com:DaydreamAtNight/DaydreamAtNight.github.io.git</span>\n <span class=\"hljs-attr\">branch:</span> <span class=\"hljs-string\">master</span></code></pre>\n<p>4.Go to the blog <code>root</code>, deploy the blog to remote, d for deploy</p>\n<pre><code class=\"hljs shell\"><span class=\"hljs-meta\">$ </span><span class=\"language-bash\">hexo clean</span>\n<span class=\"hljs-meta\">$ </span><span class=\"language-bash\">hexo g</span>\n<span class=\"hljs-meta\">$ </span><span class=\"language-bash\">hexo d</span></code></pre>\n<p>Open https://daydreamatnight.github.io/ to see if it works</p>\n<h2 id=\"change-theme-to-yilia\">Change theme to yilia</h2>\n<p>default theme of hexo is called landscape and it's not beautiful enough to most of the people. Yilia is a fast, simple, elegant and popular theme. Thought it has not been updated since Nov 2017, it still a good choice for fresh bloggers.</p>\n<h3 id=\"download-and-deploy-yilia\">Download and deploy yilia</h3>\n<p>1.go to the blog <code>root</code></p>\n<pre><code class=\"hljs shell\"><span class=\"hljs-meta\">$ </span><span class=\"language-bash\">git <span class=\"hljs-built_in\">clone</span> https://github.com/litten/hexo-theme-yilia theme/yilia</span></code></pre>\n<p>2.eidt the <code>_config.yml</code> file, add</p>\n<pre><code class=\"hljs yml\"><span class=\"hljs-attr\">theme:</span> <span class=\"hljs-string\">yilia</span></code></pre>\n<p>3.clean and deploy hexo</p>\n<pre><code class=\"hljs shell\"><span class=\"hljs-meta\">$ </span><span class=\"language-bash\">hexo clean</span>\n<span class=\"hljs-meta\">$ </span><span class=\"language-bash\">hexo g</span>\n<span class=\"hljs-meta\">$ </span><span class=\"language-bash\">hexo d</span></code></pre>\n<h3 id=\"basic-customize-yillia\">Basic customize yillia</h3>\n<h4 id=\"activate-aboutme-left-slider-button\">Activate <code>aboutme</code> ‘left slider’ button</h4>\n<p>1.go to terminal run</p>\n<pre><code class=\"hljs shell\"><span class=\"hljs-meta\">$ </span><span class=\"language-bash\">npm i hexo-generator-json-content --save</span></code></pre>\n<p>2.go to the blog <code>root</code> directory, add these lines to the <code>_config.yml</code> file</p>\n<pre><code class=\"hljs yml\"><span class=\"hljs-attr\">jsonContent:</span>\n    <span class=\"hljs-attr\">meta:</span> <span class=\"hljs-literal\">false</span>\n    <span class=\"hljs-attr\">pages:</span> <span class=\"hljs-literal\">false</span>\n    <span class=\"hljs-attr\">posts:</span>\n      <span class=\"hljs-attr\">title:</span> <span class=\"hljs-literal\">true</span>\n      <span class=\"hljs-attr\">date:</span> <span class=\"hljs-literal\">true</span>\n      <span class=\"hljs-attr\">path:</span> <span class=\"hljs-literal\">true</span>\n      <span class=\"hljs-attr\">text:</span> <span class=\"hljs-literal\">false</span>\n      <span class=\"hljs-attr\">raw:</span> <span class=\"hljs-literal\">false</span>\n      <span class=\"hljs-attr\">content:</span> <span class=\"hljs-literal\">false</span>\n      <span class=\"hljs-attr\">slug:</span> <span class=\"hljs-literal\">false</span>\n      <span class=\"hljs-attr\">updated:</span> <span class=\"hljs-literal\">false</span>\n      <span class=\"hljs-attr\">comments:</span> <span class=\"hljs-literal\">false</span>\n      <span class=\"hljs-attr\">link:</span> <span class=\"hljs-literal\">false</span>\n      <span class=\"hljs-attr\">permalink:</span> <span class=\"hljs-literal\">false</span>\n      <span class=\"hljs-attr\">excerpt:</span> <span class=\"hljs-literal\">false</span>\n      <span class=\"hljs-attr\">categories:</span> <span class=\"hljs-literal\">false</span>\n      <span class=\"hljs-attr\">tags:</span> <span class=\"hljs-literal\">true</span></code></pre>\n<h4 id=\"customize-avatar\">Customize avatar</h4>\n<p>put the avatar file in directory <code>themes/yilia/source/img</code></p>\n<blockquote>\n<p>do not add to the public repository directly, or the img get cleaned every time running <code>hexo clean</code> , need to upload to the same dir again after this command.</p>\n</blockquote>\n<h4 id=\"set-favicon-icon-on-the-tab-of-website\">Set favicon (icon on the tab of website)</h4>\n<p>put the favicon img in directory <code>themes/yilia/source/img</code></p>\n<p><a href=\"https://www.bitbug.net/\">Bitbug</a> is a way of converting image into .ico file.</p>\n<h4 id=\"other-configuration\">Other configuration</h4>\n<p>Set file of yillia is in <code>themes/yilia/_config.yml</code> as:</p>\n<pre><code class=\"hljs yml\"><span class=\"hljs-comment\"># Header</span>\n<span class=\"hljs-attr\">author:</span> <span class=\"hljs-string\">Ryan</span> <span class=\"hljs-string\">LI</span>\n<span class=\"hljs-attr\">subtitle:</span> <span class=\"hljs-string\">&#x27;Daydreaming at night&#x27;</span>\n<span class=\"hljs-attr\">menu:</span>\n  <span class=\"hljs-attr\">main:</span> <span class=\"hljs-string\">/</span>\n  <span class=\"hljs-attr\">archives:</span> <span class=\"hljs-string\">/archives/index.html</span>\n  <span class=\"hljs-attr\">learn:</span> <span class=\"hljs-string\">/tags/learn/</span>\n\n<span class=\"hljs-comment\"># SubNav</span>\n<span class=\"hljs-attr\">subnav:</span>\n  <span class=\"hljs-attr\">github:</span> <span class=\"hljs-string\">&quot;https://github.com/DaydreamAtNight&quot;</span>\n  <span class=\"hljs-comment\"># weibo: &quot;#&quot;</span>\n  <span class=\"hljs-comment\"># rss: &quot;#&quot;</span>\n  <span class=\"hljs-comment\"># zhihu: &quot;#&quot;</span>\n  <span class=\"hljs-comment\">#qq: &quot;#&quot;</span>\n  <span class=\"hljs-comment\">#weixin: &quot;#&quot;</span>\n  <span class=\"hljs-comment\">#jianshu: &quot;#&quot;</span>\n  <span class=\"hljs-comment\">#douban: &quot;#&quot;</span>\n  <span class=\"hljs-comment\">#segmentfault: &quot;#&quot;</span>\n  <span class=\"hljs-comment\">#bilibili: &quot;#&quot;</span>\n  <span class=\"hljs-comment\">#acfun: &quot;#&quot;</span>\n  <span class=\"hljs-attr\">mail:</span> <span class=\"hljs-string\">&quot;mailto:lishoushou2019@gmail.com&quot;</span>\n  <span class=\"hljs-comment\">#facebook: &quot;#&quot;</span>\n  <span class=\"hljs-comment\">#google: &quot;#&quot;</span>\n  <span class=\"hljs-comment\">#twitter: &quot;#&quot;</span>\n  <span class=\"hljs-comment\">#linkedin: &quot;#&quot;</span>\n\n<span class=\"hljs-attr\">rss:</span> <span class=\"hljs-string\">/atom.xml</span>\n\n<span class=\"hljs-comment\"># 是否需要修改 root 路径</span>\n<span class=\"hljs-comment\"># 如果您的网站存放在子目录中，例如 http://yoursite.com/blog，</span>\n<span class=\"hljs-comment\"># 请将您的 url 设为 http://yoursite.com/blog 并把 root 设为 /blog/。</span>\n<span class=\"hljs-attr\">root:</span> <span class=\"hljs-string\">/</span>\n\n<span class=\"hljs-comment\"># Content</span>\n\n<span class=\"hljs-comment\"># 文章太长，截断按钮文字</span>\n<span class=\"hljs-comment\"># excerpt_link: more</span>\n<span class=\"hljs-comment\"># 文章卡片右下角常驻链接，不需要请设置为false</span>\n<span class=\"hljs-attr\">show_all_link:</span> <span class=\"hljs-string\">&#x27;show all&#x27;</span>\n<span class=\"hljs-comment\"># 数学公式</span>\n<span class=\"hljs-attr\">mathjax:</span> <span class=\"hljs-literal\">false</span>\n<span class=\"hljs-comment\"># 是否在新窗口打开链接</span>\n<span class=\"hljs-attr\">open_in_new:</span> <span class=\"hljs-literal\">false</span>\n\n<span class=\"hljs-comment\"># 打赏</span>\n<span class=\"hljs-comment\"># 打赏type设定：0-关闭打赏； 1-文章对应的md文件里有reward:true属性，才有打赏； 2-所有文章均有打赏</span>\n<span class=\"hljs-attr\">reward_type:</span> <span class=\"hljs-number\">0</span>\n<span class=\"hljs-comment\"># # 打赏wording</span>\n<span class=\"hljs-comment\"># reward_wording: &#x27;谢谢你请我吃糖果&#x27;</span>\n<span class=\"hljs-comment\"># # 支二维码图片地址，跟你设置头像的方式一样。比如：/assets/img/alipay.jpg</span>\n<span class=\"hljs-comment\"># alipay: </span>\n<span class=\"hljs-comment\"># # 微信二维码图片地址</span>\n<span class=\"hljs-comment\"># weixin: </span>\n\n<span class=\"hljs-comment\"># 目录</span>\n<span class=\"hljs-comment\"># 目录设定：0-不显示目录； 1-文章对应的md文件里有toc:true属性，才有目录； 2-所有文章均显示目录</span>\n<span class=\"hljs-attr\">toc:</span> <span class=\"hljs-number\">1</span>\n<span class=\"hljs-comment\"># 根据自己的习惯来设置，如果你的目录标题习惯有标号，置为true即可隐藏hexo重复的序号；否则置为false</span>\n<span class=\"hljs-attr\">toc_hide_index:</span> <span class=\"hljs-literal\">true</span>\n<span class=\"hljs-comment\"># 目录为空时的提示</span>\n<span class=\"hljs-attr\">toc_empty_wording:</span> <span class=\"hljs-string\">&#x27;directery none exist&#x27;</span>\n\n<span class=\"hljs-comment\"># 是否有快速回到顶部的按钮</span>\n<span class=\"hljs-attr\">top:</span> <span class=\"hljs-literal\">true</span>\n\n<span class=\"hljs-comment\"># Miscellaneous</span>\n<span class=\"hljs-attr\">baidu_analytics:</span> <span class=\"hljs-string\">&#x27;&#x27;</span>\n<span class=\"hljs-attr\">google_analytics:</span> <span class=\"hljs-string\">&#x27;&#x27;</span>\n<span class=\"hljs-attr\">favicon:</span> <span class=\"hljs-string\">/img/favicon.ico</span>\n\n<span class=\"hljs-comment\">#你的头像url</span>\n<span class=\"hljs-attr\">avatar:</span> <span class=\"hljs-string\">/img/avatar.jpeg</span>\n\n<span class=\"hljs-comment\">#是否开启分享</span>\n<span class=\"hljs-comment\"># share_jia: true</span>\n\n<span class=\"hljs-comment\"># #评论：1、多说；2、网易云跟帖；3、畅言；4、Disqus；5、Gitment</span>\n<span class=\"hljs-comment\"># #不需要使用某项，直接设置值为false，或注释掉</span>\n<span class=\"hljs-comment\"># #具体请参考wiki：https://github.com/litten/hexo-theme-yilia/wiki/</span>\n\n<span class=\"hljs-comment\"># #1、多说</span>\n<span class=\"hljs-comment\"># duoshuo: false</span>\n\n<span class=\"hljs-comment\"># #2、网易云跟帖</span>\n<span class=\"hljs-comment\"># wangyiyun: false</span>\n\n<span class=\"hljs-comment\"># #3、畅言</span>\n<span class=\"hljs-comment\"># changyan_appid: false</span>\n<span class=\"hljs-comment\"># changyan_conf: false</span>\n\n<span class=\"hljs-comment\"># #4、Disqus 在hexo根目录的config里也有disqus_shortname字段，优先使用yilia的</span>\n<span class=\"hljs-comment\"># disqus: false</span>\n\n<span class=\"hljs-comment\"># #5、Gitment</span>\n<span class=\"hljs-comment\"># gitment_owner: false      #你的 GitHub ID</span>\n<span class=\"hljs-comment\"># gitment_repo: &#x27;&#x27;          #存储评论的 repo</span>\n<span class=\"hljs-comment\"># gitment_oauth:</span>\n<span class=\"hljs-comment\">#   client_id: &#x27;&#x27;           #client ID</span>\n<span class=\"hljs-comment\">#   client_secret: &#x27;&#x27;       #client secret</span>\n\n<span class=\"hljs-comment\"># 样式定制 - 一般不需要修改，除非有很强的定制欲望…</span>\n<span class=\"hljs-attr\">style:</span>\n  <span class=\"hljs-comment\"># 头像上面的背景颜色</span>\n  <span class=\"hljs-attr\">header:</span> <span class=\"hljs-string\">&#x27;#ece0cf&#x27;</span>\n  <span class=\"hljs-comment\"># 右滑板块背景</span>\n  <span class=\"hljs-attr\">slider:</span> <span class=\"hljs-string\">&#x27;linear-gradient(45deg,#b4a698,#ece0cf)&#x27;</span>\n\n<span class=\"hljs-comment\"># slider的设置</span>\n<span class=\"hljs-attr\">slider:</span>\n  <span class=\"hljs-comment\"># 是否默认展开tags板块</span>\n  <span class=\"hljs-attr\">showTags:</span> <span class=\"hljs-literal\">false</span>\n\n<span class=\"hljs-comment\"># 智能菜单</span>\n<span class=\"hljs-comment\"># 如不需要，将该对应项置为false</span>\n<span class=\"hljs-comment\"># 比如</span>\n<span class=\"hljs-comment\">#smart_menu:</span>\n<span class=\"hljs-comment\">#  friends: false</span>\n<span class=\"hljs-attr\">smart_menu:</span>\n  <span class=\"hljs-attr\">innerArchive:</span> <span class=\"hljs-string\">&#x27;All articles&#x27;</span>\n  <span class=\"hljs-comment\"># friends: &#x27;友链&#x27;</span>\n  <span class=\"hljs-attr\">aboutme:</span> <span class=\"hljs-string\">&#x27;About me&#x27;</span>\n\n<span class=\"hljs-comment\"># friends:</span>\n<span class=\"hljs-comment\">#   友情链接1: http://localhost:4000/</span>\n<span class=\"hljs-comment\">#   友情链接2: http://localhost:4000/</span>\n<span class=\"hljs-comment\">#   友情链接3: http://localhost:4000/</span>\n<span class=\"hljs-comment\">#   友情链接4: http://localhost:4000/</span>\n<span class=\"hljs-comment\">#   友情链接5: http://localhost:4000/</span>\n<span class=\"hljs-comment\">#   友情链接6: http://localhost:4000/</span>\n\n<span class=\"hljs-attr\">aboutme:</span> <span class=\"hljs-string\">Stay</span> <span class=\"hljs-string\">hungry,</span> <span class=\"hljs-string\">stay</span> <span class=\"hljs-string\">fullish</span></code></pre>\n<h2 id=\"advance-customize\">Advance customize</h2>\n<h3 id=\"stop-visit-litten.me9005\">Stop visit litten.me:9005</h3>\n<p>Sometimes the user's client information is collected, see <a href=\"https://github.com/litten/hexo-theme-yilia/issues/528\">here</a> for details.</p>\n<p>Stop reporting by clear the contents in <code>themes/yilia/source-src/js/report.js</code></p>\n<h3 id=\"limit-display-numbers-on-the-main-page\">Limit display numbers on the main page</h3>\n<p>Simply insert <code>&lt;! -- more --&gt;</code> to show only what comes before it while collapse the afterwards, click on the article title to read it in full.</p>\n<h3 id=\"easily-add-pics-to-blogs-via-hexo-renderer-marked-plugin\">Easily add pics to blogs via hexo-renderer-marked plugin</h3>\n<p>1.find <code>post_asset_folder</code> in <code>_config.yml</code> file in the blog <code>root</code> directory, set to be true</p>\n<pre><code class=\"hljs yml\"><span class=\"hljs-string\">post_asset_folder:true</span></code></pre>\n<p>2.Install plugin</p>\n<pre><code class=\"hljs shell\">npm install hexo-renderer-marked --save</code></pre>\n<p>3.change <code>_config.yml</code> in blog <code>root</code> directory as</p>\n<pre><code class=\"hljs yml\"><span class=\"hljs-attr\">post_asset_folder:</span> <span class=\"hljs-literal\">true</span>\n<span class=\"hljs-attr\">marked:</span>\n  <span class=\"hljs-attr\">prependRoot:</span> <span class=\"hljs-literal\">true</span>\n  <span class=\"hljs-attr\">postAsset:</span> <span class=\"hljs-literal\">true</span></code></pre>\n<p>then img can be easily add with <code>![img description](img.png)</code> after add the image to the folder with the same name as the article in <code>/source/_posts/</code></p>\n<p>4.change Typora pereference as</p>\n<p><img src=\"typora setting.png\" alt=\"typora setting\" style=\"zoom:50%;\" /></p>\n<p>img can drag into typro, yet blogname need to be deleted before deploying</p>\n<h3 id=\"show-number-of-articles-and-words-on-the-left-panel\">Show number of articles and words on the left panel</h3>\n<p>1.add wordcount plugin in terminal</p>\n<pre><code class=\"hljs shell\">npm i --save hexo-wordcount</code></pre>\n<p>2.change <code>themes/yilia/layout/_partial/left-col.ejs</code></p>\n<p>after</p>\n<pre><code class=\"hljs html\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">nav</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;header-menu&quot;</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">ul</span>&gt;</span>\n    &lt;% for (var i in theme.menu)&#123; %&gt;\n      <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">li</span>&gt;</span><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">a</span> <span class=\"hljs-attr\">href</span>=<span class=\"hljs-string\">&quot;&lt;%- url_for(theme.menu[i]) %&gt;&quot;</span>&gt;</span>&lt;%= i %&gt;<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">a</span>&gt;</span><span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">li</span>&gt;</span>\n    &lt;%&#125;%&gt;\n  <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">ul</span>&gt;</span>\n<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">nav</span>&gt;</span></code></pre>\n<p>add</p>\n<pre><code class=\"hljs html\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">span</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;post-count&quot;</span>&gt;</span>&lt;%=site.posts.length%&gt; articles\n\t\t\t<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">span</span>&gt;</span>&lt;%= totalcount(site, &#x27;0,0.0a&#x27;) %&gt;<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">span</span>&gt;</span> words<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">span</span>&gt;</span></code></pre>\n<p>add style sheet in <code>themes/yilia/source/main.0cf68a.css</code></p>\n<pre><code class=\"hljs css\"><span class=\"hljs-selector-class\">.post-count</span>&#123;\n  <span class=\"hljs-attribute\">font-size</span>: <span class=\"hljs-number\">12px</span>;\n  <span class=\"hljs-attribute\">color</span>: <span class=\"hljs-number\">#696969</span>;\n&#125;</code></pre>\n<h3 id=\"show-number-of-visits-in-the-footer\">Show number of visits in the footer</h3>\n<p><a href=\"https://busuanzi.ibruce.info/\">busuanzi</a> is in use, which is super easy to deploy</p>\n<p>change <code>themes/yilia/layout/_partial/footer.ejs</code> as</p>\n<pre><code class=\"hljs html\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">footer</span> <span class=\"hljs-attr\">id</span>=<span class=\"hljs-string\">&quot;footer&quot;</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;outer&quot;</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">id</span>=<span class=\"hljs-string\">&quot;footer-info&quot;</span>&gt;</span>\n    \t<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;footer-left&quot;</span>&gt;</span>\n    \t\t<span class=\"hljs-comment\">&lt;!-- total visits number --&gt;</span>\n          &lt;% if (theme.busuanzi &amp;&amp; theme.busuanzi.enable)&#123; %&gt;\n            <span class=\"hljs-comment\">&lt;!-- busuanzi statistics --&gt;</span>\n            <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">span</span> <span class=\"hljs-attr\">id</span>=<span class=\"hljs-string\">&quot;busuanzi_value_site_pv&quot;</span>&gt;</span><span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">span</span>&gt;</span><span class=\"hljs-symbol\">&amp;nbsp;</span>visits in total\n            <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">script</span> <span class=\"hljs-attr\">async</span> <span class=\"hljs-attr\">src</span>=<span class=\"hljs-string\">&quot;//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js&quot;</span>&gt;</span><span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">script</span>&gt;</span>\n          &lt;% &#125; %&gt;\n        <span class=\"hljs-comment\">&lt;!-- end --&gt;</span>\n    \t<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">div</span>&gt;</span>\n      \t<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;footer-right&quot;</span>&gt;</span>\n      \t\t<span class=\"hljs-symbol\">&amp;copy;</span> &lt;%= date(new Date(), &#x27;YYYY&#x27;) %&gt; &lt;%= config.author || config.title %&gt;\n      \t<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">div</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">div</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">div</span>&gt;</span>\n<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">footer</span>&gt;</span></code></pre>\n<p>and add</p>\n<pre><code class=\"hljs yml\"><span class=\"hljs-attr\">busuanzi:</span>\n  <span class=\"hljs-attr\">enable:</span> <span class=\"hljs-literal\">true</span></code></pre>\n<h3 id=\"add-button-of-hiding-the-left-panel\">Add button of hiding the left panel</h3>\n<p>Refer to <a href=\"https://cqh-i.github.io/2019/08/07/hexo-yilia%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E9%9A%90%E8%97%8F%E5%B7%A6%E8%BE%B9%E6%A0%8F%E7%9B%AE%E6%8C%89%E9%92%AE/\">hexo yilia主题添加隐藏左边栏目按钮</a></p>\n<p>1.add style list to file <code>/themes/yilia/source/main.0cf68a.css</code></p>\n<pre><code class=\"hljs css\"><span class=\"hljs-comment\">/*stylesheet for hide the left panel*/</span>\n<span class=\"hljs-selector-class\">.mymenucontainer</span> &#123;\n\t<span class=\"hljs-attribute\">display</span>:block;\n\t<span class=\"hljs-attribute\">cursor</span>:pointer;\n\t<span class=\"hljs-attribute\">left</span>:<span class=\"hljs-number\">0</span>;\n\t<span class=\"hljs-attribute\">top</span>:<span class=\"hljs-number\">0</span>;\n\t<span class=\"hljs-attribute\">width</span>:<span class=\"hljs-number\">35px</span>;\n\t<span class=\"hljs-attribute\">height</span>:<span class=\"hljs-number\">35px</span>;\n\t<span class=\"hljs-attribute\">z-index</span>:<span class=\"hljs-number\">9999</span>;\n\t<span class=\"hljs-attribute\">position</span>:fixed;\n&#125;\n<span class=\"hljs-selector-class\">.bar1</span> &#123;\n\t<span class=\"hljs-attribute\">width</span>:<span class=\"hljs-number\">35px</span>;\n\t<span class=\"hljs-attribute\">height</span>:<span class=\"hljs-number\">3px</span>;\n\t<span class=\"hljs-attribute\">border-radius</span>:<span class=\"hljs-number\">3px</span>;\n\t<span class=\"hljs-attribute\">background-color</span>:<span class=\"hljs-number\">#8E6D51</span>;\n\t<span class=\"hljs-attribute\">margin</span>:<span class=\"hljs-number\">6px</span> <span class=\"hljs-number\">0</span>;\n\t<span class=\"hljs-attribute\">transition</span>:<span class=\"hljs-number\">0.1s</span>;\n\t-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">rotate</span>(-<span class=\"hljs-number\">45deg</span>) <span class=\"hljs-built_in\">translate</span>(-<span class=\"hljs-number\">8px</span>,<span class=\"hljs-number\">8px</span>);\n\t<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">rotate</span>(-<span class=\"hljs-number\">45deg</span>) <span class=\"hljs-built_in\">translate</span>(-<span class=\"hljs-number\">8px</span>,<span class=\"hljs-number\">8px</span>);\n&#125;\n<span class=\"hljs-selector-class\">.bar2</span> &#123;\n\t<span class=\"hljs-attribute\">width</span>:<span class=\"hljs-number\">35px</span>;\n\t<span class=\"hljs-attribute\">height</span>:<span class=\"hljs-number\">3px</span>;\n\t<span class=\"hljs-attribute\">border-radius</span>:<span class=\"hljs-number\">3px</span>;\n\t<span class=\"hljs-attribute\">background-color</span>:<span class=\"hljs-number\">#8E6D51</span>;\n\t<span class=\"hljs-attribute\">margin</span>:<span class=\"hljs-number\">6px</span> <span class=\"hljs-number\">0</span>;\n\t<span class=\"hljs-attribute\">transition</span>:<span class=\"hljs-number\">0.1s</span>;\n\t<span class=\"hljs-attribute\">opacity</span>:<span class=\"hljs-number\">0</span>;\n&#125;\n<span class=\"hljs-selector-class\">.bar3</span> &#123;\n\t<span class=\"hljs-attribute\">width</span>:<span class=\"hljs-number\">35px</span>;\n\t<span class=\"hljs-attribute\">height</span>:<span class=\"hljs-number\">3px</span>;\n\t<span class=\"hljs-attribute\">border-radius</span>:<span class=\"hljs-number\">3px</span>;\n\t<span class=\"hljs-attribute\">background-color</span>:<span class=\"hljs-number\">#8E6D51</span>;\n\t<span class=\"hljs-attribute\">margin</span>:<span class=\"hljs-number\">6px</span> <span class=\"hljs-number\">0</span>;\n\t<span class=\"hljs-attribute\">transition</span>:<span class=\"hljs-number\">0.1s</span>;\n\t-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">rotate</span>(<span class=\"hljs-number\">45deg</span>) <span class=\"hljs-built_in\">translate</span>(-<span class=\"hljs-number\">4px</span>,-<span class=\"hljs-number\">6px</span>);\n\t<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">rotate</span>(<span class=\"hljs-number\">45deg</span>) <span class=\"hljs-built_in\">translate</span>(-<span class=\"hljs-number\">4px</span>,-<span class=\"hljs-number\">6px</span>);\n&#125;\n<span class=\"hljs-selector-class\">.change</span> <span class=\"hljs-selector-class\">.bar1</span> &#123;\n\t-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">rotate</span>(<span class=\"hljs-number\">0deg</span>) <span class=\"hljs-built_in\">translate</span>(<span class=\"hljs-number\">0px</span>,<span class=\"hljs-number\">0px</span>);\n\t<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">rotate</span>(<span class=\"hljs-number\">0deg</span>) <span class=\"hljs-built_in\">translate</span>(<span class=\"hljs-number\">0px</span>,<span class=\"hljs-number\">0px</span>);\n&#125;\n<span class=\"hljs-selector-class\">.change</span> <span class=\"hljs-selector-class\">.bar2</span> &#123;\n\t<span class=\"hljs-attribute\">opacity</span>:<span class=\"hljs-number\">1</span>;\n&#125;\n<span class=\"hljs-selector-class\">.change</span> <span class=\"hljs-selector-class\">.bar3</span> &#123;\n\t-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">rotate</span>(<span class=\"hljs-number\">0deg</span>) <span class=\"hljs-built_in\">translate</span>(<span class=\"hljs-number\">0px</span>,<span class=\"hljs-number\">0px</span>);\n\t<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">rotate</span>(<span class=\"hljs-number\">0deg</span>) <span class=\"hljs-built_in\">translate</span>(<span class=\"hljs-number\">0px</span>,<span class=\"hljs-number\">0px</span>);\n&#125;\n<span class=\"hljs-comment\">/*stylesheet for hide the left panel end*/</span></code></pre>\n<p>2.go to <code>/themes/yilia/layout/layout.ejs</code> add before <code>&lt;div class=\"left-col\"</code></p>\n<pre><code class=\"hljs html\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;mymenucontainer&quot;</span> <span class=\"hljs-attr\">onclick</span>=<span class=\"hljs-string\">&quot;myFunction(this)&quot;</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;bar1&quot;</span>&gt;</span><span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">div</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;bar2&quot;</span>&gt;</span><span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">div</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;bar3&quot;</span>&gt;</span><span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">div</span>&gt;</span>\n<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">div</span>&gt;</span></code></pre>\n<p>3.add between <code>&lt;/body&gt;</code> and <code>&lt;/html&gt;</code></p>\n<pre><code class=\"hljs js\">&lt;script&gt;\n    <span class=\"hljs-keyword\">var</span> hide = <span class=\"hljs-literal\">false</span>;\n    <span class=\"hljs-keyword\">function</span> <span class=\"hljs-title function_\">myFunction</span>(<span class=\"hljs-params\">x</span>) &#123;\n        x.<span class=\"hljs-property\">classList</span>.<span class=\"hljs-title function_\">toggle</span>(<span class=\"hljs-string\">&quot;change&quot;</span>);\n        <span class=\"hljs-keyword\">if</span>(hide == <span class=\"hljs-literal\">false</span>)&#123;\n            $(<span class=\"hljs-string\">&quot;.left-col&quot;</span>).<span class=\"hljs-title function_\">css</span>(<span class=\"hljs-string\">&#x27;display&#x27;</span>, <span class=\"hljs-string\">&#x27;none&#x27;</span>);\n            $(<span class=\"hljs-string\">&quot;.mid-col&quot;</span>).<span class=\"hljs-title function_\">css</span>(<span class=\"hljs-string\">&quot;left&quot;</span>, <span class=\"hljs-number\">6</span>);\n            $(<span class=\"hljs-string\">&quot;.tools-col&quot;</span>).<span class=\"hljs-title function_\">css</span>(<span class=\"hljs-string\">&#x27;display&#x27;</span>, <span class=\"hljs-string\">&#x27;none&#x27;</span>);\n            $(<span class=\"hljs-string\">&quot;.tools-col.hide&quot;</span>).<span class=\"hljs-title function_\">css</span>(<span class=\"hljs-string\">&#x27;display&#x27;</span>, <span class=\"hljs-string\">&#x27;none&#x27;</span>);\n            hide = <span class=\"hljs-literal\">true</span>;\n        &#125;<span class=\"hljs-keyword\">else</span>&#123;\n            $(<span class=\"hljs-string\">&quot;.left-col&quot;</span>).<span class=\"hljs-title function_\">css</span>(<span class=\"hljs-string\">&#x27;display&#x27;</span>, <span class=\"hljs-string\">&#x27;&#x27;</span>);\n            $(<span class=\"hljs-string\">&quot;.mid-col&quot;</span>).<span class=\"hljs-title function_\">css</span>(<span class=\"hljs-string\">&quot;left&quot;</span>, <span class=\"hljs-number\">300</span>);\n            $(<span class=\"hljs-string\">&quot;.tools-col&quot;</span>).<span class=\"hljs-title function_\">css</span>(<span class=\"hljs-string\">&#x27;display&#x27;</span>, <span class=\"hljs-string\">&#x27;&#x27;</span>);\n            $(<span class=\"hljs-string\">&quot;.tools-col.hide&quot;</span>).<span class=\"hljs-title function_\">css</span>(<span class=\"hljs-string\">&#x27;display&#x27;</span>, <span class=\"hljs-string\">&#x27;&#x27;</span>);\n            hide = <span class=\"hljs-literal\">false</span>;\n        &#125;\n    &#125;\n&lt;/script&gt;</code></pre>\n<h3 id=\"beautiful-contents-navigation-in-articles\">Beautiful contents navigation in articles</h3>\n<p>Default navigator is kind of ugly so found a more beautiful version, to use default version, simply change <code>toc: 2</code> in file <code>themes/yilia/_config.yml</code></p>\n<p>1.add this block at the end of <code>themes/yilia/source/main.0cf68a.css</code></p>\n<pre><code class=\"hljs css\"><span class=\"hljs-comment\">/* navigator */</span>\n<span class=\"hljs-selector-id\">#container</span> <span class=\"hljs-selector-class\">.show-toc-btn</span>,<span class=\"hljs-selector-id\">#container</span> <span class=\"hljs-selector-class\">.toc-article</span>&#123;<span class=\"hljs-attribute\">display</span>:block&#125;\n<span class=\"hljs-selector-class\">.toc-article</span>&#123;<span class=\"hljs-attribute\">z-index</span>:<span class=\"hljs-number\">100</span>;<span class=\"hljs-attribute\">background</span>:<span class=\"hljs-number\">#fff</span>;<span class=\"hljs-attribute\">border</span>:<span class=\"hljs-number\">1px</span> solid <span class=\"hljs-number\">#ccc</span>;<span class=\"hljs-attribute\">max-width</span>:<span class=\"hljs-number\">250px</span>;<span class=\"hljs-attribute\">min-width</span>:<span class=\"hljs-number\">150px</span>;<span class=\"hljs-attribute\">max-height</span>:<span class=\"hljs-number\">500px</span>;<span class=\"hljs-attribute\">overflow-y</span>:auto;-webkit-<span class=\"hljs-attribute\">box-shadow</span>:<span class=\"hljs-number\">5px</span> <span class=\"hljs-number\">5px</span> <span class=\"hljs-number\">2px</span> <span class=\"hljs-number\">#ccc</span>;<span class=\"hljs-attribute\">box-shadow</span>:<span class=\"hljs-number\">5px</span> <span class=\"hljs-number\">5px</span> <span class=\"hljs-number\">2px</span> <span class=\"hljs-number\">#ccc</span>;<span class=\"hljs-attribute\">font-size</span>:<span class=\"hljs-number\">12px</span>;<span class=\"hljs-attribute\">padding</span>:<span class=\"hljs-number\">10px</span>;<span class=\"hljs-attribute\">position</span>:fixed;<span class=\"hljs-attribute\">right</span>:<span class=\"hljs-number\">35px</span>;<span class=\"hljs-attribute\">top</span>:<span class=\"hljs-number\">129px</span>&#125;<span class=\"hljs-selector-class\">.toc-article</span> <span class=\"hljs-selector-class\">.toc-close</span>&#123;<span class=\"hljs-attribute\">font-weight</span>:<span class=\"hljs-number\">700</span>;<span class=\"hljs-attribute\">font-size</span>:<span class=\"hljs-number\">20px</span>;<span class=\"hljs-attribute\">cursor</span>:pointer;<span class=\"hljs-attribute\">float</span><span class=\"hljs-selector-pseudo\">:right</span>;<span class=\"hljs-attribute\">color</span>:<span class=\"hljs-number\">#ccc</span>&#125;<span class=\"hljs-selector-class\">.toc-article</span> <span class=\"hljs-selector-class\">.toc-close</span><span class=\"hljs-selector-pseudo\">:hover</span>&#123;<span class=\"hljs-attribute\">color</span>:<span class=\"hljs-number\">#000</span>&#125;<span class=\"hljs-selector-class\">.toc-article</span> <span class=\"hljs-selector-class\">.toc</span>&#123;<span class=\"hljs-attribute\">font-size</span>:<span class=\"hljs-number\">12px</span>;<span class=\"hljs-attribute\">padding</span>:<span class=\"hljs-number\">0</span>;<span class=\"hljs-attribute\">line-height</span>:<span class=\"hljs-number\">20px</span>&#125;<span class=\"hljs-selector-class\">.toc-article</span> <span class=\"hljs-selector-class\">.toc</span> <span class=\"hljs-selector-class\">.toc-number</span>&#123;<span class=\"hljs-attribute\">color</span>:<span class=\"hljs-number\">#333</span>&#125;<span class=\"hljs-selector-class\">.toc-article</span> <span class=\"hljs-selector-class\">.toc</span> <span class=\"hljs-selector-class\">.toc-text</span><span class=\"hljs-selector-pseudo\">:hover</span>&#123;<span class=\"hljs-attribute\">text-decoration</span>:underline;<span class=\"hljs-attribute\">color</span>:<span class=\"hljs-number\">#2a6496</span>&#125;<span class=\"hljs-selector-class\">.toc-article</span> <span class=\"hljs-selector-tag\">li</span>&#123;<span class=\"hljs-attribute\">list-style-type</span>:none&#125;<span class=\"hljs-selector-class\">.toc-article</span> <span class=\"hljs-selector-class\">.toc-level-1</span>&#123;<span class=\"hljs-attribute\">margin</span>:<span class=\"hljs-number\">4px</span> <span class=\"hljs-number\">0</span>&#125;<span class=\"hljs-selector-class\">.toc-article</span> <span class=\"hljs-selector-class\">.toc-child</span>&#123;&#125;<span class=\"hljs-keyword\">@-moz-keyframes</span> cd-bounce-<span class=\"hljs-number\">1</span>&#123;<span class=\"hljs-number\">0%</span>&#123;<span class=\"hljs-attribute\">opacity</span>:<span class=\"hljs-number\">0</span>;-o-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-moz-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-ms-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>)&#125;<span class=\"hljs-number\">60%</span>&#123;<span class=\"hljs-attribute\">opacity</span>:<span class=\"hljs-number\">1</span>;-o-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);-moz-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);-ms-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>)&#125;<span class=\"hljs-number\">100%</span>&#123;-o-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-moz-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-ms-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>)&#125;&#125;<span class=\"hljs-keyword\">@-webkit-keyframes</span> cd-bounce-<span class=\"hljs-number\">1</span>&#123;<span class=\"hljs-number\">0%</span>&#123;<span class=\"hljs-attribute\">opacity</span>:<span class=\"hljs-number\">0</span>;-o-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-moz-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-ms-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>)&#125;<span class=\"hljs-number\">60%</span>&#123;<span class=\"hljs-attribute\">opacity</span>:<span class=\"hljs-number\">1</span>;-o-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);-moz-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);-ms-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>)&#125;<span class=\"hljs-number\">100%</span>&#123;-o-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-moz-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-ms-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>)&#125;&#125;<span class=\"hljs-keyword\">@-o-keyframes</span> cd-bounce-<span class=\"hljs-number\">1</span>&#123;<span class=\"hljs-number\">0%</span>&#123;<span class=\"hljs-attribute\">opacity</span>:<span class=\"hljs-number\">0</span>;-o-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-moz-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-ms-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>)&#125;<span class=\"hljs-number\">60%</span>&#123;<span class=\"hljs-attribute\">opacity</span>:<span class=\"hljs-number\">1</span>;-o-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);-moz-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);-ms-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>)&#125;<span class=\"hljs-number\">100%</span>&#123;-o-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-moz-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-ms-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>)&#125;&#125;<span class=\"hljs-keyword\">@keyframes</span> cd-bounce-<span class=\"hljs-number\">1</span>&#123;<span class=\"hljs-number\">0%</span>&#123;<span class=\"hljs-attribute\">opacity</span>:<span class=\"hljs-number\">0</span>;-o-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-moz-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-ms-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>)&#125;<span class=\"hljs-number\">60%</span>&#123;<span class=\"hljs-attribute\">opacity</span>:<span class=\"hljs-number\">1</span>;-o-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);-moz-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);-ms-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>);<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1.01</span>)&#125;<span class=\"hljs-number\">100%</span>&#123;-o-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-webkit-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-moz-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);-ms-<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>);<span class=\"hljs-attribute\">transform</span>:<span class=\"hljs-built_in\">scale</span>(<span class=\"hljs-number\">1</span>)&#125;&#125;<span class=\"hljs-selector-class\">.show-toc-btn</span>&#123;<span class=\"hljs-attribute\">display</span>:none;<span class=\"hljs-attribute\">z-index</span>:<span class=\"hljs-number\">10</span>;<span class=\"hljs-attribute\">width</span>:<span class=\"hljs-number\">30px</span>;<span class=\"hljs-attribute\">min-height</span>:<span class=\"hljs-number\">14px</span>;<span class=\"hljs-attribute\">overflow</span>:hidden;<span class=\"hljs-attribute\">padding</span>:<span class=\"hljs-number\">4px</span> <span class=\"hljs-number\">6px</span> <span class=\"hljs-number\">8px</span> <span class=\"hljs-number\">5px</span>;<span class=\"hljs-attribute\">border</span>:<span class=\"hljs-number\">1px</span> solid <span class=\"hljs-number\">#ddd</span>;<span class=\"hljs-attribute\">border-right</span>:none;<span class=\"hljs-attribute\">position</span>:fixed;<span class=\"hljs-attribute\">right</span>:<span class=\"hljs-number\">40px</span>;<span class=\"hljs-attribute\">text-align</span>:center;<span class=\"hljs-attribute\">background-color</span>:<span class=\"hljs-number\">#f9f9f9</span>&#125;<span class=\"hljs-selector-class\">.show-toc-btn</span> <span class=\"hljs-selector-class\">.btn-bg</span>&#123;<span class=\"hljs-attribute\">margin-top</span>:<span class=\"hljs-number\">2px</span>;<span class=\"hljs-attribute\">display</span>:block;<span class=\"hljs-attribute\">width</span>:<span class=\"hljs-number\">16px</span>;<span class=\"hljs-attribute\">height</span>:<span class=\"hljs-number\">14px</span>;<span class=\"hljs-attribute\">background</span>:<span class=\"hljs-built_in\">url</span>(<span class=\"hljs-string\">http://7xtawy.com1.z0.glb.clouddn.com/show.png</span>) no-repeat;-webkit-<span class=\"hljs-attribute\">background-size</span>:<span class=\"hljs-number\">100%</span>;-moz-<span class=\"hljs-attribute\">background-size</span>:<span class=\"hljs-number\">100%</span>;<span class=\"hljs-attribute\">background-size</span>:<span class=\"hljs-number\">100%</span>&#125;<span class=\"hljs-selector-class\">.show-toc-btn</span> <span class=\"hljs-selector-class\">.btn-text</span>&#123;<span class=\"hljs-attribute\">color</span>:<span class=\"hljs-number\">#999</span>;<span class=\"hljs-attribute\">font-size</span>:<span class=\"hljs-number\">12px</span>&#125;<span class=\"hljs-selector-class\">.show-toc-btn</span><span class=\"hljs-selector-pseudo\">:hover</span>&#123;<span class=\"hljs-attribute\">cursor</span>:pointer&#125;<span class=\"hljs-selector-class\">.show-toc-btn</span><span class=\"hljs-selector-pseudo\">:hover</span> <span class=\"hljs-selector-class\">.btn-bg</span>&#123;<span class=\"hljs-attribute\">background-position</span>:<span class=\"hljs-number\">0</span> -<span class=\"hljs-number\">16px</span>&#125;<span class=\"hljs-selector-class\">.show-toc-btn</span><span class=\"hljs-selector-pseudo\">:hover</span> <span class=\"hljs-selector-class\">.btn-text</span>&#123;<span class=\"hljs-attribute\">font-size</span>:<span class=\"hljs-number\">12px</span>;<span class=\"hljs-attribute\">color</span>:<span class=\"hljs-number\">#ea8010</span>&#125;\n<span class=\"hljs-selector-class\">.toc-article</span> <span class=\"hljs-selector-tag\">li</span> <span class=\"hljs-selector-tag\">ol</span>, <span class=\"hljs-selector-class\">.toc-article</span> <span class=\"hljs-selector-tag\">li</span> <span class=\"hljs-selector-tag\">ul</span> &#123;\n    <span class=\"hljs-attribute\">margin-left</span>: <span class=\"hljs-number\">30px</span>;\n&#125;\n<span class=\"hljs-selector-class\">.toc-article</span> <span class=\"hljs-selector-tag\">ol</span>, <span class=\"hljs-selector-class\">.toc-article</span> <span class=\"hljs-selector-tag\">ul</span> &#123;\n    <span class=\"hljs-attribute\">margin</span>: <span class=\"hljs-number\">10px</span> <span class=\"hljs-number\">0</span>;\n&#125;</code></pre>\n<p>2.after <code>&lt;/header&gt;&lt;% &#125; %&gt;</code> in file <code>themes/yilia/layout/_partial/article.ejs</code> add</p>\n<pre><code class=\"hljs html\"><span class=\"hljs-comment\">&lt;!-- navigator --&gt;</span>\n&lt;% if (!index &amp;&amp; post.toc)&#123; %&gt;\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">p</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;show-toc-btn&quot;</span> <span class=\"hljs-attr\">id</span>=<span class=\"hljs-string\">&quot;show-toc-btn&quot;</span> <span class=\"hljs-attr\">onclick</span>=<span class=\"hljs-string\">&quot;showToc();&quot;</span> <span class=\"hljs-attr\">style</span>=<span class=\"hljs-string\">&quot;display:none&quot;</span>&gt;</span>\n        <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">span</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;btn-bg&quot;</span>&gt;</span><span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">span</span>&gt;</span>\n        <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">span</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;btn-text&quot;</span>&gt;</span>...<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">span</span>&gt;</span>\n        <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">p</span>&gt;</span>\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">id</span>=<span class=\"hljs-string\">&quot;toc-article&quot;</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;toc-article&quot;</span>&gt;</span>\n      <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">span</span> <span class=\"hljs-attr\">id</span>=<span class=\"hljs-string\">&quot;toc-close&quot;</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;toc-close&quot;</span> <span class=\"hljs-attr\">title</span>=<span class=\"hljs-string\">&quot;hide navigator&quot;</span> <span class=\"hljs-attr\">onclick</span>=<span class=\"hljs-string\">&quot;showBtn();&quot;</span>&gt;</span>×<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">span</span>&gt;</span>\n      <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">strong</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;toc-title&quot;</span>&gt;</span>navigator<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">strong</span>&gt;</span>\n        &lt;%- toc(post.content) %&gt;\n      <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">div</span>&gt;</span>\n<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">script</span> <span class=\"hljs-attr\">type</span>=<span class=\"hljs-string\">&quot;text/javascript&quot;</span>&gt;</span><span class=\"language-javascript\"></span>\n<span class=\"language-javascript\">  <span class=\"hljs-keyword\">function</span> <span class=\"hljs-title function_\">showToc</span>(<span class=\"hljs-params\"></span>)&#123;</span>\n<span class=\"language-javascript\">      <span class=\"hljs-keyword\">var</span> toc_article = <span class=\"hljs-variable language_\">document</span>.<span class=\"hljs-title function_\">getElementById</span>(<span class=\"hljs-string\">&quot;toc-article&quot;</span>);</span>\n<span class=\"language-javascript\">      <span class=\"hljs-keyword\">var</span> show_toc_btn = <span class=\"hljs-variable language_\">document</span>.<span class=\"hljs-title function_\">getElementById</span>(<span class=\"hljs-string\">&quot;show-toc-btn&quot;</span>);</span>\n<span class=\"language-javascript\">      toc_article.<span class=\"hljs-title function_\">setAttribute</span>(<span class=\"hljs-string\">&quot;style&quot;</span>,<span class=\"hljs-string\">&quot;display:block&quot;</span>);</span>\n<span class=\"language-javascript\">      show_toc_btn.<span class=\"hljs-title function_\">setAttribute</span>(<span class=\"hljs-string\">&quot;style&quot;</span>,<span class=\"hljs-string\">&quot;display:none&quot;</span>);</span>\n<span class=\"language-javascript\">      &#125;;</span>\n<span class=\"language-javascript\">  <span class=\"hljs-keyword\">function</span> <span class=\"hljs-title function_\">showBtn</span>(<span class=\"hljs-params\"></span>)&#123;</span>\n<span class=\"language-javascript\">      <span class=\"hljs-keyword\">var</span> toc_article = <span class=\"hljs-variable language_\">document</span>.<span class=\"hljs-title function_\">getElementById</span>(<span class=\"hljs-string\">&quot;toc-article&quot;</span>);</span>\n<span class=\"language-javascript\">      <span class=\"hljs-keyword\">var</span> show_toc_btn = <span class=\"hljs-variable language_\">document</span>.<span class=\"hljs-title function_\">getElementById</span>(<span class=\"hljs-string\">&quot;show-toc-btn&quot;</span>);</span>\n<span class=\"language-javascript\">      toc_article.<span class=\"hljs-title function_\">setAttribute</span>(<span class=\"hljs-string\">&quot;style&quot;</span>,<span class=\"hljs-string\">&quot;display:none&quot;</span>);</span>\n<span class=\"language-javascript\">      show_toc_btn.<span class=\"hljs-title function_\">setAttribute</span>(<span class=\"hljs-string\">&quot;style&quot;</span>,<span class=\"hljs-string\">&quot;display:block&quot;</span>);</span>\n<span class=\"language-javascript\">      &#125;;</span>\n<span class=\"language-javascript\"></span><span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">script</span>&gt;</span>\n    &lt;% &#125; %&gt;\n<span class=\"hljs-comment\">&lt;!-- navigator end --&gt;</span></code></pre>\n<p>3.add <code>toc:true</code> to the articles that need the navigator.</p>\n<h3 id=\"add-custormize-header-to-articles\">Add custormize header to articles</h3>\n<p>when run <code>hexo new</code> to initiate a new blog, a defaul head would generate, change it by</p>\n<p>change the <code>scaffolds/post.md</code> in the <code>root</code> directory</p>\n<pre><code class=\"hljs txt\">---\ntitle: &#123;&#123; title &#125;&#125;\ndate: &#123;&#123; date &#125;&#125;\nauthor: daydreamatnight\ntoc: true\ndeclare: true\ntags:\n---</code></pre>\n<h4 id=\"more-headers-to-choose-when-writing-a-blog\">more headers to choose when writing a blog</h4>\n<p>before a blog, more paras can be chosen to add</p>\n<pre><code class=\"hljs txt\">--- \ntitle: #你的博客文章名 \ntoc: ture #toc \ndate: 2020-09-07 09:25:00 #文章时间 \nauthor: GavenLee #作者 \nimg: /source/images/xxx.jpg #图片 \ntop: true #是否顶置 \ncover: true #是否在引导页轮播 \ncoverImg: /images/1.jpg #轮播图片 \npassword: #阅读密码这里被加密 \nmathjax: false #mathjax \nsummary: 这是你自定义的文章摘要内容，如果这个属性有值，文章卡片摘要就显示这段文字，否则程序会自动截取文章的部分内容作为摘要 \ncategories: Markdown #分类 \ntags: #标签 \nabbrlink: HexoLearn #链接 \n---</code></pre>\n<h3 id=\"disable-auto-wrap-in-code-block\">Disable auto wrap in code block</h3>\n<p>locate and delete <code>white-space:pre-wrap</code> in file <code>themes/yilia/source/main.0cf68a.css</code></p>\n<h3 id=\"add-copy-button-to-code-block\">Add copy button to code block</h3>\n<p>1.create a <code>clipboard_use.js</code> file in directory <code>themes/yilia/source</code></p>\n<pre><code class=\"hljs js\">$(<span class=\"hljs-string\">&quot;.highlight&quot;</span>).<span class=\"hljs-title function_\">wrap</span>(<span class=\"hljs-string\">&quot;&lt;div class=&#x27;code-wrapper&#x27; style=&#x27;position:relative&#x27;&gt;&lt;/div&gt;&quot;</span>);\n<span class=\"hljs-comment\">/*create copy button after page loaded*/</span>\n!<span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">e, t, a</span>) &#123;\n    <span class=\"hljs-comment\">/* code */</span>\n    <span class=\"hljs-keyword\">var</span> initCopyCode = <span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\"></span>) &#123;\n        <span class=\"hljs-keyword\">var</span> copyHtml = <span class=\"hljs-string\">&#x27;&#x27;</span>;\n        copyHtml += <span class=\"hljs-string\">&#x27;&lt;button class=&quot;btn-copy&quot; data-clipboard-snippet=&quot;&quot;&gt;&#x27;</span>;\n        copyHtml += <span class=\"hljs-string\">&#x27;  &lt;i class=&quot;fa fa-clipboard&quot;&gt;&lt;/i&gt;&lt;span&gt;copy&lt;/span&gt;&#x27;</span>;\n        copyHtml += <span class=\"hljs-string\">&#x27;&lt;/button&gt;&#x27;</span>;\n        $(<span class=\"hljs-string\">&quot;.highlight .code&quot;</span>).<span class=\"hljs-title function_\">before</span>(copyHtml);\n        <span class=\"hljs-keyword\">var</span> clipboard = <span class=\"hljs-keyword\">new</span> <span class=\"hljs-title class_\">ClipboardJS</span>(<span class=\"hljs-string\">&#x27;.btn-copy&#x27;</span>, &#123;\n            <span class=\"hljs-attr\">target</span>: <span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">trigger</span>) &#123;\n                <span class=\"hljs-keyword\">return</span> trigger.<span class=\"hljs-property\">nextElementSibling</span>;\n            &#125;\n        &#125;);\n        clipboard.<span class=\"hljs-title function_\">on</span>(<span class=\"hljs-string\">&#x27;success&#x27;</span>, <span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">e</span>) &#123;\n            e.<span class=\"hljs-property\">trigger</span>.<span class=\"hljs-property\">innerHTML</span> = <span class=\"hljs-string\">&quot;&lt;i class=&#x27;fa fa-check&#x27; style=&#x27;color:green&#x27;&gt;&lt;/i&gt;&lt;span style=&#x27;color:green&#x27;&gt;copy success&lt;/span&gt;&quot;</span>\n            <span class=\"hljs-built_in\">setTimeout</span>(<span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\"></span>) &#123;\n                e.<span class=\"hljs-property\">trigger</span>.<span class=\"hljs-property\">innerHTML</span> = <span class=\"hljs-string\">&quot;&lt;i class=&#x27;fa fa-clipboard&#x27;&gt;&lt;/i&gt;&lt;span&gt;copy&lt;/span&gt;&quot;</span>\n            &#125;, <span class=\"hljs-number\">1000</span>)\n            e.<span class=\"hljs-title function_\">clearSelection</span>();\n        &#125;);\n        clipboard.<span class=\"hljs-title function_\">on</span>(<span class=\"hljs-string\">&#x27;error&#x27;</span>, <span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">e</span>) &#123;\n            e.<span class=\"hljs-property\">trigger</span>.<span class=\"hljs-property\">innerHTML</span> = <span class=\"hljs-string\">&quot;&lt;i class=&#x27;fa fa-exclamation&#x27; style=&#x27;color:red&#x27;&gt;&lt;/i&gt;&lt;span style=&#x27;color:red&#x27;&gt;copy success&lt;/span&gt;&quot;</span>\n            <span class=\"hljs-built_in\">setTimeout</span>(<span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\"></span>) &#123;\n                e.<span class=\"hljs-property\">trigger</span>.<span class=\"hljs-property\">innerHTML</span> = <span class=\"hljs-string\">&quot;&lt;i class=&#x27;fa fa-clipboard&#x27;&gt;&lt;/i&gt;&lt;span&gt;copy&lt;/span&gt;&quot;</span>\n            &#125;, <span class=\"hljs-number\">1000</span>)\n            e.<span class=\"hljs-title function_\">clearSelection</span>();\n        &#125;);\n    &#125;\n    <span class=\"hljs-title function_\">initCopyCode</span>();\n&#125;(<span class=\"hljs-variable language_\">window</span>, <span class=\"hljs-variable language_\">document</span>);</code></pre>\n<p>2.load .js file, edit <code>themes/yilia/layout/layout.ejs</code> file, add before <code>&lt;/body&gt;</code>.</p>\n<pre><code class=\"hljs html\"><span class=\"hljs-comment\">&lt;!-- copy button in code block--&gt;</span>\n<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">script</span> <span class=\"hljs-attr\">type</span>=<span class=\"hljs-string\">&quot;text/javascript&quot;</span> <span class=\"hljs-attr\">src</span>=<span class=\"hljs-string\">&quot;https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.js&quot;</span>&gt;</span><span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">script</span>&gt;</span>\n<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">script</span> <span class=\"hljs-attr\">type</span>=<span class=\"hljs-string\">&quot;text/javascript&quot;</span> <span class=\"hljs-attr\">src</span>=<span class=\"hljs-string\">&quot;https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js&quot;</span>&gt;</span><span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">script</span>&gt;</span>\n<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">script</span> <span class=\"hljs-attr\">type</span>=<span class=\"hljs-string\">&quot;text/javascript&quot;</span> <span class=\"hljs-attr\">src</span>=<span class=\"hljs-string\">&quot;/clipboard_use.js&quot;</span>&gt;</span><span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">script</span>&gt;</span></code></pre>\n<p>3.add stylesheet the end of <code>themes/yilia/source/main.0cf68a.css</code></p>\n<pre><code class=\"hljs css\"><span class=\"hljs-comment\">/* code copy button */</span>\n<span class=\"hljs-selector-class\">.btn-copy</span> &#123;\n  <span class=\"hljs-attribute\">display</span>: inline-block;\n  <span class=\"hljs-attribute\">cursor</span>: pointer;\n  <span class=\"hljs-attribute\">background-color</span>: <span class=\"hljs-number\">#eee</span>;\n  <span class=\"hljs-attribute\">background-image</span>: <span class=\"hljs-built_in\">linear-gradient</span>(<span class=\"hljs-number\">#fcfcfc</span>, <span class=\"hljs-number\">#eee</span>);\n  <span class=\"hljs-attribute\">border</span>: <span class=\"hljs-number\">1px</span> solid <span class=\"hljs-number\">#d5d5d5</span>;\n  <span class=\"hljs-attribute\">border-radius</span>: <span class=\"hljs-number\">3px</span>;\n  -webkit-user-select: none;\n  -moz-user-select: none;\n  -ms-user-select: none;\n  user-select: none;\n  -webkit-appearance: none;\n  <span class=\"hljs-attribute\">font-size</span>: <span class=\"hljs-number\">13px</span>;\n  <span class=\"hljs-attribute\">font-weight</span>: <span class=\"hljs-number\">700</span>;\n  <span class=\"hljs-attribute\">line-height</span>: <span class=\"hljs-number\">20px</span>;\n  <span class=\"hljs-attribute\">color</span>: <span class=\"hljs-number\">#333</span>;\n  -webkit-<span class=\"hljs-attribute\">transition</span>: opacity .<span class=\"hljs-number\">3s</span> ease-in-out;\n  -o-<span class=\"hljs-attribute\">transition</span>: opacity .<span class=\"hljs-number\">3s</span> ease-in-out;\n  <span class=\"hljs-attribute\">transition</span>: opacity .<span class=\"hljs-number\">3s</span> ease-in-out;\n  <span class=\"hljs-attribute\">padding</span>: <span class=\"hljs-number\">2px</span> <span class=\"hljs-number\">6px</span>;\n  <span class=\"hljs-attribute\">position</span>: absolute;\n  <span class=\"hljs-attribute\">right</span>: <span class=\"hljs-number\">5px</span>;\n  <span class=\"hljs-attribute\">top</span>: <span class=\"hljs-number\">5px</span>;\n  <span class=\"hljs-attribute\">opacity</span>: <span class=\"hljs-number\">0</span>;\n&#125;\n<span class=\"hljs-selector-class\">.btn-copy</span> <span class=\"hljs-selector-tag\">span</span> &#123;\n  <span class=\"hljs-attribute\">margin-left</span>: <span class=\"hljs-number\">5px</span>;\n&#125;\n<span class=\"hljs-selector-class\">.highlight</span><span class=\"hljs-selector-pseudo\">:hover</span> <span class=\"hljs-selector-class\">.btn-copy</span> &#123;\n  <span class=\"hljs-attribute\">opacity</span>: <span class=\"hljs-number\">1</span>;\n&#125;\n<span class=\"hljs-comment\">/* code copy button end */</span></code></pre>\n<p>4.add copy button icon, edit <code>themes/yilia/layout/_partia/head.ejs</code> add before <code>&lt;/head&gt;</code></p>\n<pre><code class=\"hljs html\"><span class=\"hljs-comment\">&lt;!-- copy button icon --&gt;</span>\n<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">link</span> <span class=\"hljs-attr\">rel</span>=<span class=\"hljs-string\">&quot;stylesheet&quot;</span> <span class=\"hljs-attr\">type</span>=<span class=\"hljs-string\">&quot;text/css&quot;</span> <span class=\"hljs-attr\">href</span>=<span class=\"hljs-string\">&quot;//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css&quot;</span>&gt;</span></code></pre>\n<h3 id=\"allow-search-engines-to-index-this-blog\">Allow search engines to index this Blog</h3>\n<h4 id=\"index-google-to-this-blog\">index Google to this Blog</h4>\n<p>check if google can find you, enter <code>site:daydreamatnight.github.io</code> to see</p>\n<p><img src=\"check google search.png\" alt=\"check google search\" style=\"zoom:50%;\" /></p>\n<h5 id=\"add-url-to-goole-search-console\">Add url to goole search console</h5>\n<p>1.open <a href=\"https://search.google.com/search-console/welcome\">google console</a> , add URL link of the blog (https://daydreamatnight.github.io), in the <code>URL prefix</code> block, click <code>CONTINUE</code></p>\n<p><img src=\"google search console.png\" alt=\"google console\" style=\"zoom:50%;\" /></p>\n<p>2.upload the html file to the blog <code>root</code> directory and deploy the website, then clicke verify.</p>\n<p><img src=\"google console varification.png\" alt=\"google console varification\" style=\"zoom:50%;\" /></p>\n<p>little buggy here, see <a href=\"https://hilyy.xyz/how-to-allow-google-to-index-your-hexo-blog-website-google-search-console-verification-methods/\"><strong>don’t upload</strong> the file <strong>using hexo</strong> command</a></p>\n<h5 id=\"add-sitemap-for-google\">add sitemap for google</h5>\n<p>add sitemap for google and baidu together</p>\n<blockquote>\n<p>A <em>sitemap</em> is a file where you provide information about the pages, videos, and other files on your site, and the relationships between them. Search engines like Google read this file to more intelligently crawl your site. A sitemap tells Google which pages and files you think are important in your site, and also provides valuable information about these files: for example, for pages, when the page was last updated, how often the page is changed, and any alternate language versions of a page.</p>\n</blockquote>\n<p>1.install sitemap plugins</p>\n<pre><code class=\"hljs shell\"><span class=\"hljs-meta\">$ </span><span class=\"language-bash\">npm install hexo-generator-sitemap --save</span>\n<span class=\"hljs-meta\">$ </span><span class=\"language-bash\">npm install hexo-generator-baidu-sitemap --save</span></code></pre>\n<p>2.add to the <code>_config.yml</code> in the blog <code>root</code></p>\n<pre><code class=\"hljs yml\"><span class=\"hljs-comment\"># hexo sitemap</span>\n<span class=\"hljs-attr\">sitemap:</span>\n  <span class=\"hljs-attr\">path:</span> <span class=\"hljs-string\">sitemap.xml</span>\n<span class=\"hljs-attr\">baidusitemap:</span>\n  <span class=\"hljs-attr\">path:</span> <span class=\"hljs-string\">baidusitemap.xml</span></code></pre>\n<p>3.Deploy the blog, go to https://daydreamatnight.github.io/sitemap.xml and https://daydreamatnight.github.io/baidusitemap.xml to see if sitemaps are uploaded</p>\n<p>4.Go to Google Search Console, in the left panel, click <code>Sitemaps</code>, enter your sitemap URL <code>sitemap.xml</code></p>\n<p><img src=\"can't fetch sitemap.png\" alt=\"can't fetch sitemap\" style=\"zoom:50%;\" /></p>\n<p>Googlebot won't download the sitemap immediately. Give it time.</p>\n<h5 id=\"add-robots.txt\">add robots.txt</h5>\n<blockquote>\n<p>A robots.txt file tells search engine crawlers which URLs the crawler can access on your site. This is used mainly to avoid overloading your site with requests; <strong>it is not a mechanism for keeping a web page out of Google</strong>. To keep a web page out of Google, <a href=\"https://developers.google.com/search/docs/advanced/crawling/block-indexing\">block indexing with <code>noindex</code></a> or password-protect the page.</p>\n<p>A robots.txt file is used primarily to manage crawler traffic to your site, and <em>usually</em> to keep a file off Google, depending on the file type:</p>\n</blockquote>\n<pre><code class=\"hljs txt\">User-agent: *\nAllow: /\nAllow: /archives/\nAllow: /tags/\nAllow: /categories/\nAllow: /about/\nAllow: /guestbook/\nAllow: /others/\n\n\nDisallow: /js/\nDisallow: /css/\nDisallow: /lib/\n\nSitemap: https://daydreamatnight.github.io/sitemap.xml\nSitemap: https://daydreamatnight.github.io/baidusitemap.xml</code></pre>\n<p>deploy the blog and wait.</p>\n<h5 id=\"check-if-sitemap-is-available\">check if sitemap is available</h5>\n<p>After uploaded several updates, my sitemap still didn't fetched by google. So I went to check, it turns out my url setting in <code>_config.yml</code> is wrong. So I changed it to be my home url. And check it with <a href=\"https://www.jcchouinard.com/url-inspection-tool/\">URL Inspection Tool</a>.</p>\n<p>1.Open <a href=\"https://search.google.com/search-console\">google search console</a>, add the url of sitemap in the upper url inspecting box.</p>\n<p><img src=\"Google%20sitemap%20inspect.png\" alt=\"Google sitemap inspect URL is not on Google \" style=\"zoom:22%;\" /></p>\n<p><img src=\"Google%20sitemap%20inspect%202.png\" alt=\"Google sitemap inspect 2\" style=\"zoom:22%;\" /></p>\n<p>It's normal it shows <code>URL is not on Google</code> because it shouldn't as a sitemap.</p>\n<p>2.click <code>live test</code> to check the availability.</p>\n<p><img src=\"Google%20sitemap%20inspect%203.png\" alt=\"Google sitemap inspect 3\" style=\"zoom:75%;\" /></p>\n<p>It should be available, then just wait.</p>\n<h4 id=\"index-bing-to-this-blog\">index Bing to this Blog</h4>\n<p>1.go to <a href=\"https://www.bing.com/webmasters/\">Bing webmaster</a> and login</p>\n<p>2.connect with google webmaster.</p>\n<p><img src=\"bing%20sitemap.png\" alt=\"bing sitemap\" style=\"zoom:75%;\" /></p>\n<p><img src=\"being%20sitemap%20connect%20google.png\" alt=\"being sitemap connect google\" style=\"zoom:75%;\" /></p>\n<p><img src=\"bing%20search%20console%20success.png\" alt=\"bing search console success\" style=\"zoom:75%;\" /></p>\n<h4 id=\"index-baidu-to-this-blognot-possibly-working\">index baidu to this Blog(not possibly working)</h4>\n<p>go to the <a href=\"https://ziyuan.baidu.com/site/index\">baidu search console</a> ,</p>\n<p><img src=\"baidu console.png\" alt=\"baidu console\" style=\"zoom:50%;\" /></p>\n<p>Click <code>添加网站</code> and input every thing, do similar thing</p>\n<p><img src=\"baidu console varification.png\" alt=\"baidu console varification\" style=\"zoom:50%;\" /></p>\n<p>add sitemap</p>\n<p><img src=\"baidu console sitemap.png\" alt=\"baidu console sitemap\" style=\"zoom:50%;\" /></p>\n<p>just wait forever, this could take 2000 years, so give up</p>\n<h3 id=\"add-copyright-statement\">Add copyright statement</h3>\n<p>1.open file <code>themes/yilia/layout/_partial/article.ejs</code> add before <code>&lt;% if ((theme.reward_type === 2 || (theme.reward_type === 1 &amp;&amp; post.reward)) &amp;&amp; !index)&#123; %&gt;</code></p>\n<pre><code class=\"hljs html\"><span class=\"hljs-comment\">&lt;!-- add copyright statement --&gt;</span>\n&lt;% if(theme.declare)&#123;%&gt;\n    &lt;%- partial(&#x27;post/declare&#x27;) %&gt;\n&lt;% &#125; %&gt;\n<span class=\"hljs-comment\">&lt;!-- end --&gt;</span></code></pre>\n<p>2.create new file <code>declare.ejs</code> under <code>themes/yilia/layout/_partial/post/</code> with:</p>\n<pre><code class=\"hljs html\"><span class=\"hljs-comment\">&lt;!--add copyright statement https://github.com/JoeyBling/hexo-theme-yilia-plus/commit/c1215e132f6d5621c5fea83d3c4f7ccbcca074a3--&gt;</span>\n&lt;%\n  var sUrl = url.replace(/index\\.html$/, &#x27;&#x27;);\n  sUrl = /^(http:|https:)\\/\\//.test(sUrl) ? sUrl : &#x27;https:&#x27; + sUrl;\n%&gt;\n\n<span class=\"hljs-comment\">&lt;!-- #copyright setting：0-close statement; 1-declare statement if declare: true in the article header; 2-always declare the copyright --&gt;</span>\n&lt;% if ((theme.declare.declare_type === 2 || (theme.declare.declare_type === 1 &amp;&amp; post.declare)) &amp;&amp; !index)&#123; %&gt;\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;declare&quot;</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">strong</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;author&quot;</span>&gt;</span>author: <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">strong</span>&gt;</span>\n    &lt;% if(config.author != undefined)&#123; %&gt;\n      &lt;%= config.author%&gt;\n    &lt;% &#125;else&#123;%&gt;\n      <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">font</span> <span class=\"hljs-attr\">color</span>=<span class=\"hljs-string\">&quot;red&quot;</span>&gt;</span>please add right &quot;author&quot; name in &quot;_config.yml&quot; in the blog root<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">font</span>&gt;</span>\n    &lt;%&#125;%&gt;\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">br</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">strong</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;create-time&quot;</span>&gt;</span>posting date: <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">strong</span>&gt;</span>\n    &lt;%- date(post.date, &#x27;YYYY-MM-DD HH:MM:SS&#x27;) %&gt;\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">br</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">strong</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;update-time&quot;</span>&gt;</span>last update: <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">strong</span>&gt;</span>\n    &lt;%- date(post.updated, &#x27;YYYY-MM-DD HH:MM:SS&#x27;) %&gt;\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">br</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">strong</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;article-titles&quot;</span>&gt;</span>article title: <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">strong</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">a</span> <span class=\"hljs-attr\">href</span>=<span class=\"hljs-string\">&quot;&lt;%= config.url %&gt;/&lt;%= post.path %&gt;&quot;</span> <span class=\"hljs-attr\">title</span>=<span class=\"hljs-string\">&quot;&lt;%= post.title %&gt;&quot;</span> <span class=\"hljs-attr\">target</span>=<span class=\"hljs-string\">&quot;_blank&quot;</span>&gt;</span>&lt;%= post.title %&gt;<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">a</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">br</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">strong</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;article-url&quot;</span>&gt;</span>article link: <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">strong</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">a</span> <span class=\"hljs-attr\">href</span>=<span class=\"hljs-string\">&quot;&lt;%= config.url %&gt;/&lt;%= post.path %&gt;&quot;</span> <span class=\"hljs-attr\">title</span>=<span class=\"hljs-string\">&quot;&lt;%= post.title %&gt;&quot;</span> <span class=\"hljs-attr\">target</span>=<span class=\"hljs-string\">&quot;_blank&quot;</span>&gt;</span>&lt;%= config.url %&gt;/&lt;%= post.path %&gt;<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">a</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">br</span>&gt;</span>\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">strong</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;copyright&quot;</span>&gt;</span>copyright:<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">strong</span>&gt;</span>\n    This work is licensed under a\n    <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">a</span> <span class=\"hljs-attr\">rel</span>=<span class=\"hljs-string\">&quot;license&quot;</span> <span class=\"hljs-attr\">href</span>=<span class=\"hljs-string\">&quot;&lt;%= theme.declare.licensee_url%&gt;&quot;</span> <span class=\"hljs-attr\">title</span>=<span class=\"hljs-string\">&quot;&lt;%= theme.declare.licensee_alias %&gt;&quot;</span>&gt;</span>&lt;%= theme.declare.licensee_name%&gt;<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">a</span>&gt;</span>\n    licience \n    &lt;% if(theme.declare.licensee_img != undefined)&#123; %&gt;\n      <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">a</span> <span class=\"hljs-attr\">rel</span>=<span class=\"hljs-string\">&quot;license&quot;</span> <span class=\"hljs-attr\">href</span>=<span class=\"hljs-string\">&quot;&lt;%= theme.declare.licensee_url%&gt;&quot;</span>&gt;</span><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">img</span> <span class=\"hljs-attr\">alt</span>=<span class=\"hljs-string\">&quot;知识共享许可协议&quot;</span> <span class=\"hljs-attr\">style</span>=<span class=\"hljs-string\">&quot;border-width:0&quot;</span> <span class=\"hljs-attr\">src</span>=<span class=\"hljs-string\">&quot;&lt;%= theme.declare.licensee_img%&gt;&quot;</span>/&gt;</span><span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">a</span>&gt;</span>\n    &lt;% &#125; %&gt;\n  <span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">div</span>&gt;</span>\n&lt;% &#125; else &#123;%&gt;\n  <span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">&quot;declare&quot;</span> <span class=\"hljs-attr\">hidden</span>=<span class=\"hljs-string\">&quot;hidden&quot;</span>&gt;</span><span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">div</span>&gt;</span>\n&lt;% &#125; %&gt;\n<span class=\"hljs-comment\">&lt;!-- add copyright statement --&gt;</span></code></pre>\n<p>3.add stylesheet the end of <code>themes/yilia/source/main.0cf68a.css</code></p>\n<pre><code class=\"hljs css\"><span class=\"hljs-comment\">/*stylesheet for the delcare*/</span>\n<span class=\"hljs-selector-class\">.declare</span> &#123;\n  <span class=\"hljs-attribute\">background-color</span>: <span class=\"hljs-number\">#eaeaea</span>;\n  <span class=\"hljs-attribute\">margin-top</span>: <span class=\"hljs-number\">2em</span>;\n  <span class=\"hljs-attribute\">border-left</span>: <span class=\"hljs-number\">3px</span> solid <span class=\"hljs-number\">#ff1700</span>;\n  <span class=\"hljs-attribute\">padding</span>: .<span class=\"hljs-number\">5em</span> <span class=\"hljs-number\">1em</span>; \n&#125;\n<span class=\"hljs-comment\">/*stylesheet for the delcare end*/</span></code></pre>\n<p>4.add at the end of <code>themes/yilia/_config.yml</code> file:</p>\n<pre><code class=\"hljs awk\">declare:\n  declare_type: <span class=\"hljs-number\">1</span>\n  licensee_url: http:<span class=\"hljs-regexp\">//</span>creativecommons.org<span class=\"hljs-regexp\">/licenses/</span>by-nc-sa<span class=\"hljs-regexp\">/4.0/</span>      \n  licensee_name: <span class=\"hljs-string\">&#x27;CC BY-NC-SA 4.0&#x27;</span>                              \n  licensee_alias: <span class=\"hljs-string\">&#x27;CC BY-NC-SA 4.0&#x27;</span>     \n  licensee_img: https:<span class=\"hljs-regexp\">//i</span>.creativecommons.org<span class=\"hljs-regexp\">/l/</span>by-nc-sa<span class=\"hljs-regexp\">/4.0/</span><span class=\"hljs-number\">80</span>x15.png</code></pre>\n<h3 id=\"add-mind-map-support\">Add mind-map support</h3>\n<pre><code class=\"hljs shell\">npm install hexo-markmap</code></pre>\n<p>Detailed in its <a href=\"https://github.com/MaxChang3/hexo-markmap\">Github</a></p>\n<p>Example:</p>\n<pre><code class=\"hljs markdown\">&#123;% markmap 300px %&#125;\n<span class=\"hljs-bullet\">-</span> Testa\n<span class=\"hljs-bullet\">  -</span> test1\n<span class=\"hljs-bullet\">  -</span> test2\n<span class=\"hljs-bullet\">-</span> Testb\n<span class=\"hljs-bullet\">  -</span> test1\n<span class=\"hljs-bullet\">  -</span> test2\n&#123;%endmarkmap%&#125;</code></pre>\n<h3 id=\"add-latex-math-support\">Add Latex math support</h3>\n<p>Change the renderer to the more powerful pandoc:</p>\n<p>1.Install pandoc on macOS:</p>\n<pre><code class=\"hljs cmake\">copybrew <span class=\"hljs-keyword\">install</span> pandoc</code></pre>\n<p>2.in the blog root directory uninstall the default renderer then install the pandoc renderer:</p>\n<pre><code class=\"hljs ada\">copynpm uninstall hexo-renderer-marked <span class=\"hljs-comment\">--save</span>\nnpm install hexo-renderer-pandoc <span class=\"hljs-comment\">--save</span></code></pre>\n<p>3.install the hexo math plugin</p>\n<pre><code class=\"hljs cmake\">copynpm <span class=\"hljs-keyword\">install</span> hexo-<span class=\"hljs-keyword\">math</span> --save</code></pre>\n<p>4.add these lines to the hexo <code>_config</code> file</p>\n<pre><code class=\"hljs nestedtext\"><span class=\"hljs-attribute\">copymarkdown</span><span class=\"hljs-punctuation\">:</span>\n  <span class=\"hljs-attribute\">plugins</span><span class=\"hljs-punctuation\">:</span>\n    <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">markdown-it-footnote</span>\n    <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">markdown-it-sup</span>\n    <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">markdown-it-sub</span>\n    <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">markdown-it-abbr</span>\n    <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">markdown-it-emoji</span>\n    <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">hexo-math</span></code></pre>\n<p>5.add these lines to the theme <code>_config</code> file</p>\n<pre><code class=\"hljs yaml\"><span class=\"hljs-string\">copy#</span> <span class=\"hljs-string\">MathJax</span> <span class=\"hljs-string\">Support</span>\n<span class=\"hljs-attr\">mathjax:</span>\n  <span class=\"hljs-attr\">enable:</span> <span class=\"hljs-literal\">true</span>\n  <span class=\"hljs-attr\">per_page:</span> <span class=\"hljs-literal\">true</span></code></pre>\n<p>6.rebuild the to blog see changes</p>\n<p>7.Examples: <span class=\"math inline\">\\(this_{is}an\\frac{inline}{equation}\\)</span> <span class=\"math display\">\\[\n\\begin{equation}\n    \\mathbf{K}_\\mathbf{1}=\\frac{1}{\\Delta r}\\ \\left[\\begin{matrix}\\begin{matrix}-1&amp;1\\\\-1&amp;1\\\\\\end{matrix}&amp;\\ &amp;\\ \\\\\\begin{matrix}\\ &amp;\\ddots\\\\\\end{matrix}&amp;\\begin{matrix}\\ddots&amp;\\ \\\\\\end{matrix}&amp;\\ \\\\\\ &amp;-1\\ &amp;1\\\\\\end{matrix}\\right],\\ \\ {\\ \\mathbf{K}}_\\mathbf{2}=\\frac{1}{\\Delta r}\\ \\left[\\begin{matrix}\\begin{matrix}-1&amp;1\\\\\\ &amp;\\ddots\\\\\\end{matrix}&amp;\\begin{matrix}\\\\\\ddots\\\\\\end{matrix}&amp;\\ \\\\\\begin{matrix}\\ &amp;\\ \\\\\\end{matrix}&amp;-1&amp;1\\ \\\\\\ &amp;-1\\ &amp;1\\\\\\end{matrix}\\right]\n    \\label{K2}\n\\end{equation}\n\\]</span></p>\n<h3 id=\"the-last-snapshot\">The last snapshot</h3>\n<p>Ok, never spend time on a no-longer maintained project. Here's the last figure of it.</p>\n<p><img src=\"Last snapshot.png\" alt=\"Last snapshot\" style=\"zoom:80%;\" /></p>\n<h2 id=\"reference\">Reference</h2>\n<p>https://flatironschool.com/blog/the-benefits-of-blogging-how-and-why-to-keep-a-technical-blog/</p>\n<p>https://weblog.masukomi.org/2015/10/18/static-vs-dynamic-blogging/</p>\n<p>https://www.cnblogs.com/aoguai/p/11781505.html</p>\n<p>https://www.kblog.top/post/30452.html</p>\n<p>https://wkzqn.gitee.io/2020/02/16/typora%E7%BC%96%E5%86%99hexo%E5%8D%9A%E5%AE%A2%E6%97%B6%E7%9A%84%E5%9B%BE%E7%89%87%E6%98%BE%E7%A4%BA/</p>\n<p>https://segmentfault.com/a/1190000009478837#articleHeader5</p>\n<p>https://hilyy.xyz/how-to-allow-google-to-index-your-hexo-blog-website-google-search-console-verification-methods/</p>\n<p>https://busuanzi.ibruce.info/</p>\n<p>https://creativecommons.org/choose/results-one?license_code=by-nc-sa&amp;jurisdiction=&amp;version=4.0&amp;lang=en</p>\n<p>https://www.jcchouinard.com/sitemap-could-not-be-read-couldnt-fetch-in-google-search-console/</p>\n<p>https://cqh-i.github.io/2019/08/07/hexo-yilia%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E9%9A%90%E8%97%8F%E5%B7%A6%E8%BE%B9%E6%A0%8F%E7%9B%AE%E6%8C%89%E9%92%AE/</p>","wordcount":29761},{"title":"paper reading: A gentle introduction to graph neural networks","author":"Ryan LI","declare":true,"date":"2022-04-14T05:58:38.000Z","_content":"> This is a tech blog written by google research team in 2021 that introducing the graph neural network. GNN has gradually become popular in the last 4 years. Personally, I think the graph structure looks similar to the CFD mesh, and there are works  focusing on simulating physics via GNN.\n\n> This is a [series of paper reading notes](https://daydreamatnight.github.io/2022/04/02/paper-reading-start/), hopefully, to push me to read paper casually and to leave some record of what I've learned.\n\n<!-- more -->\n\nPaper link: [A gentle introduction to graph neural networks](https://staging.distill.pub/2021/gnn-intro/?ref=https://githubhelp.com)\n\nUseful link: https://www.bilibili.com/video/BV1iT4y1d7zP\n\n## Notes\n\n*Because this blog has introduced GNN in detail and explained well with the interactive diagrams, it almost leaves me no need for extra notes. As a result, a throughout reading of the [original blog](https://staging.distill.pub/2021/gnn-intro/?ref=https://githubhelp.com) is recommended. And as a result this notes are only in pieces.*\n\n<img src=\" GNN interative archtecture.png\" alt=\"GNN interative archtecture\" style=\"zoom:50%;\" />\n\n>### Graph-level task\n>\n>In a graph-level task, our goal is to predict the property of an entire graph. For example, for a molecule represented as a graph, we might want to predict what the molecule smells like, or whether it will bind to a receptor implicated in a disease.\n>\n><img src=\" GNN graph level task.png\" alt=\"GNN graph level task\" style=\"zoom:50%;\" />\n\nThe example is actually a simple task example, and the loops can be detected with ordinary algorithm such as: [Fast and Slow Pointer: Floyd's Cycle Detection Algorithm](https://codeburst.io/fast-and-slow-pointer-floyds-cycle-detection-algorithm-9c7a8693f491). But with more complicated task, GNN can be useful. And here is the related Leetcode question: [No.141: Linked List Cycle](https://leetcode.com/problems/linked-list-cycle/description/).\n\n\n\n> <img src=\" GNN graph message passing.png\" alt=\"GNN graph message passing\" style=\"zoom:50%;\" />\n>\n> This is **reminiscent of standard convolution**: in essence, message passing and convolution are operations to aggregate and process the information of an element’s neighbors in order to update the element’s value. In graphs, the element is a node, and in images, the element is a pixel. **However**, the number of neighboring nodes in a graph can be variable, unlike in an image where each pixel has a set number of neighboring elements.\n\nWhen introducing the message passing method, this blog makes an analogy with the convolution. And in addition to the however part mentioned in this blog, another difference is that in convolution, each element's value is weighted differently while in the aggregation method they are all the same. \n\nInterestingly, similar weights can be achieved with Graph Attention Networks, which is introduced in later section.\n\n> Another way of communicating information between graph attributes is via attention. For example, when we consider the sum-aggregation of a node and its 1-degree neighboring nodes we could also consider using a weighted sum.\n>\n> A common scoring function is the inner product and nodes are often transformed before scoring into query and key vectors via a linear map to increase the expressivity of the scoring mechanism.\n\n## Reviews\n\nWriting: the whole article is well coherent and fluent, building the knowledge of GNN step by step, from a highly simplified model to the real GNN. The beautiful interactive figures make the article easy to read and digestible. Yet lacking mathematics and codes is both pros and cons. Unfortunately, *tarting today Distill will be taking a one year hiatus, which may be extended indefinitely.* \n\nGraph neural network: a graph is a powerful tool so that all kinds of data can be described as a graph. But this power leads to a huge difficulty in optimisation. One reason is the sparsity, the dynamic structure makes it difficult the train on CPU or GPU. Another is that GNN is very sensitive to hyper-parameters, just like the experiment section of this blog shows. As such, it is an active research area yet rarely deployed in industry.\n","source":"_posts/paper-reading-A-gentle-introduction-to-graph-neural-networks.md","raw":"---\ntitle: 'paper reading: A gentle introduction to graph neural networks'\nauthor: Ryan LI\ndeclare: true\ndate: 2022-04-14 13:58:38\ntags:\n  - paper reading\n  - deep learning\n---\n> This is a tech blog written by google research team in 2021 that introducing the graph neural network. GNN has gradually become popular in the last 4 years. Personally, I think the graph structure looks similar to the CFD mesh, and there are works  focusing on simulating physics via GNN.\n\n> This is a [series of paper reading notes](https://daydreamatnight.github.io/2022/04/02/paper-reading-start/), hopefully, to push me to read paper casually and to leave some record of what I've learned.\n\n<!-- more -->\n\nPaper link: [A gentle introduction to graph neural networks](https://staging.distill.pub/2021/gnn-intro/?ref=https://githubhelp.com)\n\nUseful link: https://www.bilibili.com/video/BV1iT4y1d7zP\n\n## Notes\n\n*Because this blog has introduced GNN in detail and explained well with the interactive diagrams, it almost leaves me no need for extra notes. As a result, a throughout reading of the [original blog](https://staging.distill.pub/2021/gnn-intro/?ref=https://githubhelp.com) is recommended. And as a result this notes are only in pieces.*\n\n<img src=\" GNN interative archtecture.png\" alt=\"GNN interative archtecture\" style=\"zoom:50%;\" />\n\n>### Graph-level task\n>\n>In a graph-level task, our goal is to predict the property of an entire graph. For example, for a molecule represented as a graph, we might want to predict what the molecule smells like, or whether it will bind to a receptor implicated in a disease.\n>\n><img src=\" GNN graph level task.png\" alt=\"GNN graph level task\" style=\"zoom:50%;\" />\n\nThe example is actually a simple task example, and the loops can be detected with ordinary algorithm such as: [Fast and Slow Pointer: Floyd's Cycle Detection Algorithm](https://codeburst.io/fast-and-slow-pointer-floyds-cycle-detection-algorithm-9c7a8693f491). But with more complicated task, GNN can be useful. And here is the related Leetcode question: [No.141: Linked List Cycle](https://leetcode.com/problems/linked-list-cycle/description/).\n\n\n\n> <img src=\" GNN graph message passing.png\" alt=\"GNN graph message passing\" style=\"zoom:50%;\" />\n>\n> This is **reminiscent of standard convolution**: in essence, message passing and convolution are operations to aggregate and process the information of an element’s neighbors in order to update the element’s value. In graphs, the element is a node, and in images, the element is a pixel. **However**, the number of neighboring nodes in a graph can be variable, unlike in an image where each pixel has a set number of neighboring elements.\n\nWhen introducing the message passing method, this blog makes an analogy with the convolution. And in addition to the however part mentioned in this blog, another difference is that in convolution, each element's value is weighted differently while in the aggregation method they are all the same. \n\nInterestingly, similar weights can be achieved with Graph Attention Networks, which is introduced in later section.\n\n> Another way of communicating information between graph attributes is via attention. For example, when we consider the sum-aggregation of a node and its 1-degree neighboring nodes we could also consider using a weighted sum.\n>\n> A common scoring function is the inner product and nodes are often transformed before scoring into query and key vectors via a linear map to increase the expressivity of the scoring mechanism.\n\n## Reviews\n\nWriting: the whole article is well coherent and fluent, building the knowledge of GNN step by step, from a highly simplified model to the real GNN. The beautiful interactive figures make the article easy to read and digestible. Yet lacking mathematics and codes is both pros and cons. Unfortunately, *tarting today Distill will be taking a one year hiatus, which may be extended indefinitely.* \n\nGraph neural network: a graph is a powerful tool so that all kinds of data can be described as a graph. But this power leads to a huge difficulty in optimisation. One reason is the sparsity, the dynamic structure makes it difficult the train on CPU or GPU. Another is that GNN is very sensitive to hyper-parameters, just like the experiment section of this blog shows. As such, it is an active research area yet rarely deployed in industry.\n","slug":"paper-reading-A-gentle-introduction-to-graph-neural-networks","published":1,"updated":"2022-04-30T19:30:59.010Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl2n5f4080007p9ybehc1eock","content":"<blockquote>\n<p>This is a tech blog written by google research team in 2021 that introducing the graph neural network. GNN has gradually become popular in the last 4 years. Personally, I think the graph structure looks similar to the CFD mesh, and there are works focusing on simulating physics via GNN.</p>\n</blockquote>\n<blockquote>\n<p>This is a <a href=\"https://daydreamatnight.github.io/2022/04/02/paper-reading-start/\">series of paper reading notes</a>, hopefully, to push me to read paper casually and to leave some record of what I've learned.</p>\n</blockquote>\n<span id=\"more\"></span>\n<p>Paper link: <a href=\"https://staging.distill.pub/2021/gnn-intro/?ref=https://githubhelp.com\">A gentle introduction to graph neural networks</a></p>\n<p>Useful link: https://www.bilibili.com/video/BV1iT4y1d7zP</p>\n<h2 id=\"notes\">Notes</h2>\n<p><em>Because this blog has introduced GNN in detail and explained well with the interactive diagrams, it almost leaves me no need for extra notes. As a result, a throughout reading of the <a href=\"https://staging.distill.pub/2021/gnn-intro/?ref=https://githubhelp.com\">original blog</a> is recommended. And as a result this notes are only in pieces.</em></p>\n<p><img src=\" GNN interative archtecture.png\" srcset=\"/img/loading.gif\" lazyload alt=\"GNN interative archtecture\" style=\"zoom:50%;\" /></p>\n<blockquote>\n<h3 id=\"graph-level-task\">Graph-level task</h3>\n<p>In a graph-level task, our goal is to predict the property of an entire graph. For example, for a molecule represented as a graph, we might want to predict what the molecule smells like, or whether it will bind to a receptor implicated in a disease.</p>\n<p><img src=\" GNN graph level task.png\" srcset=\"/img/loading.gif\" lazyload alt=\"GNN graph level task\" style=\"zoom:50%;\" /></p>\n</blockquote>\n<p>The example is actually a simple task example, and the loops can be detected with ordinary algorithm such as: <a href=\"https://codeburst.io/fast-and-slow-pointer-floyds-cycle-detection-algorithm-9c7a8693f491\">Fast and Slow Pointer: Floyd's Cycle Detection Algorithm</a>. But with more complicated task, GNN can be useful. And here is the related Leetcode question: <a href=\"https://leetcode.com/problems/linked-list-cycle/description/\">No.141: Linked List Cycle</a>.</p>\n<blockquote>\n<p><img src=\" GNN graph message passing.png\" srcset=\"/img/loading.gif\" lazyload alt=\"GNN graph message passing\" style=\"zoom:50%;\" /></p>\n<p>This is <strong>reminiscent of standard convolution</strong>: in essence, message passing and convolution are operations to aggregate and process the information of an element’s neighbors in order to update the element’s value. In graphs, the element is a node, and in images, the element is a pixel. <strong>However</strong>, the number of neighboring nodes in a graph can be variable, unlike in an image where each pixel has a set number of neighboring elements.</p>\n</blockquote>\n<p>When introducing the message passing method, this blog makes an analogy with the convolution. And in addition to the however part mentioned in this blog, another difference is that in convolution, each element's value is weighted differently while in the aggregation method they are all the same.</p>\n<p>Interestingly, similar weights can be achieved with Graph Attention Networks, which is introduced in later section.</p>\n<blockquote>\n<p>Another way of communicating information between graph attributes is via attention. For example, when we consider the sum-aggregation of a node and its 1-degree neighboring nodes we could also consider using a weighted sum.</p>\n<p>A common scoring function is the inner product and nodes are often transformed before scoring into query and key vectors via a linear map to increase the expressivity of the scoring mechanism.</p>\n</blockquote>\n<h2 id=\"reviews\">Reviews</h2>\n<p>Writing: the whole article is well coherent and fluent, building the knowledge of GNN step by step, from a highly simplified model to the real GNN. The beautiful interactive figures make the article easy to read and digestible. Yet lacking mathematics and codes is both pros and cons. Unfortunately, <em>tarting today Distill will be taking a one year hiatus, which may be extended indefinitely.</em></p>\n<p>Graph neural network: a graph is a powerful tool so that all kinds of data can be described as a graph. But this power leads to a huge difficulty in optimisation. One reason is the sparsity, the dynamic structure makes it difficult the train on CPU or GPU. Another is that GNN is very sensitive to hyper-parameters, just like the experiment section of this blog shows. As such, it is an active research area yet rarely deployed in industry.</p>\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>This is a tech blog written by google research team in 2021 that introducing the graph neural network. GNN has gradually become popular in the last 4 years. Personally, I think the graph structure looks similar to the CFD mesh, and there are works focusing on simulating physics via GNN.</p>\n</blockquote>\n<blockquote>\n<p>This is a <a href=\"https://daydreamatnight.github.io/2022/04/02/paper-reading-start/\">series of paper reading notes</a>, hopefully, to push me to read paper casually and to leave some record of what I've learned.</p>\n</blockquote>","more":"<p>Paper link: <a href=\"https://staging.distill.pub/2021/gnn-intro/?ref=https://githubhelp.com\">A gentle introduction to graph neural networks</a></p>\n<p>Useful link: https://www.bilibili.com/video/BV1iT4y1d7zP</p>\n<h2 id=\"notes\">Notes</h2>\n<p><em>Because this blog has introduced GNN in detail and explained well with the interactive diagrams, it almost leaves me no need for extra notes. As a result, a throughout reading of the <a href=\"https://staging.distill.pub/2021/gnn-intro/?ref=https://githubhelp.com\">original blog</a> is recommended. And as a result this notes are only in pieces.</em></p>\n<p><img src=\" GNN interative archtecture.png\" alt=\"GNN interative archtecture\" style=\"zoom:50%;\" /></p>\n<blockquote>\n<h3 id=\"graph-level-task\">Graph-level task</h3>\n<p>In a graph-level task, our goal is to predict the property of an entire graph. For example, for a molecule represented as a graph, we might want to predict what the molecule smells like, or whether it will bind to a receptor implicated in a disease.</p>\n<p><img src=\" GNN graph level task.png\" alt=\"GNN graph level task\" style=\"zoom:50%;\" /></p>\n</blockquote>\n<p>The example is actually a simple task example, and the loops can be detected with ordinary algorithm such as: <a href=\"https://codeburst.io/fast-and-slow-pointer-floyds-cycle-detection-algorithm-9c7a8693f491\">Fast and Slow Pointer: Floyd's Cycle Detection Algorithm</a>. But with more complicated task, GNN can be useful. And here is the related Leetcode question: <a href=\"https://leetcode.com/problems/linked-list-cycle/description/\">No.141: Linked List Cycle</a>.</p>\n<blockquote>\n<p><img src=\" GNN graph message passing.png\" alt=\"GNN graph message passing\" style=\"zoom:50%;\" /></p>\n<p>This is <strong>reminiscent of standard convolution</strong>: in essence, message passing and convolution are operations to aggregate and process the information of an element’s neighbors in order to update the element’s value. In graphs, the element is a node, and in images, the element is a pixel. <strong>However</strong>, the number of neighboring nodes in a graph can be variable, unlike in an image where each pixel has a set number of neighboring elements.</p>\n</blockquote>\n<p>When introducing the message passing method, this blog makes an analogy with the convolution. And in addition to the however part mentioned in this blog, another difference is that in convolution, each element's value is weighted differently while in the aggregation method they are all the same.</p>\n<p>Interestingly, similar weights can be achieved with Graph Attention Networks, which is introduced in later section.</p>\n<blockquote>\n<p>Another way of communicating information between graph attributes is via attention. For example, when we consider the sum-aggregation of a node and its 1-degree neighboring nodes we could also consider using a weighted sum.</p>\n<p>A common scoring function is the inner product and nodes are often transformed before scoring into query and key vectors via a linear map to increase the expressivity of the scoring mechanism.</p>\n</blockquote>\n<h2 id=\"reviews\">Reviews</h2>\n<p>Writing: the whole article is well coherent and fluent, building the knowledge of GNN step by step, from a highly simplified model to the real GNN. The beautiful interactive figures make the article easy to read and digestible. Yet lacking mathematics and codes is both pros and cons. Unfortunately, <em>tarting today Distill will be taking a one year hiatus, which may be extended indefinitely.</em></p>\n<p>Graph neural network: a graph is a powerful tool so that all kinds of data can be described as a graph. But this power leads to a huge difficulty in optimisation. One reason is the sparsity, the dynamic structure makes it difficult the train on CPU or GPU. Another is that GNN is very sensitive to hyper-parameters, just like the experiment section of this blog shows. As such, it is an active research area yet rarely deployed in industry.</p>","wordcount":2887},{"title":"paper reading: AlexNet","author":"Ryan LI","toc":true,"declare":true,"date":"2022-04-07T14:25:50.000Z","_content":"\n> It has been 10 years since AlexNet has been brought out. It is one of the cornerstones of this surge of deep learning.\n\n> This is a [series of paper reading notes](https://daydreamatnight.github.io/2022/04/02/paper-reading-start/), hopefully, to push me to read paper casually and to leave some record of what I've learned.\n\n<!-- more -->\n\npaper link: [ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)\n\nuseful link: https://www.bilibili.com/video/BV1ih411J7Kz\n\n## Little history\n\nIt hasn't gotten much attention by the area of machine learning for the first 2-3 years since it got published, because this paper is written rather as a technical report than an academic paper. A good paper needs new thoughts for the model, or at least some explanations, while this paper only presented how they applied 3 tricks and how good their results are. However, there was no doubt an influential hit in the area of computer vision, which has a passion for refreshing the top list. And this influence spread to other areas gradually with deeper studies on it.\n\n## Notes by sections\n\n### 0. Absturct \n\n> To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation.\n\nIn addition to a brief introduction to the model, the use of GPU is also mentioned in the abstract. And works around GPU are mentioned all the time. It was really a tough engineering job from the perspective of the first writer.  But it is not important for an acdemic paper. Besides, since the emergence of CUDA in 2007, the application of GPU in the ML field in 2012 is not uncommon, and MATLAB is mainly used as a ML tool with a large number of GPU acceleration libraries.\n\n> We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry\n\nAt last, the result in the ILSVRC-2012 competition is as good as knocking the second to the ground and then showing off with a set of backflips. So personally it might look like a technical report, but it's still an outstanding paper and absolutly worth reading.\n\n### 7. Discussion\n\nIn stead of conclusion, this paper leaves a discussion as the last section, which is unsual.\n\n> For example, removing any of the middle layers results in a loss of about 2% for the top-1 performance of the network. So the depth really is important for achieving our results\n\nThe depth is important, but it is insufficient to be simply concluded from the degradation caused by removing one middle layer, ignoring other effects such as superparameter settings. And, considering only the conculsion, a more complete one might be, depth and width are both very important. The ratio of height and width matters.\n\n> To simplify our experiments, we did not use any unsupervised pre-training even though we expect that it will help\n\nBefore AlexNet, it was common to warm up the NN with massive unlabelled images before the actual training i.e. use an unsupervised model as an initial. And the goal of the field of machine learning was to extract the features of data through large-scale unsupervised models. However, this sentence steered the entire field from unsupervised to supervised learning, which, according to the machine learning pioneers such as Hinton and LeCun, was a \"wrong route\". But with the rise of the pre-trained language models such as Bert, and the contrative learning model in CV field such as MoCo, the unsupervised route is gradually comming back to the foreground. \n\n<img src=\"unsupervise learning cake.png\" alt=\"unsupervise learning cake\" style=\"zoom:80%;\" />\n\n> Ultimately we would like to use very large and deep convolutional nets on video sequences where the temporal structure provides very helpful information that is missing or far less obvious in static images.\n\nActually video sequences are still a tough area beacause of the high computational comsumpution and the copyright issues.\n\n### Key figure\n\n<img src=\"alexnet results.png\" alt=\"alexnet results\" style=\"zoom:67%;\" />\n\nThe right part is the most important result in this paper, though it isn't been discussed much in this paper. Actually it shows the last layer feature vectors perform really well in the semantic space i.e. deep neural network is very suitable to extract features from data.\n\n### 1. Introduction\n\n> To improve their performance, we can collect larger datasets, learn more powerful models, and use better techniques for preventing overfitting.\n\nThe paper leads one route of deep learning, which is, with large dataset and model, developing powerful regularization methods to prevent overfitting. However there is a new route, which is focusing on designing good architecture s.t. the overfitting won't happen with large model.\n\n> Our network contains a number of new and unusual features which improve its performance and reduce its training time, which are detailed in Section 3.\n\n> we used several effective techniques for preventing overfitting, which are described in Section 4.\n\nThese two are the innovative points. People can then follow their work later, which makes this paper a cornerstone.\n\n### 2. Dataset\n\n> We did not pre-process the images in any other way, except for subtracting the mean activity over the training set from each pixel. So we trained our network on the (centered) raw RGB values of the pixels.\n\nThere is one more point that is not emphasized. Previously, features of an image (such as SIFT) were always used as input instead of raw RGB values. Datasets such as ImageNet provided SIFT of their image set as well. The end-to-end nature is the selling point of a series of deep learning papers that follow.\n\n### 3. Architecture\n\n#### 3.1. ReLU\n\nFrom a present point of view, ReLU is not that important for speeding up the training process. Other  activation functions still work. It's the simplicity of ReLU that makes it stick.\n\n### 4. Reducing Overfitting\n\nA metaphore of overfitting: In order to get a high score on an exam, you memorize all the answers to the exercises instead of understanding the question.\n\n#### 4.1 Data Augmentation\n\n> The second form of data augmentation consists of altering the intensities of the RGB channels in training images. Specifically, we perform PCA on the set of RGB pixel values throughout the ImageNet training set.\n\nPCA is here use as a augmentation method which follow-up work don't follow. For example, in ResNet a standard color augmentation is used with no fancy methods. And nowadays, standard color augmentation wins.\n\n#### 4.2 dropout\n\n> There is, however, a very efficient version of model combination that only costs about a factor of two during training. The recently-introduced technique, called “dropout”\n\nHere dropout is considered a light version of model ensembling, but later [study below](https://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf) has shown that the effect of dropout is actually equivalent to weight decay/regularization, yet there is no specific weight decay method equivalent to it algorithmically.\n\n>> one way to obtain some of the benefits of dropout without stochasticity is to marginalize the noise to obtain a regularizer that does the same thing as the dropout procedure, in expectation. We showed that for linear regression this regularizer is a modified form of L2 regularization. For more complicated models, it is not obvious how to obtain an equivalent regularizer. \n\n### 5. Details of learning\n\n> we trained our models using stochastic gradient descent with a batch size of 128 examples, momentum of 0.9, and weight decay of 0.0005\n\nmomentum, weight decay with SGD has become a standard method afterwards.\n\n> We initialized the weights in each layer from a zero-mean Gaussian distribution with standard deviation 0.01\n\n(0, 0.01) is usually chosen as the initialization parameter pair in most standard-sized models. (0, 0.02) is in use even for large models like Bert.\n\n> We trained the network for roughly 90 cycles through the training set of 1.2 million images, which took five to six days on two NVIDIA GTX 580 3GB GPUs.\n\nSimilar to what is happening now with training NLP, maybe it will drive the next evolution in hardware. And probably hardware similar to TPU would be popular.\n\n### 6. Results\n\n#### 6.1 Qualitative Evaluations\n\n> The kernels on GPU 1 are largely color-agnostic, while the kernels on on GPU 2 are largely color-specific. The kernels on GPU 1 are largely color-agnostic, while the kernels on on GPU 2 are largely color-specific\n\nInteresting problem but less focused by follow up work.\n\n> consider the feature activations induced by an image at the last, 4096-dimensional hidden layer. If two images produce feature activation vectors with a small Euclidean separation, we can say that the higher levels of the neural network consider them to be similar.\n\nThis is an intuitive work as talked before, and follow up work such as [Visualizing and understanding convolutional networks ](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53) dig deeper trying to interperate the NN. And interpretion is very important for works related to physics or [fairness](https://ieeexplore.ieee.org/abstract/document/9113719/).\n\n## Reference\n\n[Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. *The journal of machine learning research*, *15*(1), 1929-1958.](https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer,)\n\n[Zeiler, M. D., & Fergus, R. (2014, September). Visualizing and understanding convolutional networks. In *European conference on computer vision* (pp. 818-833). Springer, Cham.](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53)\n\n[Du, M., Yang, F., Zou, N., & Hu, X. (2020). Fairness in deep learning: A computational perspective. *IEEE Intelligent Systems*, *36*(4), 25-34.](https://ieeexplore.ieee.org/abstract/document/9113719/)\n","source":"_posts/paper-reading-AlexNet.md","raw":"---\ntitle: 'paper reading: AlexNet'\nauthor: Ryan LI\ntoc: true\ndeclare: true\ndate: 2022-04-07 22:25:50\ntags:\n  - paper reading\n  - deep learning\n---\n\n> It has been 10 years since AlexNet has been brought out. It is one of the cornerstones of this surge of deep learning.\n\n> This is a [series of paper reading notes](https://daydreamatnight.github.io/2022/04/02/paper-reading-start/), hopefully, to push me to read paper casually and to leave some record of what I've learned.\n\n<!-- more -->\n\npaper link: [ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)\n\nuseful link: https://www.bilibili.com/video/BV1ih411J7Kz\n\n## Little history\n\nIt hasn't gotten much attention by the area of machine learning for the first 2-3 years since it got published, because this paper is written rather as a technical report than an academic paper. A good paper needs new thoughts for the model, or at least some explanations, while this paper only presented how they applied 3 tricks and how good their results are. However, there was no doubt an influential hit in the area of computer vision, which has a passion for refreshing the top list. And this influence spread to other areas gradually with deeper studies on it.\n\n## Notes by sections\n\n### 0. Absturct \n\n> To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation.\n\nIn addition to a brief introduction to the model, the use of GPU is also mentioned in the abstract. And works around GPU are mentioned all the time. It was really a tough engineering job from the perspective of the first writer.  But it is not important for an acdemic paper. Besides, since the emergence of CUDA in 2007, the application of GPU in the ML field in 2012 is not uncommon, and MATLAB is mainly used as a ML tool with a large number of GPU acceleration libraries.\n\n> We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry\n\nAt last, the result in the ILSVRC-2012 competition is as good as knocking the second to the ground and then showing off with a set of backflips. So personally it might look like a technical report, but it's still an outstanding paper and absolutly worth reading.\n\n### 7. Discussion\n\nIn stead of conclusion, this paper leaves a discussion as the last section, which is unsual.\n\n> For example, removing any of the middle layers results in a loss of about 2% for the top-1 performance of the network. So the depth really is important for achieving our results\n\nThe depth is important, but it is insufficient to be simply concluded from the degradation caused by removing one middle layer, ignoring other effects such as superparameter settings. And, considering only the conculsion, a more complete one might be, depth and width are both very important. The ratio of height and width matters.\n\n> To simplify our experiments, we did not use any unsupervised pre-training even though we expect that it will help\n\nBefore AlexNet, it was common to warm up the NN with massive unlabelled images before the actual training i.e. use an unsupervised model as an initial. And the goal of the field of machine learning was to extract the features of data through large-scale unsupervised models. However, this sentence steered the entire field from unsupervised to supervised learning, which, according to the machine learning pioneers such as Hinton and LeCun, was a \"wrong route\". But with the rise of the pre-trained language models such as Bert, and the contrative learning model in CV field such as MoCo, the unsupervised route is gradually comming back to the foreground. \n\n<img src=\"unsupervise learning cake.png\" alt=\"unsupervise learning cake\" style=\"zoom:80%;\" />\n\n> Ultimately we would like to use very large and deep convolutional nets on video sequences where the temporal structure provides very helpful information that is missing or far less obvious in static images.\n\nActually video sequences are still a tough area beacause of the high computational comsumpution and the copyright issues.\n\n### Key figure\n\n<img src=\"alexnet results.png\" alt=\"alexnet results\" style=\"zoom:67%;\" />\n\nThe right part is the most important result in this paper, though it isn't been discussed much in this paper. Actually it shows the last layer feature vectors perform really well in the semantic space i.e. deep neural network is very suitable to extract features from data.\n\n### 1. Introduction\n\n> To improve their performance, we can collect larger datasets, learn more powerful models, and use better techniques for preventing overfitting.\n\nThe paper leads one route of deep learning, which is, with large dataset and model, developing powerful regularization methods to prevent overfitting. However there is a new route, which is focusing on designing good architecture s.t. the overfitting won't happen with large model.\n\n> Our network contains a number of new and unusual features which improve its performance and reduce its training time, which are detailed in Section 3.\n\n> we used several effective techniques for preventing overfitting, which are described in Section 4.\n\nThese two are the innovative points. People can then follow their work later, which makes this paper a cornerstone.\n\n### 2. Dataset\n\n> We did not pre-process the images in any other way, except for subtracting the mean activity over the training set from each pixel. So we trained our network on the (centered) raw RGB values of the pixels.\n\nThere is one more point that is not emphasized. Previously, features of an image (such as SIFT) were always used as input instead of raw RGB values. Datasets such as ImageNet provided SIFT of their image set as well. The end-to-end nature is the selling point of a series of deep learning papers that follow.\n\n### 3. Architecture\n\n#### 3.1. ReLU\n\nFrom a present point of view, ReLU is not that important for speeding up the training process. Other  activation functions still work. It's the simplicity of ReLU that makes it stick.\n\n### 4. Reducing Overfitting\n\nA metaphore of overfitting: In order to get a high score on an exam, you memorize all the answers to the exercises instead of understanding the question.\n\n#### 4.1 Data Augmentation\n\n> The second form of data augmentation consists of altering the intensities of the RGB channels in training images. Specifically, we perform PCA on the set of RGB pixel values throughout the ImageNet training set.\n\nPCA is here use as a augmentation method which follow-up work don't follow. For example, in ResNet a standard color augmentation is used with no fancy methods. And nowadays, standard color augmentation wins.\n\n#### 4.2 dropout\n\n> There is, however, a very efficient version of model combination that only costs about a factor of two during training. The recently-introduced technique, called “dropout”\n\nHere dropout is considered a light version of model ensembling, but later [study below](https://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf) has shown that the effect of dropout is actually equivalent to weight decay/regularization, yet there is no specific weight decay method equivalent to it algorithmically.\n\n>> one way to obtain some of the benefits of dropout without stochasticity is to marginalize the noise to obtain a regularizer that does the same thing as the dropout procedure, in expectation. We showed that for linear regression this regularizer is a modified form of L2 regularization. For more complicated models, it is not obvious how to obtain an equivalent regularizer. \n\n### 5. Details of learning\n\n> we trained our models using stochastic gradient descent with a batch size of 128 examples, momentum of 0.9, and weight decay of 0.0005\n\nmomentum, weight decay with SGD has become a standard method afterwards.\n\n> We initialized the weights in each layer from a zero-mean Gaussian distribution with standard deviation 0.01\n\n(0, 0.01) is usually chosen as the initialization parameter pair in most standard-sized models. (0, 0.02) is in use even for large models like Bert.\n\n> We trained the network for roughly 90 cycles through the training set of 1.2 million images, which took five to six days on two NVIDIA GTX 580 3GB GPUs.\n\nSimilar to what is happening now with training NLP, maybe it will drive the next evolution in hardware. And probably hardware similar to TPU would be popular.\n\n### 6. Results\n\n#### 6.1 Qualitative Evaluations\n\n> The kernels on GPU 1 are largely color-agnostic, while the kernels on on GPU 2 are largely color-specific. The kernels on GPU 1 are largely color-agnostic, while the kernels on on GPU 2 are largely color-specific\n\nInteresting problem but less focused by follow up work.\n\n> consider the feature activations induced by an image at the last, 4096-dimensional hidden layer. If two images produce feature activation vectors with a small Euclidean separation, we can say that the higher levels of the neural network consider them to be similar.\n\nThis is an intuitive work as talked before, and follow up work such as [Visualizing and understanding convolutional networks ](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53) dig deeper trying to interperate the NN. And interpretion is very important for works related to physics or [fairness](https://ieeexplore.ieee.org/abstract/document/9113719/).\n\n## Reference\n\n[Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. *The journal of machine learning research*, *15*(1), 1929-1958.](https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer,)\n\n[Zeiler, M. D., & Fergus, R. (2014, September). Visualizing and understanding convolutional networks. In *European conference on computer vision* (pp. 818-833). Springer, Cham.](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53)\n\n[Du, M., Yang, F., Zou, N., & Hu, X. (2020). Fairness in deep learning: A computational perspective. *IEEE Intelligent Systems*, *36*(4), 25-34.](https://ieeexplore.ieee.org/abstract/document/9113719/)\n","slug":"paper-reading-AlexNet","published":1,"updated":"2022-04-30T19:31:01.661Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl2n5f4080009p9yb8uc8drd1","content":"<blockquote>\n<p>It has been 10 years since AlexNet has been brought out. It is one of the cornerstones of this surge of deep learning.</p>\n</blockquote>\n<blockquote>\n<p>This is a <a href=\"https://daydreamatnight.github.io/2022/04/02/paper-reading-start/\">series of paper reading notes</a>, hopefully, to push me to read paper casually and to leave some record of what I've learned.</p>\n</blockquote>\n<span id=\"more\"></span>\n<p>paper link: <a href=\"https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html\">ImageNet Classification with Deep Convolutional Neural Networks</a></p>\n<p>useful link: https://www.bilibili.com/video/BV1ih411J7Kz</p>\n<h2 id=\"little-history\">Little history</h2>\n<p>It hasn't gotten much attention by the area of machine learning for the first 2-3 years since it got published, because this paper is written rather as a technical report than an academic paper. A good paper needs new thoughts for the model, or at least some explanations, while this paper only presented how they applied 3 tricks and how good their results are. However, there was no doubt an influential hit in the area of computer vision, which has a passion for refreshing the top list. And this influence spread to other areas gradually with deeper studies on it.</p>\n<h2 id=\"notes-by-sections\">Notes by sections</h2>\n<h3 id=\"absturct\">0. Absturct</h3>\n<blockquote>\n<p>To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation.</p>\n</blockquote>\n<p>In addition to a brief introduction to the model, the use of GPU is also mentioned in the abstract. And works around GPU are mentioned all the time. It was really a tough engineering job from the perspective of the first writer. But it is not important for an acdemic paper. Besides, since the emergence of CUDA in 2007, the application of GPU in the ML field in 2012 is not uncommon, and MATLAB is mainly used as a ML tool with a large number of GPU acceleration libraries.</p>\n<blockquote>\n<p>We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry</p>\n</blockquote>\n<p>At last, the result in the ILSVRC-2012 competition is as good as knocking the second to the ground and then showing off with a set of backflips. So personally it might look like a technical report, but it's still an outstanding paper and absolutly worth reading.</p>\n<h3 id=\"discussion\">7. Discussion</h3>\n<p>In stead of conclusion, this paper leaves a discussion as the last section, which is unsual.</p>\n<blockquote>\n<p>For example, removing any of the middle layers results in a loss of about 2% for the top-1 performance of the network. So the depth really is important for achieving our results</p>\n</blockquote>\n<p>The depth is important, but it is insufficient to be simply concluded from the degradation caused by removing one middle layer, ignoring other effects such as superparameter settings. And, considering only the conculsion, a more complete one might be, depth and width are both very important. The ratio of height and width matters.</p>\n<blockquote>\n<p>To simplify our experiments, we did not use any unsupervised pre-training even though we expect that it will help</p>\n</blockquote>\n<p>Before AlexNet, it was common to warm up the NN with massive unlabelled images before the actual training i.e. use an unsupervised model as an initial. And the goal of the field of machine learning was to extract the features of data through large-scale unsupervised models. However, this sentence steered the entire field from unsupervised to supervised learning, which, according to the machine learning pioneers such as Hinton and LeCun, was a \"wrong route\". But with the rise of the pre-trained language models such as Bert, and the contrative learning model in CV field such as MoCo, the unsupervised route is gradually comming back to the foreground.</p>\n<p><img src=\"unsupervise learning cake.png\" srcset=\"/img/loading.gif\" lazyload alt=\"unsupervise learning cake\" style=\"zoom:80%;\" /></p>\n<blockquote>\n<p>Ultimately we would like to use very large and deep convolutional nets on video sequences where the temporal structure provides very helpful information that is missing or far less obvious in static images.</p>\n</blockquote>\n<p>Actually video sequences are still a tough area beacause of the high computational comsumpution and the copyright issues.</p>\n<h3 id=\"key-figure\">Key figure</h3>\n<p><img src=\"alexnet results.png\" srcset=\"/img/loading.gif\" lazyload alt=\"alexnet results\" style=\"zoom:67%;\" /></p>\n<p>The right part is the most important result in this paper, though it isn't been discussed much in this paper. Actually it shows the last layer feature vectors perform really well in the semantic space i.e. deep neural network is very suitable to extract features from data.</p>\n<h3 id=\"introduction\">1. Introduction</h3>\n<blockquote>\n<p>To improve their performance, we can collect larger datasets, learn more powerful models, and use better techniques for preventing overfitting.</p>\n</blockquote>\n<p>The paper leads one route of deep learning, which is, with large dataset and model, developing powerful regularization methods to prevent overfitting. However there is a new route, which is focusing on designing good architecture s.t. the overfitting won't happen with large model.</p>\n<blockquote>\n<p>Our network contains a number of new and unusual features which improve its performance and reduce its training time, which are detailed in Section 3.</p>\n</blockquote>\n<blockquote>\n<p>we used several effective techniques for preventing overfitting, which are described in Section 4.</p>\n</blockquote>\n<p>These two are the innovative points. People can then follow their work later, which makes this paper a cornerstone.</p>\n<h3 id=\"dataset\">2. Dataset</h3>\n<blockquote>\n<p>We did not pre-process the images in any other way, except for subtracting the mean activity over the training set from each pixel. So we trained our network on the (centered) raw RGB values of the pixels.</p>\n</blockquote>\n<p>There is one more point that is not emphasized. Previously, features of an image (such as SIFT) were always used as input instead of raw RGB values. Datasets such as ImageNet provided SIFT of their image set as well. The end-to-end nature is the selling point of a series of deep learning papers that follow.</p>\n<h3 id=\"architecture\">3. Architecture</h3>\n<h4 id=\"relu\">3.1. ReLU</h4>\n<p>From a present point of view, ReLU is not that important for speeding up the training process. Other activation functions still work. It's the simplicity of ReLU that makes it stick.</p>\n<h3 id=\"reducing-overfitting\">4. Reducing Overfitting</h3>\n<p>A metaphore of overfitting: In order to get a high score on an exam, you memorize all the answers to the exercises instead of understanding the question.</p>\n<h4 id=\"data-augmentation\">4.1 Data Augmentation</h4>\n<blockquote>\n<p>The second form of data augmentation consists of altering the intensities of the RGB channels in training images. Specifically, we perform PCA on the set of RGB pixel values throughout the ImageNet training set.</p>\n</blockquote>\n<p>PCA is here use as a augmentation method which follow-up work don't follow. For example, in ResNet a standard color augmentation is used with no fancy methods. And nowadays, standard color augmentation wins.</p>\n<h4 id=\"dropout\">4.2 dropout</h4>\n<blockquote>\n<p>There is, however, a very efficient version of model combination that only costs about a factor of two during training. The recently-introduced technique, called “dropout”</p>\n</blockquote>\n<p>Here dropout is considered a light version of model ensembling, but later <a href=\"https://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf\">study below</a> has shown that the effect of dropout is actually equivalent to weight decay/regularization, yet there is no specific weight decay method equivalent to it algorithmically.</p>\n<blockquote>\n<blockquote>\n<p>one way to obtain some of the benefits of dropout without stochasticity is to marginalize the noise to obtain a regularizer that does the same thing as the dropout procedure, in expectation. We showed that for linear regression this regularizer is a modified form of L2 regularization. For more complicated models, it is not obvious how to obtain an equivalent regularizer.</p>\n</blockquote>\n</blockquote>\n<h3 id=\"details-of-learning\">5. Details of learning</h3>\n<blockquote>\n<p>we trained our models using stochastic gradient descent with a batch size of 128 examples, momentum of 0.9, and weight decay of 0.0005</p>\n</blockquote>\n<p>momentum, weight decay with SGD has become a standard method afterwards.</p>\n<blockquote>\n<p>We initialized the weights in each layer from a zero-mean Gaussian distribution with standard deviation 0.01</p>\n</blockquote>\n<p>(0, 0.01) is usually chosen as the initialization parameter pair in most standard-sized models. (0, 0.02) is in use even for large models like Bert.</p>\n<blockquote>\n<p>We trained the network for roughly 90 cycles through the training set of 1.2 million images, which took five to six days on two NVIDIA GTX 580 3GB GPUs.</p>\n</blockquote>\n<p>Similar to what is happening now with training NLP, maybe it will drive the next evolution in hardware. And probably hardware similar to TPU would be popular.</p>\n<h3 id=\"results\">6. Results</h3>\n<h4 id=\"qualitative-evaluations\">6.1 Qualitative Evaluations</h4>\n<blockquote>\n<p>The kernels on GPU 1 are largely color-agnostic, while the kernels on on GPU 2 are largely color-specific. The kernels on GPU 1 are largely color-agnostic, while the kernels on on GPU 2 are largely color-specific</p>\n</blockquote>\n<p>Interesting problem but less focused by follow up work.</p>\n<blockquote>\n<p>consider the feature activations induced by an image at the last, 4096-dimensional hidden layer. If two images produce feature activation vectors with a small Euclidean separation, we can say that the higher levels of the neural network consider them to be similar.</p>\n</blockquote>\n<p>This is an intuitive work as talked before, and follow up work such as <a href=\"https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53\">Visualizing and understanding convolutional networks</a> dig deeper trying to interperate the NN. And interpretion is very important for works related to physics or <a href=\"https://ieeexplore.ieee.org/abstract/document/9113719/\">fairness</a>.</p>\n<h2 id=\"reference\">Reference</h2>\n<p><a href=\"https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer,\">Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., &amp; Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. <em>The journal of machine learning research</em>, <em>15</em>(1), 1929-1958.</a></p>\n<p><a href=\"https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53\">Zeiler, M. D., &amp; Fergus, R. (2014, September). Visualizing and understanding convolutional networks. In <em>European conference on computer vision</em> (pp. 818-833). Springer, Cham.</a></p>\n<p><a href=\"https://ieeexplore.ieee.org/abstract/document/9113719/\">Du, M., Yang, F., Zou, N., &amp; Hu, X. (2020). Fairness in deep learning: A computational perspective. <em>IEEE Intelligent Systems</em>, <em>36</em>(4), 25-34.</a></p>\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>It has been 10 years since AlexNet has been brought out. It is one of the cornerstones of this surge of deep learning.</p>\n</blockquote>\n<blockquote>\n<p>This is a <a href=\"https://daydreamatnight.github.io/2022/04/02/paper-reading-start/\">series of paper reading notes</a>, hopefully, to push me to read paper casually and to leave some record of what I've learned.</p>\n</blockquote>","more":"<p>paper link: <a href=\"https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html\">ImageNet Classification with Deep Convolutional Neural Networks</a></p>\n<p>useful link: https://www.bilibili.com/video/BV1ih411J7Kz</p>\n<h2 id=\"little-history\">Little history</h2>\n<p>It hasn't gotten much attention by the area of machine learning for the first 2-3 years since it got published, because this paper is written rather as a technical report than an academic paper. A good paper needs new thoughts for the model, or at least some explanations, while this paper only presented how they applied 3 tricks and how good their results are. However, there was no doubt an influential hit in the area of computer vision, which has a passion for refreshing the top list. And this influence spread to other areas gradually with deeper studies on it.</p>\n<h2 id=\"notes-by-sections\">Notes by sections</h2>\n<h3 id=\"absturct\">0. Absturct</h3>\n<blockquote>\n<p>To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation.</p>\n</blockquote>\n<p>In addition to a brief introduction to the model, the use of GPU is also mentioned in the abstract. And works around GPU are mentioned all the time. It was really a tough engineering job from the perspective of the first writer. But it is not important for an acdemic paper. Besides, since the emergence of CUDA in 2007, the application of GPU in the ML field in 2012 is not uncommon, and MATLAB is mainly used as a ML tool with a large number of GPU acceleration libraries.</p>\n<blockquote>\n<p>We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry</p>\n</blockquote>\n<p>At last, the result in the ILSVRC-2012 competition is as good as knocking the second to the ground and then showing off with a set of backflips. So personally it might look like a technical report, but it's still an outstanding paper and absolutly worth reading.</p>\n<h3 id=\"discussion\">7. Discussion</h3>\n<p>In stead of conclusion, this paper leaves a discussion as the last section, which is unsual.</p>\n<blockquote>\n<p>For example, removing any of the middle layers results in a loss of about 2% for the top-1 performance of the network. So the depth really is important for achieving our results</p>\n</blockquote>\n<p>The depth is important, but it is insufficient to be simply concluded from the degradation caused by removing one middle layer, ignoring other effects such as superparameter settings. And, considering only the conculsion, a more complete one might be, depth and width are both very important. The ratio of height and width matters.</p>\n<blockquote>\n<p>To simplify our experiments, we did not use any unsupervised pre-training even though we expect that it will help</p>\n</blockquote>\n<p>Before AlexNet, it was common to warm up the NN with massive unlabelled images before the actual training i.e. use an unsupervised model as an initial. And the goal of the field of machine learning was to extract the features of data through large-scale unsupervised models. However, this sentence steered the entire field from unsupervised to supervised learning, which, according to the machine learning pioneers such as Hinton and LeCun, was a \"wrong route\". But with the rise of the pre-trained language models such as Bert, and the contrative learning model in CV field such as MoCo, the unsupervised route is gradually comming back to the foreground.</p>\n<p><img src=\"unsupervise learning cake.png\" alt=\"unsupervise learning cake\" style=\"zoom:80%;\" /></p>\n<blockquote>\n<p>Ultimately we would like to use very large and deep convolutional nets on video sequences where the temporal structure provides very helpful information that is missing or far less obvious in static images.</p>\n</blockquote>\n<p>Actually video sequences are still a tough area beacause of the high computational comsumpution and the copyright issues.</p>\n<h3 id=\"key-figure\">Key figure</h3>\n<p><img src=\"alexnet results.png\" alt=\"alexnet results\" style=\"zoom:67%;\" /></p>\n<p>The right part is the most important result in this paper, though it isn't been discussed much in this paper. Actually it shows the last layer feature vectors perform really well in the semantic space i.e. deep neural network is very suitable to extract features from data.</p>\n<h3 id=\"introduction\">1. Introduction</h3>\n<blockquote>\n<p>To improve their performance, we can collect larger datasets, learn more powerful models, and use better techniques for preventing overfitting.</p>\n</blockquote>\n<p>The paper leads one route of deep learning, which is, with large dataset and model, developing powerful regularization methods to prevent overfitting. However there is a new route, which is focusing on designing good architecture s.t. the overfitting won't happen with large model.</p>\n<blockquote>\n<p>Our network contains a number of new and unusual features which improve its performance and reduce its training time, which are detailed in Section 3.</p>\n</blockquote>\n<blockquote>\n<p>we used several effective techniques for preventing overfitting, which are described in Section 4.</p>\n</blockquote>\n<p>These two are the innovative points. People can then follow their work later, which makes this paper a cornerstone.</p>\n<h3 id=\"dataset\">2. Dataset</h3>\n<blockquote>\n<p>We did not pre-process the images in any other way, except for subtracting the mean activity over the training set from each pixel. So we trained our network on the (centered) raw RGB values of the pixels.</p>\n</blockquote>\n<p>There is one more point that is not emphasized. Previously, features of an image (such as SIFT) were always used as input instead of raw RGB values. Datasets such as ImageNet provided SIFT of their image set as well. The end-to-end nature is the selling point of a series of deep learning papers that follow.</p>\n<h3 id=\"architecture\">3. Architecture</h3>\n<h4 id=\"relu\">3.1. ReLU</h4>\n<p>From a present point of view, ReLU is not that important for speeding up the training process. Other activation functions still work. It's the simplicity of ReLU that makes it stick.</p>\n<h3 id=\"reducing-overfitting\">4. Reducing Overfitting</h3>\n<p>A metaphore of overfitting: In order to get a high score on an exam, you memorize all the answers to the exercises instead of understanding the question.</p>\n<h4 id=\"data-augmentation\">4.1 Data Augmentation</h4>\n<blockquote>\n<p>The second form of data augmentation consists of altering the intensities of the RGB channels in training images. Specifically, we perform PCA on the set of RGB pixel values throughout the ImageNet training set.</p>\n</blockquote>\n<p>PCA is here use as a augmentation method which follow-up work don't follow. For example, in ResNet a standard color augmentation is used with no fancy methods. And nowadays, standard color augmentation wins.</p>\n<h4 id=\"dropout\">4.2 dropout</h4>\n<blockquote>\n<p>There is, however, a very efficient version of model combination that only costs about a factor of two during training. The recently-introduced technique, called “dropout”</p>\n</blockquote>\n<p>Here dropout is considered a light version of model ensembling, but later <a href=\"https://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf\">study below</a> has shown that the effect of dropout is actually equivalent to weight decay/regularization, yet there is no specific weight decay method equivalent to it algorithmically.</p>\n<blockquote>\n<blockquote>\n<p>one way to obtain some of the benefits of dropout without stochasticity is to marginalize the noise to obtain a regularizer that does the same thing as the dropout procedure, in expectation. We showed that for linear regression this regularizer is a modified form of L2 regularization. For more complicated models, it is not obvious how to obtain an equivalent regularizer.</p>\n</blockquote>\n</blockquote>\n<h3 id=\"details-of-learning\">5. Details of learning</h3>\n<blockquote>\n<p>we trained our models using stochastic gradient descent with a batch size of 128 examples, momentum of 0.9, and weight decay of 0.0005</p>\n</blockquote>\n<p>momentum, weight decay with SGD has become a standard method afterwards.</p>\n<blockquote>\n<p>We initialized the weights in each layer from a zero-mean Gaussian distribution with standard deviation 0.01</p>\n</blockquote>\n<p>(0, 0.01) is usually chosen as the initialization parameter pair in most standard-sized models. (0, 0.02) is in use even for large models like Bert.</p>\n<blockquote>\n<p>We trained the network for roughly 90 cycles through the training set of 1.2 million images, which took five to six days on two NVIDIA GTX 580 3GB GPUs.</p>\n</blockquote>\n<p>Similar to what is happening now with training NLP, maybe it will drive the next evolution in hardware. And probably hardware similar to TPU would be popular.</p>\n<h3 id=\"results\">6. Results</h3>\n<h4 id=\"qualitative-evaluations\">6.1 Qualitative Evaluations</h4>\n<blockquote>\n<p>The kernels on GPU 1 are largely color-agnostic, while the kernels on on GPU 2 are largely color-specific. The kernels on GPU 1 are largely color-agnostic, while the kernels on on GPU 2 are largely color-specific</p>\n</blockquote>\n<p>Interesting problem but less focused by follow up work.</p>\n<blockquote>\n<p>consider the feature activations induced by an image at the last, 4096-dimensional hidden layer. If two images produce feature activation vectors with a small Euclidean separation, we can say that the higher levels of the neural network consider them to be similar.</p>\n</blockquote>\n<p>This is an intuitive work as talked before, and follow up work such as <a href=\"https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53\">Visualizing and understanding convolutional networks</a> dig deeper trying to interperate the NN. And interpretion is very important for works related to physics or <a href=\"https://ieeexplore.ieee.org/abstract/document/9113719/\">fairness</a>.</p>\n<h2 id=\"reference\">Reference</h2>\n<p><a href=\"https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer,\">Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., &amp; Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. <em>The journal of machine learning research</em>, <em>15</em>(1), 1929-1958.</a></p>\n<p><a href=\"https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53\">Zeiler, M. D., &amp; Fergus, R. (2014, September). Visualizing and understanding convolutional networks. In <em>European conference on computer vision</em> (pp. 818-833). Springer, Cham.</a></p>\n<p><a href=\"https://ieeexplore.ieee.org/abstract/document/9113719/\">Du, M., Yang, F., Zou, N., &amp; Hu, X. (2020). Fairness in deep learning: A computational perspective. <em>IEEE Intelligent Systems</em>, <em>36</em>(4), 25-34.</a></p>","wordcount":7689},{"title":"learning rate schedule","author":"Ryan LI","toc":true,"declare":true,"date":"2022-03-08T01:32:32.000Z","_content":"\n> Learning rate schedule is one commonly used trick to control the process of training. Different kinds of learning tricks are presented every day. In this article, I have put together classical methods theories and apply them in this little competition.\n\n> Recently, I joined a [Kaggle image classification competition](https://www.kaggle.com/c/classify-leaves/), I used the pretrained ResNet50 plus other tricks and here is to record some of them I've learned for now. \n\n<!-- more -->\n\n### Introduction\n\nLearning rate is one critical parameter in alliterative algorithms, including PDE and ODE solving, optimization, and eigenvalue calculation. In the deep learning area, the learning rate is more than critical because of the notorious difficulty on [Stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent).\n\nStrictly, there are two ways of adjusting the learning rate: \n\n- learning rate scheduling: \n\n  adjust the global learning rate during iteration\n\n- adaptive learning rate: \n\n  adjust the learning rate for each parameter based on their gradients updates(moments), also called adaptive gradient or gradient descent optimization.\n\nIn this article, **learning rate schedule is mainly discussed**. Afterward, \"learning rate\" refers to the \"global learning rate\".\n\n### Methods of learning rate scheduling\n\nApart from the constant learning rate, there are several ways to schedule the learning rate:\n\n  - change with epoch numbers\n    -   learning rate decay: linear, step...\n    \n    -   learning rate down then up: stochastic gradient descent with warm restarts(SGDR) and Cyclical Learning rates(CLR) \n    \n    -  warmup\n  \n  - change on some validation measurements: plateau \n\n#### learning rate decay\n\nUnder the upper concepts of decaying the learning rate while training, how to choose a specific decay policy is personal. It can be continuous or step, linear or polynomial, exponential or trigonometric. \n\nIn articles, stepped learning rate decay is more often used as the default choice. For example, [Zagoruyko, S., & Komodakis, N ](https://arxiv.org/abs/1605.07146) set the initial learning rate as 0.1 and drop it by 0.2 every 60 epochs on their modified version of ResNet. And this version of learning rate decay is set as the control group to compare with the SGDR strategy later in [Ilya Loshchilov & Frank Hutter's work](https://arxiv.org/abs/1608.03983).  And in practice, the cosine annealing policy is a common choice today and can be used either alone or in combination with warmup and SGDR.\n\n##### Explanation\n\nBecause of the presence of stochastic noise, the entire gradient descent process is not straightforward. With a constant learning rate, as shown in the gradient contour map below, the minima can not be reached with a constant step (blue) due to the relatively small steps at the bottom. And a lower minimum can be reached if the learning rate descends with the gradient i.e. epoch(green).\n\n  <img src=\"SGD%20with%20learning%20rate%20decay.png\" alt=\"SGD with learning rate decay\" style=\"zoom:80%;\" />\n\n  #### SGDR and CLR\n\n  ##### SGDR\n\nStochastic gradient descent with warm restarts(SGDR) is firstly proposed to Deep learning in [Ilya Loshchilov & Frank Hutter's work](https://arxiv.org/abs/1608.03983). They introduced a policy of reinitializing the learning rate every certain number of epochs. Applying cosine annealing learning rate decay during each resulting \"mini-run\", the results perform fascinating.\n\n  <img src=\"SGDR.png\" alt=\"SGDR\" style=\"zoom:75%;\" />\n\n  <img src=\"SGDR_REsult.png\" alt=\"SGDR_REsult\" style=\"zoom:100%;\" />\n\nAs shown in the charts, compared to 2 default step learnin rate decay policies, they enacted several SGDR policies with different T_0 and T_mul. T_0 refers to the epoch interval of the first \"mini-run\" and the epoch interval is multiplied by T_mul after each restart. As a result, at the ith \"mini-run\", T_i = T_0*T_mul^(i) \n\nAnd they suggests a SGDR policy with a small T0 = 1 or 10 at start, and set Tmult = 2 to double the epoch interval after every restart. And they claim by this policy, at least 2× to 4× fewer epochs are required to achieve a comparable result than before.\n\n  ##### CLR\n\nA similar method called cyclical Learning rates(CLR) is proposed later by [Leslie N. Smith](https://ieeexplore.ieee.org/abstract/document/7926641/), where 2 kinds of triangular and exponential CLR policies are demonstrated on CIFAR-10 and CIFAR-100 with most kinds of mainstream CNN modules.\n\n  <img src=\"CLR.png\" alt=\"CLR\" style=\"zoom:75%;\" />\n\nSimilarly, compared with a default fixed learnin rate, the demonstrats that their policies outperforms in accuracy and efficiency on several datasetes.\n\n  > one obtains the same test classification accuracy of 81.4% after only 25, 000 iterations with the triangular2 policy as obtained by running the standard hyper-parameter settings for 70, 000 iterations.  \n\n  ##### explanation\n\nBecause of the nonconvexity, it is common sense that reaching a global minima is impossible. With a standard learning rate decay, a saddle point, or unstable local minima is more likely to trap the descending process as shown below. But cyclical Learning rates(CLR) and stochastic gradient descent with warm restarts(SGDR) would allow the process to  “jump” from one local minimum to another regularly until a stable one.\n\n  <img src=\"2d%20cyclic%20learning%20rate%20schedule.png\" alt=\"2d cyclic learning rate schedule\" style=\"zoom:80%;\" />\n\n  <img src=\"cyclic%20learning%20rate%20schedule.png\" alt=\"cyclic learning rate schedule\" style=\"zoom:80%;\" />\n\nStill there are several choices, but Cosine Cyclical and Cosine Annealing with Warm Restarts are more common.\n\n  #### learning rate warmup\n\nLearning rate warmup is first applied in the famous [Resnet](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) paper in one of its experiments.\n\n  > In this case, we find that the initial learning rate of 0.1 is slightly too large to start converging5 . So we use 0.01 to warm up the training until the training error is below 80% (about 400 iterations), and then go back to 0.1 and continue training.  \n\nAnd later [Goyal and He's work](https://arxiv.org/pdf/1706.02677.pdf) makes a major influence, where constant and gradual methods of warmup are discussed. And gradual warmup is proved to be effective on large minibatch size.\n\n  > As we discussed, for large minibatches (e.g., 8k) the linear scaling rule breaks down when the network is changing rapidly, which commonly occurs in early stages of training. We find that this issue can be alleviated by a properly designed warmup [16], namely, a strategy of using less aggressive learning rates at the start of training. \n\n  <img src=\"warmup%20on%20large%20batches.png\" alt=\"warmup on large batches\" style=\"zoom:100%;\" />\n\nIn practice, warmup are always combined with other learning rate methods afterwards. And linear warmup is a default method.\n\n  #### Reducing the learning rate on plateau\n\nApart from methods scheduling the learning rate with epoch, a dynamic learning rate decay method is also an option. It denotes the process of decaying the learning rate only when the optimizer fails to improve the accuracy or decrease the loss in serval epochs. \n\nFor example, in [AlexNet](https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html), \n\n> The heuristic which we followed was to divide the learning rate by 10 when the validation error rate stopped improving with the current learning rate. The learning rate was initialized at 0.01 and reduced three times prior to termination.\n\nIn  [Resnet](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) after the warm-up,\n\n> The learning rate starts from 0.1 and is divided by 10 when the error plateaus\n\n\n\n### Apply learning rate scheduling in PyTorch\n\n> `torch.optim.lr_scheduler` provides several methods to adjust the learning rate based on the number of epochs. \n\nFor example, \n\n```python\ndef train_ch6(net, train_iter, test_iter, num_epochs, lr, device):\n    print('training on', device)\n    net.to(device)\n    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs*len(train_iter)/10, eta_min=1e-9)\n    loss = LSR(0.1) \n    for epoch in range(num_epochs):\n        net.train()\n        for i, (X, y) in enumerate(train_iter):\n            X, y = X.to(device), y.to(device)\n            optimizer.zero_grad()\n            y_hat = net(X)\n            l = loss(y_hat, y)\n            l.backward()\n            optimizer.step()\n            scheduler.step()\n```\n\nApart from well defined `lr_scheduler` ,  `torch.optim.lr_scheduler.LambdaLR` allow us to apply self define scheduler such as:\n\n```python\nprint('training on', device)\nnet.to(device)\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\n\nt=10*len(train_iter)#warmup\nT=num_epochs*len(train_iter)\nlambda1 = lambda epoch: (0.9*epoch / t+0.1) if epoch < t else  0.1  if 0.5 * (1+math.cos(math.pi*(epoch - t)/(T-t)))<0.1 else 0.5 * (1+math.cos(math.pi*(epoch - t)/(T-t)))\n\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n\n# plot learningrate_decay\nlr_plot = []\nfor _i in range(num_epochs):\n    for _j in range(len(train_iter)):\n        optimizer.step()\n        lr_plot.append(optimizer.param_groups[0][\"lr\"])\n        scheduler.step()\nplt.plot(lr_plot)\n```\n### Should we do scheduling with adaptive learning rate method?\n\nFrom [Should we do learning rate decay for adam optimizer](https://stackoverflow.com/questions/39517431/should-we-do-learning-rate-decay-for-adam-optimizer)?, I found it as a arguable question.\n\n>It depends. ADAM updates any parameter with an individual learning rate. This means that every parameter in the network has a specific learning rate associated. \n>\n>But* the single learning rate for each parameter is computed using lambda (the initial learning rate) as an upper limit. This means that every single learning rate can vary from 0 (no update) to lambda (maximum update).\n>\n>It's true, that the learning rates adapt themselves during training steps, but if you want to be sure that every update step doesn't exceed lambda you can than lower lambda using exponential decay or whatever. It can help to reduce loss during the latest step of training, when the computed loss with the previously associated lambda parameter has stopped to decrease.\n\n>  In my experience it usually not necessary to do learning rate decay with Adam optimizer. \n>\n> The theory is that Adam already handles learning rate optimization ([check reference](http://arxiv.org/pdf/1412.6980v8.pdf)) :\n>\n> > \"We propose Adam, a method for efficient stochastic optimization that only requires first-order gradients with little memory requirement. The method **computes individual adaptive learning rates** for different parameters from estimates of first and second moments of the gradients; the name Adam is derived from adaptive moment estimation.\"\n>\n> As with any deep learning problem YMMV, one size does not fit all, you should try different approaches and see what works for you, etc. etc.\n\n>\n>  Yes, absolutely. From my own experience, it's very useful to Adam with learning rate decay. Without decay, you have to set a very small learning rate so the loss won't begin to diverge after decrease to a point.\n\nBut in the article [Decoupled weight decay regularization](https://arxiv.org/abs/1711.05101)(AdamW), it is encouraged.\n\n> Adam can substantially benefit from a scheduled learning rate multiplier. The fact that Adam is an adaptive gradient algorithm and as such adapts the learning rate for each parameter does not rule out the possibility to substantially improve its performance by using a global learning rate multiplier, scheduled, e.g., by cosine annealing.  \n\nIn the CLR article, the authors encourage the combination of CLR methods with Adam as well.\n\n> Adaptive learning rates are fundamentally different from CLR policies, and CLR can be combined with adaptive learning rates, as shown in Section 4.1. I \n\nAll in all, theoretically, the adaptive learning rate methods such as Adam adjust the learning rate for each parameters under a upper limit as the global learning rate, which can be adjusted by scheduling. \n\nIn practice, at least SGDR and CLR have been proved to be useful combining with optimizers.\n\n### Experiment: Adam vs Adam + SGDR \n\nIn this little experiment, the best setting in the last article is set as baseline, with Adam with constant learning rate. Leave other settings, Adam with cosine annealing learning rate, and AdamW with cosine annealing learning rate are compared.\n\n`global learning rate = 0.005` \n\n`scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = **int**(num_epochs***len**(train_iter)/10), T_mult=1, eta_min=1e-9)`\n\n<img src=\"experiment.png\" alt=\"experiment\" style=\"zoom:80%;\" />\n\nAs shown in the line charts, SGDR lift both the training and test accuracies. And the overfitting of the baseline method is alleviated as well. \n\nIn the second and  sub-figure, the fluctuation in the process of gradient descend caused by the cosine learning rate is obvious. And after each learning rate restart, the rate of the descend also gets a restart.  And it takes fewer epochs than to get the same accuracy than the baseline.\n\n### Reference\n\n[Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. *arXiv preprint arXiv:1412.6980*.](https://arxiv.org/pdf/1412.6980.pdf)\n\n[Adaptive Learning Rate Method](https://wiki.tum.de/display/lfdv/Adaptive+Learning+Rate+Method) \n\n[Learning Rate Schedules and Adaptive Learning Rate Methods](https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1) \n\n[Learning Rate Decay and methods in Deep Learning](https://medium.com/analytics-vidhya/learning-rate-decay-and-methods-in-deep-learning-2cee564f910b#:~:text=Learning%20rate%20decay%20is%20a,help%20both%20optimization%20and%20generalization.) \n\n[A Newbie’s Guide to Stochastic Gradient Descent With Restarts](https://towardsdatascience.com/https-medium-com-reina-wang-tw-stochastic-gradient-descent-with-restarts-5f511975163)\n\n[Zagoruyko, S., & Komodakis, N. (2016). Wide residual networks. *arXiv preprint arXiv:1605.07146*.](https://arxiv.org/abs/1605.07146)\n\n[Loshchilov, I., & Hutter, F. (2016). Sgdr: Stochastic gradient descent with warm restarts. *arXiv preprint arXiv:1608.03983*.](https://arxiv.org/abs/1608.03983) \n\n[Smith, L. N. (2017, March). Cyclical learning rates for training neural networks. In *2017 IEEE winter conference on applications of computer vision (WACV)* (pp. 464-472). IEEE.](https://ieeexplore.ieee.org/abstract/document/7926641/) \n\n[He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In *Proceedings of the IEEE conference on computer vision and pattern recognition* (pp. 770-778).](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) \n\n[Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., ... & He, K. (2017). Accurate, large minibatch sgd: Training imagenet in 1 hour. *arXiv preprint arXiv:1706.02677*.](https://arxiv.org/abs/1706.0267)\n\n[Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. *Advances in neural information processing systems*, *25*.](https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)\n\n[torch.optim — PyTorch 1.10 documentation](https://pytorch.org/docs/stable/optim.html) \n\n[Should we do learning rate decay for adam optimizer](https://stackoverflow.com/questions/39517431/should-we-do-learning-rate-decay-for-adam-optimizer)\n\n[Loshchilov, I., & Hutter, F. (2017). Decoupled weight decay regularization. *arXiv preprint arXiv:1711.05101*.](https://arxiv.org/abs/1711.05101)\n\n[Guide to Pytorch Learning Rate Scheduling](https://www.kaggle.com/isbhargav/guide-to-pytorch-learning-rate-scheduling)","source":"_posts/learning-rate-schedule.md","raw":"---\ntitle: learning rate schedule\nauthor: Ryan LI\ntoc: true\ndeclare: true\ndate: 2022-03-08 09:32:32\ntags:\n\t- deep learning\n\t- deep learning tricks\n---\n\n> Learning rate schedule is one commonly used trick to control the process of training. Different kinds of learning tricks are presented every day. In this article, I have put together classical methods theories and apply them in this little competition.\n\n> Recently, I joined a [Kaggle image classification competition](https://www.kaggle.com/c/classify-leaves/), I used the pretrained ResNet50 plus other tricks and here is to record some of them I've learned for now. \n\n<!-- more -->\n\n### Introduction\n\nLearning rate is one critical parameter in alliterative algorithms, including PDE and ODE solving, optimization, and eigenvalue calculation. In the deep learning area, the learning rate is more than critical because of the notorious difficulty on [Stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent).\n\nStrictly, there are two ways of adjusting the learning rate: \n\n- learning rate scheduling: \n\n  adjust the global learning rate during iteration\n\n- adaptive learning rate: \n\n  adjust the learning rate for each parameter based on their gradients updates(moments), also called adaptive gradient or gradient descent optimization.\n\nIn this article, **learning rate schedule is mainly discussed**. Afterward, \"learning rate\" refers to the \"global learning rate\".\n\n### Methods of learning rate scheduling\n\nApart from the constant learning rate, there are several ways to schedule the learning rate:\n\n  - change with epoch numbers\n    -   learning rate decay: linear, step...\n    \n    -   learning rate down then up: stochastic gradient descent with warm restarts(SGDR) and Cyclical Learning rates(CLR) \n    \n    -  warmup\n  \n  - change on some validation measurements: plateau \n\n#### learning rate decay\n\nUnder the upper concepts of decaying the learning rate while training, how to choose a specific decay policy is personal. It can be continuous or step, linear or polynomial, exponential or trigonometric. \n\nIn articles, stepped learning rate decay is more often used as the default choice. For example, [Zagoruyko, S., & Komodakis, N ](https://arxiv.org/abs/1605.07146) set the initial learning rate as 0.1 and drop it by 0.2 every 60 epochs on their modified version of ResNet. And this version of learning rate decay is set as the control group to compare with the SGDR strategy later in [Ilya Loshchilov & Frank Hutter's work](https://arxiv.org/abs/1608.03983).  And in practice, the cosine annealing policy is a common choice today and can be used either alone or in combination with warmup and SGDR.\n\n##### Explanation\n\nBecause of the presence of stochastic noise, the entire gradient descent process is not straightforward. With a constant learning rate, as shown in the gradient contour map below, the minima can not be reached with a constant step (blue) due to the relatively small steps at the bottom. And a lower minimum can be reached if the learning rate descends with the gradient i.e. epoch(green).\n\n  <img src=\"SGD%20with%20learning%20rate%20decay.png\" alt=\"SGD with learning rate decay\" style=\"zoom:80%;\" />\n\n  #### SGDR and CLR\n\n  ##### SGDR\n\nStochastic gradient descent with warm restarts(SGDR) is firstly proposed to Deep learning in [Ilya Loshchilov & Frank Hutter's work](https://arxiv.org/abs/1608.03983). They introduced a policy of reinitializing the learning rate every certain number of epochs. Applying cosine annealing learning rate decay during each resulting \"mini-run\", the results perform fascinating.\n\n  <img src=\"SGDR.png\" alt=\"SGDR\" style=\"zoom:75%;\" />\n\n  <img src=\"SGDR_REsult.png\" alt=\"SGDR_REsult\" style=\"zoom:100%;\" />\n\nAs shown in the charts, compared to 2 default step learnin rate decay policies, they enacted several SGDR policies with different T_0 and T_mul. T_0 refers to the epoch interval of the first \"mini-run\" and the epoch interval is multiplied by T_mul after each restart. As a result, at the ith \"mini-run\", T_i = T_0*T_mul^(i) \n\nAnd they suggests a SGDR policy with a small T0 = 1 or 10 at start, and set Tmult = 2 to double the epoch interval after every restart. And they claim by this policy, at least 2× to 4× fewer epochs are required to achieve a comparable result than before.\n\n  ##### CLR\n\nA similar method called cyclical Learning rates(CLR) is proposed later by [Leslie N. Smith](https://ieeexplore.ieee.org/abstract/document/7926641/), where 2 kinds of triangular and exponential CLR policies are demonstrated on CIFAR-10 and CIFAR-100 with most kinds of mainstream CNN modules.\n\n  <img src=\"CLR.png\" alt=\"CLR\" style=\"zoom:75%;\" />\n\nSimilarly, compared with a default fixed learnin rate, the demonstrats that their policies outperforms in accuracy and efficiency on several datasetes.\n\n  > one obtains the same test classification accuracy of 81.4% after only 25, 000 iterations with the triangular2 policy as obtained by running the standard hyper-parameter settings for 70, 000 iterations.  \n\n  ##### explanation\n\nBecause of the nonconvexity, it is common sense that reaching a global minima is impossible. With a standard learning rate decay, a saddle point, or unstable local minima is more likely to trap the descending process as shown below. But cyclical Learning rates(CLR) and stochastic gradient descent with warm restarts(SGDR) would allow the process to  “jump” from one local minimum to another regularly until a stable one.\n\n  <img src=\"2d%20cyclic%20learning%20rate%20schedule.png\" alt=\"2d cyclic learning rate schedule\" style=\"zoom:80%;\" />\n\n  <img src=\"cyclic%20learning%20rate%20schedule.png\" alt=\"cyclic learning rate schedule\" style=\"zoom:80%;\" />\n\nStill there are several choices, but Cosine Cyclical and Cosine Annealing with Warm Restarts are more common.\n\n  #### learning rate warmup\n\nLearning rate warmup is first applied in the famous [Resnet](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) paper in one of its experiments.\n\n  > In this case, we find that the initial learning rate of 0.1 is slightly too large to start converging5 . So we use 0.01 to warm up the training until the training error is below 80% (about 400 iterations), and then go back to 0.1 and continue training.  \n\nAnd later [Goyal and He's work](https://arxiv.org/pdf/1706.02677.pdf) makes a major influence, where constant and gradual methods of warmup are discussed. And gradual warmup is proved to be effective on large minibatch size.\n\n  > As we discussed, for large minibatches (e.g., 8k) the linear scaling rule breaks down when the network is changing rapidly, which commonly occurs in early stages of training. We find that this issue can be alleviated by a properly designed warmup [16], namely, a strategy of using less aggressive learning rates at the start of training. \n\n  <img src=\"warmup%20on%20large%20batches.png\" alt=\"warmup on large batches\" style=\"zoom:100%;\" />\n\nIn practice, warmup are always combined with other learning rate methods afterwards. And linear warmup is a default method.\n\n  #### Reducing the learning rate on plateau\n\nApart from methods scheduling the learning rate with epoch, a dynamic learning rate decay method is also an option. It denotes the process of decaying the learning rate only when the optimizer fails to improve the accuracy or decrease the loss in serval epochs. \n\nFor example, in [AlexNet](https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html), \n\n> The heuristic which we followed was to divide the learning rate by 10 when the validation error rate stopped improving with the current learning rate. The learning rate was initialized at 0.01 and reduced three times prior to termination.\n\nIn  [Resnet](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) after the warm-up,\n\n> The learning rate starts from 0.1 and is divided by 10 when the error plateaus\n\n\n\n### Apply learning rate scheduling in PyTorch\n\n> `torch.optim.lr_scheduler` provides several methods to adjust the learning rate based on the number of epochs. \n\nFor example, \n\n```python\ndef train_ch6(net, train_iter, test_iter, num_epochs, lr, device):\n    print('training on', device)\n    net.to(device)\n    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs*len(train_iter)/10, eta_min=1e-9)\n    loss = LSR(0.1) \n    for epoch in range(num_epochs):\n        net.train()\n        for i, (X, y) in enumerate(train_iter):\n            X, y = X.to(device), y.to(device)\n            optimizer.zero_grad()\n            y_hat = net(X)\n            l = loss(y_hat, y)\n            l.backward()\n            optimizer.step()\n            scheduler.step()\n```\n\nApart from well defined `lr_scheduler` ,  `torch.optim.lr_scheduler.LambdaLR` allow us to apply self define scheduler such as:\n\n```python\nprint('training on', device)\nnet.to(device)\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\n\nt=10*len(train_iter)#warmup\nT=num_epochs*len(train_iter)\nlambda1 = lambda epoch: (0.9*epoch / t+0.1) if epoch < t else  0.1  if 0.5 * (1+math.cos(math.pi*(epoch - t)/(T-t)))<0.1 else 0.5 * (1+math.cos(math.pi*(epoch - t)/(T-t)))\n\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n\n# plot learningrate_decay\nlr_plot = []\nfor _i in range(num_epochs):\n    for _j in range(len(train_iter)):\n        optimizer.step()\n        lr_plot.append(optimizer.param_groups[0][\"lr\"])\n        scheduler.step()\nplt.plot(lr_plot)\n```\n### Should we do scheduling with adaptive learning rate method?\n\nFrom [Should we do learning rate decay for adam optimizer](https://stackoverflow.com/questions/39517431/should-we-do-learning-rate-decay-for-adam-optimizer)?, I found it as a arguable question.\n\n>It depends. ADAM updates any parameter with an individual learning rate. This means that every parameter in the network has a specific learning rate associated. \n>\n>But* the single learning rate for each parameter is computed using lambda (the initial learning rate) as an upper limit. This means that every single learning rate can vary from 0 (no update) to lambda (maximum update).\n>\n>It's true, that the learning rates adapt themselves during training steps, but if you want to be sure that every update step doesn't exceed lambda you can than lower lambda using exponential decay or whatever. It can help to reduce loss during the latest step of training, when the computed loss with the previously associated lambda parameter has stopped to decrease.\n\n>  In my experience it usually not necessary to do learning rate decay with Adam optimizer. \n>\n> The theory is that Adam already handles learning rate optimization ([check reference](http://arxiv.org/pdf/1412.6980v8.pdf)) :\n>\n> > \"We propose Adam, a method for efficient stochastic optimization that only requires first-order gradients with little memory requirement. The method **computes individual adaptive learning rates** for different parameters from estimates of first and second moments of the gradients; the name Adam is derived from adaptive moment estimation.\"\n>\n> As with any deep learning problem YMMV, one size does not fit all, you should try different approaches and see what works for you, etc. etc.\n\n>\n>  Yes, absolutely. From my own experience, it's very useful to Adam with learning rate decay. Without decay, you have to set a very small learning rate so the loss won't begin to diverge after decrease to a point.\n\nBut in the article [Decoupled weight decay regularization](https://arxiv.org/abs/1711.05101)(AdamW), it is encouraged.\n\n> Adam can substantially benefit from a scheduled learning rate multiplier. The fact that Adam is an adaptive gradient algorithm and as such adapts the learning rate for each parameter does not rule out the possibility to substantially improve its performance by using a global learning rate multiplier, scheduled, e.g., by cosine annealing.  \n\nIn the CLR article, the authors encourage the combination of CLR methods with Adam as well.\n\n> Adaptive learning rates are fundamentally different from CLR policies, and CLR can be combined with adaptive learning rates, as shown in Section 4.1. I \n\nAll in all, theoretically, the adaptive learning rate methods such as Adam adjust the learning rate for each parameters under a upper limit as the global learning rate, which can be adjusted by scheduling. \n\nIn practice, at least SGDR and CLR have been proved to be useful combining with optimizers.\n\n### Experiment: Adam vs Adam + SGDR \n\nIn this little experiment, the best setting in the last article is set as baseline, with Adam with constant learning rate. Leave other settings, Adam with cosine annealing learning rate, and AdamW with cosine annealing learning rate are compared.\n\n`global learning rate = 0.005` \n\n`scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = **int**(num_epochs***len**(train_iter)/10), T_mult=1, eta_min=1e-9)`\n\n<img src=\"experiment.png\" alt=\"experiment\" style=\"zoom:80%;\" />\n\nAs shown in the line charts, SGDR lift both the training and test accuracies. And the overfitting of the baseline method is alleviated as well. \n\nIn the second and  sub-figure, the fluctuation in the process of gradient descend caused by the cosine learning rate is obvious. And after each learning rate restart, the rate of the descend also gets a restart.  And it takes fewer epochs than to get the same accuracy than the baseline.\n\n### Reference\n\n[Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. *arXiv preprint arXiv:1412.6980*.](https://arxiv.org/pdf/1412.6980.pdf)\n\n[Adaptive Learning Rate Method](https://wiki.tum.de/display/lfdv/Adaptive+Learning+Rate+Method) \n\n[Learning Rate Schedules and Adaptive Learning Rate Methods](https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1) \n\n[Learning Rate Decay and methods in Deep Learning](https://medium.com/analytics-vidhya/learning-rate-decay-and-methods-in-deep-learning-2cee564f910b#:~:text=Learning%20rate%20decay%20is%20a,help%20both%20optimization%20and%20generalization.) \n\n[A Newbie’s Guide to Stochastic Gradient Descent With Restarts](https://towardsdatascience.com/https-medium-com-reina-wang-tw-stochastic-gradient-descent-with-restarts-5f511975163)\n\n[Zagoruyko, S., & Komodakis, N. (2016). Wide residual networks. *arXiv preprint arXiv:1605.07146*.](https://arxiv.org/abs/1605.07146)\n\n[Loshchilov, I., & Hutter, F. (2016). Sgdr: Stochastic gradient descent with warm restarts. *arXiv preprint arXiv:1608.03983*.](https://arxiv.org/abs/1608.03983) \n\n[Smith, L. N. (2017, March). Cyclical learning rates for training neural networks. In *2017 IEEE winter conference on applications of computer vision (WACV)* (pp. 464-472). IEEE.](https://ieeexplore.ieee.org/abstract/document/7926641/) \n\n[He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In *Proceedings of the IEEE conference on computer vision and pattern recognition* (pp. 770-778).](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) \n\n[Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., ... & He, K. (2017). Accurate, large minibatch sgd: Training imagenet in 1 hour. *arXiv preprint arXiv:1706.02677*.](https://arxiv.org/abs/1706.0267)\n\n[Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. *Advances in neural information processing systems*, *25*.](https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)\n\n[torch.optim — PyTorch 1.10 documentation](https://pytorch.org/docs/stable/optim.html) \n\n[Should we do learning rate decay for adam optimizer](https://stackoverflow.com/questions/39517431/should-we-do-learning-rate-decay-for-adam-optimizer)\n\n[Loshchilov, I., & Hutter, F. (2017). Decoupled weight decay regularization. *arXiv preprint arXiv:1711.05101*.](https://arxiv.org/abs/1711.05101)\n\n[Guide to Pytorch Learning Rate Scheduling](https://www.kaggle.com/isbhargav/guide-to-pytorch-learning-rate-scheduling)","slug":"learning-rate-schedule","published":1,"updated":"2022-04-30T19:30:56.281Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl2n5f409000ap9ybaswy20uo","content":"<blockquote>\n<p>Learning rate schedule is one commonly used trick to control the process of training. Different kinds of learning tricks are presented every day. In this article, I have put together classical methods theories and apply them in this little competition.</p>\n</blockquote>\n<blockquote>\n<p>Recently, I joined a <a href=\"https://www.kaggle.com/c/classify-leaves/\">Kaggle image classification competition</a>, I used the pretrained ResNet50 plus other tricks and here is to record some of them I've learned for now.</p>\n</blockquote>\n<span id=\"more\"></span>\n<h3 id=\"introduction\">Introduction</h3>\n<p>Learning rate is one critical parameter in alliterative algorithms, including PDE and ODE solving, optimization, and eigenvalue calculation. In the deep learning area, the learning rate is more than critical because of the notorious difficulty on <a href=\"https://en.wikipedia.org/wiki/Stochastic_gradient_descent\">Stochastic gradient descent</a>.</p>\n<p>Strictly, there are two ways of adjusting the learning rate:</p>\n<ul>\n<li><p>learning rate scheduling:</p>\n<p>adjust the global learning rate during iteration</p></li>\n<li><p>adaptive learning rate:</p>\n<p>adjust the learning rate for each parameter based on their gradients updates(moments), also called adaptive gradient or gradient descent optimization.</p></li>\n</ul>\n<p>In this article, <strong>learning rate schedule is mainly discussed</strong>. Afterward, \"learning rate\" refers to the \"global learning rate\".</p>\n<h3 id=\"methods-of-learning-rate-scheduling\">Methods of learning rate scheduling</h3>\n<p>Apart from the constant learning rate, there are several ways to schedule the learning rate:</p>\n<ul>\n<li>change with epoch numbers\n<ul>\n<li><p>learning rate decay: linear, step...</p></li>\n<li><p>learning rate down then up: stochastic gradient descent with warm restarts(SGDR) and Cyclical Learning rates(CLR)</p></li>\n<li><p>warmup</p></li>\n</ul></li>\n<li>change on some validation measurements: plateau</li>\n</ul>\n<h4 id=\"learning-rate-decay\">learning rate decay</h4>\n<p>Under the upper concepts of decaying the learning rate while training, how to choose a specific decay policy is personal. It can be continuous or step, linear or polynomial, exponential or trigonometric.</p>\n<p>In articles, stepped learning rate decay is more often used as the default choice. For example, <a href=\"https://arxiv.org/abs/1605.07146\">Zagoruyko, S., &amp; Komodakis, N</a> set the initial learning rate as 0.1 and drop it by 0.2 every 60 epochs on their modified version of ResNet. And this version of learning rate decay is set as the control group to compare with the SGDR strategy later in <a href=\"https://arxiv.org/abs/1608.03983\">Ilya Loshchilov &amp; Frank Hutter's work</a>. And in practice, the cosine annealing policy is a common choice today and can be used either alone or in combination with warmup and SGDR.</p>\n<h5 id=\"explanation\">Explanation</h5>\n<p>Because of the presence of stochastic noise, the entire gradient descent process is not straightforward. With a constant learning rate, as shown in the gradient contour map below, the minima can not be reached with a constant step (blue) due to the relatively small steps at the bottom. And a lower minimum can be reached if the learning rate descends with the gradient i.e. epoch(green).</p>\n<p><img src=\"SGD%20with%20learning%20rate%20decay.png\" srcset=\"/img/loading.gif\" lazyload alt=\"SGD with learning rate decay\" style=\"zoom:80%;\" /></p>\n<p>#### SGDR and CLR</p>\n<p>##### SGDR</p>\n<p>Stochastic gradient descent with warm restarts(SGDR) is firstly proposed to Deep learning in <a href=\"https://arxiv.org/abs/1608.03983\">Ilya Loshchilov &amp; Frank Hutter's work</a>. They introduced a policy of reinitializing the learning rate every certain number of epochs. Applying cosine annealing learning rate decay during each resulting \"mini-run\", the results perform fascinating.</p>\n<p><img src=\"SGDR.png\" srcset=\"/img/loading.gif\" lazyload alt=\"SGDR\" style=\"zoom:75%;\" /></p>\n<p><img src=\"SGDR_REsult.png\" srcset=\"/img/loading.gif\" lazyload alt=\"SGDR_REsult\" style=\"zoom:100%;\" /></p>\n<p>As shown in the charts, compared to 2 default step learnin rate decay policies, they enacted several SGDR policies with different T_0 and T_mul. T_0 refers to the epoch interval of the first \"mini-run\" and the epoch interval is multiplied by T_mul after each restart. As a result, at the ith \"mini-run\", T_i = T_0*T_mul^(i)</p>\n<p>And they suggests a SGDR policy with a small T0 = 1 or 10 at start, and set Tmult = 2 to double the epoch interval after every restart. And they claim by this policy, at least 2× to 4× fewer epochs are required to achieve a comparable result than before.</p>\n<p>##### CLR</p>\n<p>A similar method called cyclical Learning rates(CLR) is proposed later by <a href=\"https://ieeexplore.ieee.org/abstract/document/7926641/\">Leslie N. Smith</a>, where 2 kinds of triangular and exponential CLR policies are demonstrated on CIFAR-10 and CIFAR-100 with most kinds of mainstream CNN modules.</p>\n<p><img src=\"CLR.png\" srcset=\"/img/loading.gif\" lazyload alt=\"CLR\" style=\"zoom:75%;\" /></p>\n<p>Similarly, compared with a default fixed learnin rate, the demonstrats that their policies outperforms in accuracy and efficiency on several datasetes.</p>\n<blockquote>\n<p>one obtains the same test classification accuracy of 81.4% after only 25, 000 iterations with the triangular2 policy as obtained by running the standard hyper-parameter settings for 70, 000 iterations.</p>\n</blockquote>\n<p>##### explanation</p>\n<p>Because of the nonconvexity, it is common sense that reaching a global minima is impossible. With a standard learning rate decay, a saddle point, or unstable local minima is more likely to trap the descending process as shown below. But cyclical Learning rates(CLR) and stochastic gradient descent with warm restarts(SGDR) would allow the process to “jump” from one local minimum to another regularly until a stable one.</p>\n<p><img src=\"2d%20cyclic%20learning%20rate%20schedule.png\" srcset=\"/img/loading.gif\" lazyload alt=\"2d cyclic learning rate schedule\" style=\"zoom:80%;\" /></p>\n<p><img src=\"cyclic%20learning%20rate%20schedule.png\" srcset=\"/img/loading.gif\" lazyload alt=\"cyclic learning rate schedule\" style=\"zoom:80%;\" /></p>\n<p>Still there are several choices, but Cosine Cyclical and Cosine Annealing with Warm Restarts are more common.</p>\n<p>#### learning rate warmup</p>\n<p>Learning rate warmup is first applied in the famous <a href=\"https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\">Resnet</a> paper in one of its experiments.</p>\n<blockquote>\n<p>In this case, we find that the initial learning rate of 0.1 is slightly too large to start converging5 . So we use 0.01 to warm up the training until the training error is below 80% (about 400 iterations), and then go back to 0.1 and continue training.</p>\n</blockquote>\n<p>And later <a href=\"https://arxiv.org/pdf/1706.02677.pdf\">Goyal and He's work</a> makes a major influence, where constant and gradual methods of warmup are discussed. And gradual warmup is proved to be effective on large minibatch size.</p>\n<blockquote>\n<p>As we discussed, for large minibatches (e.g., 8k) the linear scaling rule breaks down when the network is changing rapidly, which commonly occurs in early stages of training. We find that this issue can be alleviated by a properly designed warmup [16], namely, a strategy of using less aggressive learning rates at the start of training.</p>\n</blockquote>\n<p><img src=\"warmup%20on%20large%20batches.png\" srcset=\"/img/loading.gif\" lazyload alt=\"warmup on large batches\" style=\"zoom:100%;\" /></p>\n<p>In practice, warmup are always combined with other learning rate methods afterwards. And linear warmup is a default method.</p>\n<p>#### Reducing the learning rate on plateau</p>\n<p>Apart from methods scheduling the learning rate with epoch, a dynamic learning rate decay method is also an option. It denotes the process of decaying the learning rate only when the optimizer fails to improve the accuracy or decrease the loss in serval epochs.</p>\n<p>For example, in <a href=\"https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html\">AlexNet</a>,</p>\n<blockquote>\n<p>The heuristic which we followed was to divide the learning rate by 10 when the validation error rate stopped improving with the current learning rate. The learning rate was initialized at 0.01 and reduced three times prior to termination.</p>\n</blockquote>\n<p>In <a href=\"https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\">Resnet</a> after the warm-up,</p>\n<blockquote>\n<p>The learning rate starts from 0.1 and is divided by 10 when the error plateaus</p>\n</blockquote>\n<h3 id=\"apply-learning-rate-scheduling-in-pytorch\">Apply learning rate scheduling in PyTorch</h3>\n<blockquote>\n<p><code>torch.optim.lr_scheduler</code> provides several methods to adjust the learning rate based on the number of epochs.</p>\n</blockquote>\n<p>For example,</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">train_ch6</span>(<span class=\"hljs-params\">net, train_iter, test_iter, num_epochs, lr, device</span>):\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&#x27;training on&#x27;</span>, device)\n    net.to(device)\n    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs*<span class=\"hljs-built_in\">len</span>(train_iter)/<span class=\"hljs-number\">10</span>, eta_min=<span class=\"hljs-number\">1e-9</span>)\n    loss = LSR(<span class=\"hljs-number\">0.1</span>) \n    <span class=\"hljs-keyword\">for</span> epoch <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(num_epochs):\n        net.train()\n        <span class=\"hljs-keyword\">for</span> i, (X, y) <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">enumerate</span>(train_iter):\n            X, y = X.to(device), y.to(device)\n            optimizer.zero_grad()\n            y_hat = net(X)\n            l = loss(y_hat, y)\n            l.backward()\n            optimizer.step()\n            scheduler.step()</code></pre></div>\n<p>Apart from well defined <code>lr_scheduler</code> , <code>torch.optim.lr_scheduler.LambdaLR</code> allow us to apply self define scheduler such as:</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\"><span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&#x27;training on&#x27;</span>, device)\nnet.to(device)\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\n\nt=<span class=\"hljs-number\">10</span>*<span class=\"hljs-built_in\">len</span>(train_iter)<span class=\"hljs-comment\">#warmup</span>\nT=num_epochs*<span class=\"hljs-built_in\">len</span>(train_iter)\nlambda1 = <span class=\"hljs-keyword\">lambda</span> epoch: (<span class=\"hljs-number\">0.9</span>*epoch / t+<span class=\"hljs-number\">0.1</span>) <span class=\"hljs-keyword\">if</span> epoch &lt; t <span class=\"hljs-keyword\">else</span>  <span class=\"hljs-number\">0.1</span>  <span class=\"hljs-keyword\">if</span> <span class=\"hljs-number\">0.5</span> * (<span class=\"hljs-number\">1</span>+math.cos(math.pi*(epoch - t)/(T-t)))&lt;<span class=\"hljs-number\">0.1</span> <span class=\"hljs-keyword\">else</span> <span class=\"hljs-number\">0.5</span> * (<span class=\"hljs-number\">1</span>+math.cos(math.pi*(epoch - t)/(T-t)))\n\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n\n<span class=\"hljs-comment\"># plot learningrate_decay</span>\nlr_plot = []\n<span class=\"hljs-keyword\">for</span> _i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(num_epochs):\n    <span class=\"hljs-keyword\">for</span> _j <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-built_in\">len</span>(train_iter)):\n        optimizer.step()\n        lr_plot.append(optimizer.param_groups[<span class=\"hljs-number\">0</span>][<span class=\"hljs-string\">&quot;lr&quot;</span>])\n        scheduler.step()\nplt.plot(lr_plot)</code></pre></div>\n<h3 id=\"should-we-do-scheduling-with-adaptive-learning-rate-method\">Should we do scheduling with adaptive learning rate method?</h3>\n<p>From <a href=\"https://stackoverflow.com/questions/39517431/should-we-do-learning-rate-decay-for-adam-optimizer\">Should we do learning rate decay for adam optimizer</a>?, I found it as a arguable question.</p>\n<blockquote>\n<p>It depends. ADAM updates any parameter with an individual learning rate. This means that every parameter in the network has a specific learning rate associated.</p>\n<p>But* the single learning rate for each parameter is computed using lambda (the initial learning rate) as an upper limit. This means that every single learning rate can vary from 0 (no update) to lambda (maximum update).</p>\n<p>It's true, that the learning rates adapt themselves during training steps, but if you want to be sure that every update step doesn't exceed lambda you can than lower lambda using exponential decay or whatever. It can help to reduce loss during the latest step of training, when the computed loss with the previously associated lambda parameter has stopped to decrease.</p>\n</blockquote>\n<blockquote>\n<p>In my experience it usually not necessary to do learning rate decay with Adam optimizer.</p>\n<p>The theory is that Adam already handles learning rate optimization (<a href=\"http://arxiv.org/pdf/1412.6980v8.pdf\">check reference</a>) :</p>\n<blockquote>\n<p>\"We propose Adam, a method for efficient stochastic optimization that only requires first-order gradients with little memory requirement. The method <strong>computes individual adaptive learning rates</strong> for different parameters from estimates of first and second moments of the gradients; the name Adam is derived from adaptive moment estimation.\"</p>\n</blockquote>\n<p>As with any deep learning problem YMMV, one size does not fit all, you should try different approaches and see what works for you, etc. etc.</p>\n</blockquote>\n<blockquote>\n<p>Yes, absolutely. From my own experience, it's very useful to Adam with learning rate decay. Without decay, you have to set a very small learning rate so the loss won't begin to diverge after decrease to a point.</p>\n</blockquote>\n<p>But in the article <a href=\"https://arxiv.org/abs/1711.05101\">Decoupled weight decay regularization</a>(AdamW), it is encouraged.</p>\n<blockquote>\n<p>Adam can substantially benefit from a scheduled learning rate multiplier. The fact that Adam is an adaptive gradient algorithm and as such adapts the learning rate for each parameter does not rule out the possibility to substantially improve its performance by using a global learning rate multiplier, scheduled, e.g., by cosine annealing.</p>\n</blockquote>\n<p>In the CLR article, the authors encourage the combination of CLR methods with Adam as well.</p>\n<blockquote>\n<p>Adaptive learning rates are fundamentally different from CLR policies, and CLR can be combined with adaptive learning rates, as shown in Section 4.1. I</p>\n</blockquote>\n<p>All in all, theoretically, the adaptive learning rate methods such as Adam adjust the learning rate for each parameters under a upper limit as the global learning rate, which can be adjusted by scheduling.</p>\n<p>In practice, at least SGDR and CLR have been proved to be useful combining with optimizers.</p>\n<h3 id=\"experiment-adam-vs-adam-sgdr\">Experiment: Adam vs Adam + SGDR</h3>\n<p>In this little experiment, the best setting in the last article is set as baseline, with Adam with constant learning rate. Leave other settings, Adam with cosine annealing learning rate, and AdamW with cosine annealing learning rate are compared.</p>\n<p><code>global learning rate = 0.005</code></p>\n<p><code>scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = **int**(num_epochs***len**(train_iter)/10), T_mult=1, eta_min=1e-9)</code></p>\n<p><img src=\"experiment.png\" srcset=\"/img/loading.gif\" lazyload alt=\"experiment\" style=\"zoom:80%;\" /></p>\n<p>As shown in the line charts, SGDR lift both the training and test accuracies. And the overfitting of the baseline method is alleviated as well.</p>\n<p>In the second and sub-figure, the fluctuation in the process of gradient descend caused by the cosine learning rate is obvious. And after each learning rate restart, the rate of the descend also gets a restart. And it takes fewer epochs than to get the same accuracy than the baseline.</p>\n<h3 id=\"reference\">Reference</h3>\n<p><a href=\"https://arxiv.org/pdf/1412.6980.pdf\">Kingma, D. P., &amp; Ba, J. (2014). Adam: A method for stochastic optimization. <em>arXiv preprint arXiv:1412.6980</em>.</a></p>\n<p><a href=\"https://wiki.tum.de/display/lfdv/Adaptive+Learning+Rate+Method\">Adaptive Learning Rate Method</a></p>\n<p><a href=\"https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1\">Learning Rate Schedules and Adaptive Learning Rate Methods</a></p>\n<p><a href=\"https://medium.com/analytics-vidhya/learning-rate-decay-and-methods-in-deep-learning-2cee564f910b#:~:text=Learning%20rate%20decay%20is%20a,help%20both%20optimization%20and%20generalization.\">Learning Rate Decay and methods in Deep Learning</a></p>\n<p><a href=\"https://towardsdatascience.com/https-medium-com-reina-wang-tw-stochastic-gradient-descent-with-restarts-5f511975163\">A Newbie’s Guide to Stochastic Gradient Descent With Restarts</a></p>\n<p><a href=\"https://arxiv.org/abs/1605.07146\">Zagoruyko, S., &amp; Komodakis, N. (2016). Wide residual networks. <em>arXiv preprint arXiv:1605.07146</em>.</a></p>\n<p><a href=\"https://arxiv.org/abs/1608.03983\">Loshchilov, I., &amp; Hutter, F. (2016). Sgdr: Stochastic gradient descent with warm restarts. <em>arXiv preprint arXiv:1608.03983</em>.</a></p>\n<p><a href=\"https://ieeexplore.ieee.org/abstract/document/7926641/\">Smith, L. N. (2017, March). Cyclical learning rates for training neural networks. In <em>2017 IEEE winter conference on applications of computer vision (WACV)</em> (pp. 464-472). IEEE.</a></p>\n<p><a href=\"https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\">He, K., Zhang, X., Ren, S., &amp; Sun, J. (2016). Deep residual learning for image recognition. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> (pp. 770-778).</a></p>\n<p><a href=\"https://arxiv.org/abs/1706.0267\">Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., ... &amp; He, K. (2017). Accurate, large minibatch sgd: Training imagenet in 1 hour. <em>arXiv preprint arXiv:1706.02677</em>.</a></p>\n<p><a href=\"https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html\">Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. <em>Advances in neural information processing systems</em>, <em>25</em>.</a></p>\n<p><a href=\"https://pytorch.org/docs/stable/optim.html\">torch.optim — PyTorch 1.10 documentation</a></p>\n<p><a href=\"https://stackoverflow.com/questions/39517431/should-we-do-learning-rate-decay-for-adam-optimizer\">Should we do learning rate decay for adam optimizer</a></p>\n<p><a href=\"https://arxiv.org/abs/1711.05101\">Loshchilov, I., &amp; Hutter, F. (2017). Decoupled weight decay regularization. <em>arXiv preprint arXiv:1711.05101</em>.</a></p>\n<p><a href=\"https://www.kaggle.com/isbhargav/guide-to-pytorch-learning-rate-scheduling\">Guide to Pytorch Learning Rate Scheduling</a></p>\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>Learning rate schedule is one commonly used trick to control the process of training. Different kinds of learning tricks are presented every day. In this article, I have put together classical methods theories and apply them in this little competition.</p>\n</blockquote>\n<blockquote>\n<p>Recently, I joined a <a href=\"https://www.kaggle.com/c/classify-leaves/\">Kaggle image classification competition</a>, I used the pretrained ResNet50 plus other tricks and here is to record some of them I've learned for now.</p>\n</blockquote>","more":"<h3 id=\"introduction\">Introduction</h3>\n<p>Learning rate is one critical parameter in alliterative algorithms, including PDE and ODE solving, optimization, and eigenvalue calculation. In the deep learning area, the learning rate is more than critical because of the notorious difficulty on <a href=\"https://en.wikipedia.org/wiki/Stochastic_gradient_descent\">Stochastic gradient descent</a>.</p>\n<p>Strictly, there are two ways of adjusting the learning rate:</p>\n<ul>\n<li><p>learning rate scheduling:</p>\n<p>adjust the global learning rate during iteration</p></li>\n<li><p>adaptive learning rate:</p>\n<p>adjust the learning rate for each parameter based on their gradients updates(moments), also called adaptive gradient or gradient descent optimization.</p></li>\n</ul>\n<p>In this article, <strong>learning rate schedule is mainly discussed</strong>. Afterward, \"learning rate\" refers to the \"global learning rate\".</p>\n<h3 id=\"methods-of-learning-rate-scheduling\">Methods of learning rate scheduling</h3>\n<p>Apart from the constant learning rate, there are several ways to schedule the learning rate:</p>\n<ul>\n<li>change with epoch numbers\n<ul>\n<li><p>learning rate decay: linear, step...</p></li>\n<li><p>learning rate down then up: stochastic gradient descent with warm restarts(SGDR) and Cyclical Learning rates(CLR)</p></li>\n<li><p>warmup</p></li>\n</ul></li>\n<li>change on some validation measurements: plateau</li>\n</ul>\n<h4 id=\"learning-rate-decay\">learning rate decay</h4>\n<p>Under the upper concepts of decaying the learning rate while training, how to choose a specific decay policy is personal. It can be continuous or step, linear or polynomial, exponential or trigonometric.</p>\n<p>In articles, stepped learning rate decay is more often used as the default choice. For example, <a href=\"https://arxiv.org/abs/1605.07146\">Zagoruyko, S., &amp; Komodakis, N</a> set the initial learning rate as 0.1 and drop it by 0.2 every 60 epochs on their modified version of ResNet. And this version of learning rate decay is set as the control group to compare with the SGDR strategy later in <a href=\"https://arxiv.org/abs/1608.03983\">Ilya Loshchilov &amp; Frank Hutter's work</a>. And in practice, the cosine annealing policy is a common choice today and can be used either alone or in combination with warmup and SGDR.</p>\n<h5 id=\"explanation\">Explanation</h5>\n<p>Because of the presence of stochastic noise, the entire gradient descent process is not straightforward. With a constant learning rate, as shown in the gradient contour map below, the minima can not be reached with a constant step (blue) due to the relatively small steps at the bottom. And a lower minimum can be reached if the learning rate descends with the gradient i.e. epoch(green).</p>\n<p><img src=\"SGD%20with%20learning%20rate%20decay.png\" alt=\"SGD with learning rate decay\" style=\"zoom:80%;\" /></p>\n<p>#### SGDR and CLR</p>\n<p>##### SGDR</p>\n<p>Stochastic gradient descent with warm restarts(SGDR) is firstly proposed to Deep learning in <a href=\"https://arxiv.org/abs/1608.03983\">Ilya Loshchilov &amp; Frank Hutter's work</a>. They introduced a policy of reinitializing the learning rate every certain number of epochs. Applying cosine annealing learning rate decay during each resulting \"mini-run\", the results perform fascinating.</p>\n<p><img src=\"SGDR.png\" alt=\"SGDR\" style=\"zoom:75%;\" /></p>\n<p><img src=\"SGDR_REsult.png\" alt=\"SGDR_REsult\" style=\"zoom:100%;\" /></p>\n<p>As shown in the charts, compared to 2 default step learnin rate decay policies, they enacted several SGDR policies with different T_0 and T_mul. T_0 refers to the epoch interval of the first \"mini-run\" and the epoch interval is multiplied by T_mul after each restart. As a result, at the ith \"mini-run\", T_i = T_0*T_mul^(i)</p>\n<p>And they suggests a SGDR policy with a small T0 = 1 or 10 at start, and set Tmult = 2 to double the epoch interval after every restart. And they claim by this policy, at least 2× to 4× fewer epochs are required to achieve a comparable result than before.</p>\n<p>##### CLR</p>\n<p>A similar method called cyclical Learning rates(CLR) is proposed later by <a href=\"https://ieeexplore.ieee.org/abstract/document/7926641/\">Leslie N. Smith</a>, where 2 kinds of triangular and exponential CLR policies are demonstrated on CIFAR-10 and CIFAR-100 with most kinds of mainstream CNN modules.</p>\n<p><img src=\"CLR.png\" alt=\"CLR\" style=\"zoom:75%;\" /></p>\n<p>Similarly, compared with a default fixed learnin rate, the demonstrats that their policies outperforms in accuracy and efficiency on several datasetes.</p>\n<blockquote>\n<p>one obtains the same test classification accuracy of 81.4% after only 25, 000 iterations with the triangular2 policy as obtained by running the standard hyper-parameter settings for 70, 000 iterations.</p>\n</blockquote>\n<p>##### explanation</p>\n<p>Because of the nonconvexity, it is common sense that reaching a global minima is impossible. With a standard learning rate decay, a saddle point, or unstable local minima is more likely to trap the descending process as shown below. But cyclical Learning rates(CLR) and stochastic gradient descent with warm restarts(SGDR) would allow the process to “jump” from one local minimum to another regularly until a stable one.</p>\n<p><img src=\"2d%20cyclic%20learning%20rate%20schedule.png\" alt=\"2d cyclic learning rate schedule\" style=\"zoom:80%;\" /></p>\n<p><img src=\"cyclic%20learning%20rate%20schedule.png\" alt=\"cyclic learning rate schedule\" style=\"zoom:80%;\" /></p>\n<p>Still there are several choices, but Cosine Cyclical and Cosine Annealing with Warm Restarts are more common.</p>\n<p>#### learning rate warmup</p>\n<p>Learning rate warmup is first applied in the famous <a href=\"https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\">Resnet</a> paper in one of its experiments.</p>\n<blockquote>\n<p>In this case, we find that the initial learning rate of 0.1 is slightly too large to start converging5 . So we use 0.01 to warm up the training until the training error is below 80% (about 400 iterations), and then go back to 0.1 and continue training.</p>\n</blockquote>\n<p>And later <a href=\"https://arxiv.org/pdf/1706.02677.pdf\">Goyal and He's work</a> makes a major influence, where constant and gradual methods of warmup are discussed. And gradual warmup is proved to be effective on large minibatch size.</p>\n<blockquote>\n<p>As we discussed, for large minibatches (e.g., 8k) the linear scaling rule breaks down when the network is changing rapidly, which commonly occurs in early stages of training. We find that this issue can be alleviated by a properly designed warmup [16], namely, a strategy of using less aggressive learning rates at the start of training.</p>\n</blockquote>\n<p><img src=\"warmup%20on%20large%20batches.png\" alt=\"warmup on large batches\" style=\"zoom:100%;\" /></p>\n<p>In practice, warmup are always combined with other learning rate methods afterwards. And linear warmup is a default method.</p>\n<p>#### Reducing the learning rate on plateau</p>\n<p>Apart from methods scheduling the learning rate with epoch, a dynamic learning rate decay method is also an option. It denotes the process of decaying the learning rate only when the optimizer fails to improve the accuracy or decrease the loss in serval epochs.</p>\n<p>For example, in <a href=\"https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html\">AlexNet</a>,</p>\n<blockquote>\n<p>The heuristic which we followed was to divide the learning rate by 10 when the validation error rate stopped improving with the current learning rate. The learning rate was initialized at 0.01 and reduced three times prior to termination.</p>\n</blockquote>\n<p>In <a href=\"https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\">Resnet</a> after the warm-up,</p>\n<blockquote>\n<p>The learning rate starts from 0.1 and is divided by 10 when the error plateaus</p>\n</blockquote>\n<h3 id=\"apply-learning-rate-scheduling-in-pytorch\">Apply learning rate scheduling in PyTorch</h3>\n<blockquote>\n<p><code>torch.optim.lr_scheduler</code> provides several methods to adjust the learning rate based on the number of epochs.</p>\n</blockquote>\n<p>For example,</p>\n<pre><code class=\"hljs python\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">train_ch6</span>(<span class=\"hljs-params\">net, train_iter, test_iter, num_epochs, lr, device</span>):\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&#x27;training on&#x27;</span>, device)\n    net.to(device)\n    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs*<span class=\"hljs-built_in\">len</span>(train_iter)/<span class=\"hljs-number\">10</span>, eta_min=<span class=\"hljs-number\">1e-9</span>)\n    loss = LSR(<span class=\"hljs-number\">0.1</span>) \n    <span class=\"hljs-keyword\">for</span> epoch <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(num_epochs):\n        net.train()\n        <span class=\"hljs-keyword\">for</span> i, (X, y) <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">enumerate</span>(train_iter):\n            X, y = X.to(device), y.to(device)\n            optimizer.zero_grad()\n            y_hat = net(X)\n            l = loss(y_hat, y)\n            l.backward()\n            optimizer.step()\n            scheduler.step()</code></pre>\n<p>Apart from well defined <code>lr_scheduler</code> , <code>torch.optim.lr_scheduler.LambdaLR</code> allow us to apply self define scheduler such as:</p>\n<pre><code class=\"hljs python\"><span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&#x27;training on&#x27;</span>, device)\nnet.to(device)\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\n\nt=<span class=\"hljs-number\">10</span>*<span class=\"hljs-built_in\">len</span>(train_iter)<span class=\"hljs-comment\">#warmup</span>\nT=num_epochs*<span class=\"hljs-built_in\">len</span>(train_iter)\nlambda1 = <span class=\"hljs-keyword\">lambda</span> epoch: (<span class=\"hljs-number\">0.9</span>*epoch / t+<span class=\"hljs-number\">0.1</span>) <span class=\"hljs-keyword\">if</span> epoch &lt; t <span class=\"hljs-keyword\">else</span>  <span class=\"hljs-number\">0.1</span>  <span class=\"hljs-keyword\">if</span> <span class=\"hljs-number\">0.5</span> * (<span class=\"hljs-number\">1</span>+math.cos(math.pi*(epoch - t)/(T-t)))&lt;<span class=\"hljs-number\">0.1</span> <span class=\"hljs-keyword\">else</span> <span class=\"hljs-number\">0.5</span> * (<span class=\"hljs-number\">1</span>+math.cos(math.pi*(epoch - t)/(T-t)))\n\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n\n<span class=\"hljs-comment\"># plot learningrate_decay</span>\nlr_plot = []\n<span class=\"hljs-keyword\">for</span> _i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(num_epochs):\n    <span class=\"hljs-keyword\">for</span> _j <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-built_in\">len</span>(train_iter)):\n        optimizer.step()\n        lr_plot.append(optimizer.param_groups[<span class=\"hljs-number\">0</span>][<span class=\"hljs-string\">&quot;lr&quot;</span>])\n        scheduler.step()\nplt.plot(lr_plot)</code></pre>\n<h3 id=\"should-we-do-scheduling-with-adaptive-learning-rate-method\">Should we do scheduling with adaptive learning rate method?</h3>\n<p>From <a href=\"https://stackoverflow.com/questions/39517431/should-we-do-learning-rate-decay-for-adam-optimizer\">Should we do learning rate decay for adam optimizer</a>?, I found it as a arguable question.</p>\n<blockquote>\n<p>It depends. ADAM updates any parameter with an individual learning rate. This means that every parameter in the network has a specific learning rate associated.</p>\n<p>But* the single learning rate for each parameter is computed using lambda (the initial learning rate) as an upper limit. This means that every single learning rate can vary from 0 (no update) to lambda (maximum update).</p>\n<p>It's true, that the learning rates adapt themselves during training steps, but if you want to be sure that every update step doesn't exceed lambda you can than lower lambda using exponential decay or whatever. It can help to reduce loss during the latest step of training, when the computed loss with the previously associated lambda parameter has stopped to decrease.</p>\n</blockquote>\n<blockquote>\n<p>In my experience it usually not necessary to do learning rate decay with Adam optimizer.</p>\n<p>The theory is that Adam already handles learning rate optimization (<a href=\"http://arxiv.org/pdf/1412.6980v8.pdf\">check reference</a>) :</p>\n<blockquote>\n<p>\"We propose Adam, a method for efficient stochastic optimization that only requires first-order gradients with little memory requirement. The method <strong>computes individual adaptive learning rates</strong> for different parameters from estimates of first and second moments of the gradients; the name Adam is derived from adaptive moment estimation.\"</p>\n</blockquote>\n<p>As with any deep learning problem YMMV, one size does not fit all, you should try different approaches and see what works for you, etc. etc.</p>\n</blockquote>\n<blockquote>\n<p>Yes, absolutely. From my own experience, it's very useful to Adam with learning rate decay. Without decay, you have to set a very small learning rate so the loss won't begin to diverge after decrease to a point.</p>\n</blockquote>\n<p>But in the article <a href=\"https://arxiv.org/abs/1711.05101\">Decoupled weight decay regularization</a>(AdamW), it is encouraged.</p>\n<blockquote>\n<p>Adam can substantially benefit from a scheduled learning rate multiplier. The fact that Adam is an adaptive gradient algorithm and as such adapts the learning rate for each parameter does not rule out the possibility to substantially improve its performance by using a global learning rate multiplier, scheduled, e.g., by cosine annealing.</p>\n</blockquote>\n<p>In the CLR article, the authors encourage the combination of CLR methods with Adam as well.</p>\n<blockquote>\n<p>Adaptive learning rates are fundamentally different from CLR policies, and CLR can be combined with adaptive learning rates, as shown in Section 4.1. I</p>\n</blockquote>\n<p>All in all, theoretically, the adaptive learning rate methods such as Adam adjust the learning rate for each parameters under a upper limit as the global learning rate, which can be adjusted by scheduling.</p>\n<p>In practice, at least SGDR and CLR have been proved to be useful combining with optimizers.</p>\n<h3 id=\"experiment-adam-vs-adam-sgdr\">Experiment: Adam vs Adam + SGDR</h3>\n<p>In this little experiment, the best setting in the last article is set as baseline, with Adam with constant learning rate. Leave other settings, Adam with cosine annealing learning rate, and AdamW with cosine annealing learning rate are compared.</p>\n<p><code>global learning rate = 0.005</code></p>\n<p><code>scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = **int**(num_epochs***len**(train_iter)/10), T_mult=1, eta_min=1e-9)</code></p>\n<p><img src=\"experiment.png\" alt=\"experiment\" style=\"zoom:80%;\" /></p>\n<p>As shown in the line charts, SGDR lift both the training and test accuracies. And the overfitting of the baseline method is alleviated as well.</p>\n<p>In the second and sub-figure, the fluctuation in the process of gradient descend caused by the cosine learning rate is obvious. And after each learning rate restart, the rate of the descend also gets a restart. And it takes fewer epochs than to get the same accuracy than the baseline.</p>\n<h3 id=\"reference\">Reference</h3>\n<p><a href=\"https://arxiv.org/pdf/1412.6980.pdf\">Kingma, D. P., &amp; Ba, J. (2014). Adam: A method for stochastic optimization. <em>arXiv preprint arXiv:1412.6980</em>.</a></p>\n<p><a href=\"https://wiki.tum.de/display/lfdv/Adaptive+Learning+Rate+Method\">Adaptive Learning Rate Method</a></p>\n<p><a href=\"https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1\">Learning Rate Schedules and Adaptive Learning Rate Methods</a></p>\n<p><a href=\"https://medium.com/analytics-vidhya/learning-rate-decay-and-methods-in-deep-learning-2cee564f910b#:~:text=Learning%20rate%20decay%20is%20a,help%20both%20optimization%20and%20generalization.\">Learning Rate Decay and methods in Deep Learning</a></p>\n<p><a href=\"https://towardsdatascience.com/https-medium-com-reina-wang-tw-stochastic-gradient-descent-with-restarts-5f511975163\">A Newbie’s Guide to Stochastic Gradient Descent With Restarts</a></p>\n<p><a href=\"https://arxiv.org/abs/1605.07146\">Zagoruyko, S., &amp; Komodakis, N. (2016). Wide residual networks. <em>arXiv preprint arXiv:1605.07146</em>.</a></p>\n<p><a href=\"https://arxiv.org/abs/1608.03983\">Loshchilov, I., &amp; Hutter, F. (2016). Sgdr: Stochastic gradient descent with warm restarts. <em>arXiv preprint arXiv:1608.03983</em>.</a></p>\n<p><a href=\"https://ieeexplore.ieee.org/abstract/document/7926641/\">Smith, L. N. (2017, March). Cyclical learning rates for training neural networks. In <em>2017 IEEE winter conference on applications of computer vision (WACV)</em> (pp. 464-472). IEEE.</a></p>\n<p><a href=\"https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\">He, K., Zhang, X., Ren, S., &amp; Sun, J. (2016). Deep residual learning for image recognition. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> (pp. 770-778).</a></p>\n<p><a href=\"https://arxiv.org/abs/1706.0267\">Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., ... &amp; He, K. (2017). Accurate, large minibatch sgd: Training imagenet in 1 hour. <em>arXiv preprint arXiv:1706.02677</em>.</a></p>\n<p><a href=\"https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html\">Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. <em>Advances in neural information processing systems</em>, <em>25</em>.</a></p>\n<p><a href=\"https://pytorch.org/docs/stable/optim.html\">torch.optim — PyTorch 1.10 documentation</a></p>\n<p><a href=\"https://stackoverflow.com/questions/39517431/should-we-do-learning-rate-decay-for-adam-optimizer\">Should we do learning rate decay for adam optimizer</a></p>\n<p><a href=\"https://arxiv.org/abs/1711.05101\">Loshchilov, I., &amp; Hutter, F. (2017). Decoupled weight decay regularization. <em>arXiv preprint arXiv:1711.05101</em>.</a></p>\n<p><a href=\"https://www.kaggle.com/isbhargav/guide-to-pytorch-learning-rate-scheduling\">Guide to Pytorch Learning Rate Scheduling</a></p>","wordcount":11090},{"title":"install d2l module on apple m1 chip for deep learning","author":"Ryan LI","toc":true,"declare":true,"date":"2022-02-28T14:30:11.000Z","_content":"\n> [d2l](https://pypi.org/project/d2l/) is a small python module wheel that needed when read the useful deep learning book \"[dive into deeplearning](https://d2l.ai/)\", which provide interactive code examples implemented with [MXNet](https://mxnet.apache.org/versions/1.9.0/), PyTorch, and Tensorflow. \n>\n> But it took me ton's of time installing this module on the new M1 MacBook Air. Actually its easy, just to record this.\n\n<!-- more -->\n\n### How to install\n\n1.install miniforge\n\nalready did, easy.\n\n2.create a new environment with python=3.8\n\nm1 Mac officially support python>=3.9, but 3.8 can be installed.\n\n```shell\nconda create -n d2l python=3.8\nconda info --env\nconda activate d2l\n```\n\n3.install torch\n\ntorch==1.8.1 and torchvision==0.9.1 is recommended and tested in the book, but [# macOS is not currently supported for lts](https://pytorch.org/). \n\nSo the most convenient choice for mac is pytorch==1.10.2, torchvision==0.2.2\n\n```python\nconda install pytorch torchvision -c pytorch\n```\n\n#4.try install d2l directly\n\n```shell\nclear\npip install d2l==0.17.3\n```\n\nthousands lines of terrifying error will come out:\n\n```shell\n$ pip install d2l==0.17.3\nCollecting d2l==0.17.3\n  Using cached d2l-0.17.3-py3-none-any.whl (82 kB)\nCollecting jupyter==1.0.0\n  Using cached jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\nCollecting numpy==1.18.5\n  Using cached numpy-1.18.5.zip (5.4 MB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nCollecting pandas==1.2.2\n  Using cached pandas-1.2.2.tar.gz (5.5 MB)\n  Installing build dependencies ... error\n  error: subprocess-exited-with-error\n  \n  × pip subprocess to install build dependencies did not run successfully.\n  │ exit code: 1\n  ╰─> [3659 lines of output]\n      Ignoring numpy: markers 'python_version == \"3.7\" and platform_system != \"AIX\"' don't match your environment\n      Ignoring numpy: markers 'python_version == \"3.7\" and platform_system == \"AIX\"' don't match your environment\n      Ignoring numpy: markers 'python_version == \"3.8\" and platform_system == \"AIX\"' don't match your environment\n      Ignoring numpy: markers 'python_version >= \"3.9\"' don't match your environment\n      Collecting setuptools\n        Using cached setuptools-60.9.3-py3-none-any.whl (1.1 MB)\n      Collecting wheel\n        Using cached wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n      Collecting Cython<3,>=0.29.21\n        Using cached Cython-0.29.28-py2.py3-none-any.whl (983 kB)\n      Collecting numpy==1.17.3\n        Using cached numpy-1.17.3.zip (6.4 MB)\n        Preparing metadata (setup.py): started\n        Preparing metadata (setup.py): finished with status 'done'\n      Building wheels for collected packages: numpy\n        Building wheel for numpy (setup.py): started\n        Building wheel for numpy (setup.py): finished with status 'error'\n        error: subprocess-exited-with-error\n      \n        × python setup.py bdist_wheel did not run successfully.\n        │ exit code: 1\n        ╰─> [3286 lines of output]\n            Running from numpy source directory.\n            blas_opt_info:\n            blas_mkl_info:\n            customize UnixCCompiler\n              libraries mkl_rt not found in ['/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib', '/usr/local/lib', '/usr/lib']\n              NOT AVAILABLE\n      \n            blis_info:\n            customize UnixCCompiler\n              libraries blis not found in ['/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib', '/usr/local/lib', '/usr/lib']\n              NOT AVAILABLE\n      \n            openblas_info:\n            customize UnixCCompiler\n            customize UnixCCompiler\n              libraries openblas not found in ['/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib', '/usr/local/lib', '/usr/lib']\n              NOT AVAILABLE\n      \n            atlas_3_10_blas_threads_info:\n            Setting PTATLAS=ATLAS\n            customize UnixCCompiler\n              libraries tatlas not found in ['/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib', '/usr/local/lib', '/usr/lib']\n              NOT AVAILABLE\n      \n            atlas_3_10_blas_info:\n            customize UnixCCompiler\n              libraries satlas not found in ['/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib', '/usr/local/lib', '/usr/lib']\n              NOT AVAILABLE\n      \n            atlas_blas_threads_info:\n            Setting PTATLAS=ATLAS\n            customize UnixCCompiler\n              libraries ptf77blas,ptcblas,atlas not found in ['/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib', '/usr/local/lib', '/usr/lib']\n              NOT AVAILABLE\n      \n            atlas_blas_info:\n            customize UnixCCompiler\n              libraries f77blas,cblas,atlas not found in ['/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib', '/usr/local/lib', '/usr/lib']\n              NOT AVAILABLE\n      \n            accelerate_info:\n            customize UnixCCompiler\n              libraries accelerate not found in ['/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib', '/usr/local/lib', '/usr/lib']\n            Library accelerate was not found. Ignoring\n            customize UnixCCompiler\n              libraries veclib not found in ['/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib', '/usr/local/lib', '/usr/lib']\n            Library veclib was not found. Ignoring\n              FOUND:\n                extra_compile_args = ['-faltivec', '-I/System/Library/Frameworks/vecLib.framework/Headers']\n                extra_link_args = ['-Wl,-framework', '-Wl,Accelerate']\n                define_macros = [('NO_ATLAS_INFO', 3), ('HAVE_CBLAS', None)]\n      \n              FOUND:\n                extra_compile_args = ['-faltivec', '-I/System/Library/Frameworks/vecLib.framework/Headers']\n                extra_link_args = ['-Wl,-framework', '-Wl,Accelerate']\n                define_macros = [('NO_ATLAS_INFO', 3), ('HAVE_CBLAS', None)]\n      \n            /bin/sh: svnversion: command not found\n            non-existing path in 'numpy/distutils': 'site.cfg'\n            lapack_opt_info:\n            lapack_mkl_info:\n            customize UnixCCompiler\n              libraries mkl_rt not found in ['/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib', '/usr/local/lib', '/usr/lib']\n              NOT AVAILABLE\n      \n            openblas_lapack_info:\n            customize UnixCCompiler\n            customize UnixCCompiler\n              libraries openblas not found in ['/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib', '/usr/local/lib', '/usr/lib']\n              NOT AVAILABLE\n      \n            openblas_clapack_info:\n            customize UnixCCompiler\n            customize UnixCCompiler\n              libraries openblas,lapack not found in ['/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib', '/usr/local/lib', '/usr/lib']\n              NOT AVAILABLE\n      \n            flame_info:\n            customize UnixCCompiler\n              libraries flame not found in ['/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib', '/usr/local/lib', '/usr/lib']\n              NOT AVAILABLE\n      \n            atlas_3_10_threads_info:\n            Setting PTATLAS=ATLAS\n            customize UnixCCompiler\n              libraries lapack_atlas not found in /opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib\n...\n...\n...\n\t\n\t\t\t\t\t\tNone - nothing done with h_files = ['build/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath/npy_math_internal.h']\n            building library \"npysort\" sources\n              adding 'build/src.macosx-11.0-arm64-3.8/numpy/core/src/common' to include_dirs.\n            None - nothing done with h_files = ['build/src.macosx-11.0-arm64-3.8/numpy/core/src/common/npy_sort.h', 'build/src.macosx-11.0-arm64-3.8/numpy/core/src/common/npy_partition.h', 'build/src.macosx-11.0-arm64-3.8/numpy/core/src/common/npy_binsearch.h']\n            building extension \"numpy.core._dummy\" sources\n              adding 'build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/config.h' to sources.\n              adding 'build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/_numpyconfig.h' to sources.\n            executing numpy/core/code_generators/generate_numpy_api.py\n              adding 'build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/__multiarray_api.h' to sources.\n            numpy.core - nothing done with h_files = ['build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/config.h', 'build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/_numpyconfig.h', 'build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/__multiarray_api.h']\n            building extension \"numpy.core._multiarray_tests\" sources\n            building extension \"numpy.core._multiarray_umath\" sources\n              adding 'build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/config.h' to sources.\n              adding 'build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/_numpyconfig.h' to sources.\n            executing numpy/core/code_generators/generate_numpy_api.py\n              adding 'build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/__multiarray_api.h' to sources.\n            executing numpy/core/code_generators/generate_ufunc_api.py\n              adding 'build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/__ufunc_api.h' to sources.\n              adding 'build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath' to include_dirs.\n              adding 'build/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath' to include_dirs.\n              adding 'build/src.macosx-11.0-arm64-3.8/numpy/core/src/common' to include_dirs.\n            numpy.core - nothing done with h_files = ['build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath/funcs.inc', 'build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath/simd.inc', 'build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath/loops.h', 'build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath/matmul.h', 'build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath/clip.h', 'build/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath/npy_math_internal.h', 'build/src.macosx-11.0-arm64-3.8/numpy/core/src/common/templ_common.h', 'build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/config.h', 'build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/_numpyconfig.h', 'build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/__multiarray_api.h', 'build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/__ufunc_api.h']\n            building extension \"numpy.core._umath_tests\" sources\n            building extension \"numpy.core._rational_tests\" sources\n            building extension \"numpy.core._struct_ufunc_tests\" sources\n            building extension \"numpy.core._operand_flag_tests\" sources\n            building extension \"numpy.fft._pocketfft_internal\" sources\n            building extension \"numpy.linalg.lapack_lite\" sources\n              adding 'numpy/linalg/lapack_lite/python_xerbla.c' to sources.\n            building extension \"numpy.linalg._umath_linalg\" sources\n              adding 'numpy/linalg/lapack_lite/python_xerbla.c' to sources.\n            building extension \"numpy.random.mt19937\" sources\n            building extension \"numpy.random.philox\" sources\n            building extension \"numpy.random.pcg64\" sources\n            building extension \"numpy.random.sfc64\" sources\n            building extension \"numpy.random.common\" sources\n            building extension \"numpy.random.bit_generator\" sources\n            building extension \"numpy.random.generator\" sources\n            building extension \"numpy.random.bounded_integers\" sources\n            building extension \"numpy.random.mtrand\" sources\n            building data_files sources\n            build_src: building npy-pkg config files\n            running build_py\n            copying numpy/version.py -> build/lib.macosx-11.0-arm64-3.8/numpy\n            copying build/src.macosx-11.0-arm64-3.8/numpy/__config__.py -> build/lib.macosx-11.0-arm64-3.8/numpy\n            copying build/src.macosx-11.0-arm64-3.8/numpy/distutils/__config__.py -> build/lib.macosx-11.0-arm64-3.8/numpy/distutils\n            running build_clib\n            customize UnixCCompiler\n            customize UnixCCompiler using build_clib\n            running build_ext\n            customize UnixCCompiler\n            customize UnixCCompiler using build_ext\n            building 'numpy.core._multiarray_umath' extension\n            compiling C sources\n            C compiler: gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/opt/homebrew/Caskroom/miniforge/base/envs/d2d/include -arch arm64 -I/opt/homebrew/Caskroom/miniforge/base/envs/d2d/include -arch arm64\n      \n            compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -DNO_ATLAS_INFO=3 -DHAVE_CBLAS -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/umath -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/common -Inumpy/core/include -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/opt/homebrew/Caskroom/miniforge/base/envs/d2d/include/python3.8 -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath -c'\n            extra options: '-faltivec -I/System/Library/Frameworks/vecLib.framework/Headers'\n            gcc: numpy/core/src/multiarray/alloc.c\n            gcc: numpy/core/src/multiarray/array_assign_scalar.c\n            gcc: numpy/core/src/multiarray/buffer.c\n            gcc: numpy/core/src/multiarray/common.c\n            gcc: numpy/core/src/multiarray/conversion_utils.c\n            gcc: numpy/core/src/multiarray/datetime_strings.c\n            gcc: numpy/core/src/multiarray/descriptor.c\n            gcc: build/src.macosx-11.0-arm64-3.8/numpy/core/src/multiarray/einsum.c\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            gcc: numpy/core/src/multiarray/hashdescr.c\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            gcc: build/src.macosx-11.0-arm64-3.8/numpy/core/src/multiarray/lowlevel_strided_loops.c\n            gcc: numpy/core/src/multiarray/multiarraymodule.c\n            gcc: numpy/core/src/multiarray/nditer_constr.c\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            gcc: numpy/core/src/multiarray/refcount.c\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            gcc: numpy/core/src/multiarray/scalarapi.c\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            gcc: numpy/core/src/multiarray/temp_elide.c\n            gcc: numpy/core/src/multiarray/vdot.c\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            gcc: build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath/loops.c\n            gcc: numpy/core/src/umath/ufunc_object.c\n            gcc: build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath/scalarmath.c\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            gcc: numpy/core/src/npymath/npy_math.c\n            gcc: numpy/core/src/common/npy_longdouble.c\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            gcc: numpy/core/src/npymath/halffloat.c\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            gcc: numpy/core/src/common/numpyos.c\n            gcc: /private/var/folders/5y/pqfqz2md0njg2slq29yxp12w0000gn/T/pip-install-mxyh83f9/numpy_51614e6143884c3bbd246341eeb3b857/numpy/_build_utils/src/apple_sgemv_fix.c\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitlyclang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n      \n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            error: Command \"gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/opt/homebrew/Caskroom/miniforge/base/envs/d2d/include -arch arm64 -I/opt/homebrew/Caskroom/miniforge/base/envs/d2d/include -arch arm64 -DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -DNO_ATLAS_INFO=3 -DHAVE_CBLAS -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/umath -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/common -Inumpy/core/include -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/opt/homebrew/Caskroom/miniforge/base/envs/d2d/include/python3.8 -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath -c numpy/core/src/multiarray/array_assign_scalar.c -o build/temp.macosx-11.0-arm64-3.8/numpy/core/src/multiarray/array_assign_scalar.o -MMD -MF build/temp.macosx-11.0-arm64-3.8/numpy/core/src/multiarray/array_assign_scalar.o.d -faltivec -I/System/Library/Frameworks/vecLib.framework/Headers\" failed with exit status 1\n            [end of output]\n      \n        note: This error originates from a subprocess, and is likely not a problem with pip.\n      error: legacy-install-failure\n      \n      × Encountered error while trying to install package.\n      ╰─> numpy\n      \n      note: This is an issue with the package mentioned above, not pip.\n      hint: See above for output from the failure.\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: subprocess-exited-with-error\n\n× pip subprocess to install build dependencies did not run successfully.\n│ exit code: 1\n╰─> See above for output.\n\nnote: This error originates from a subprocess, and is likely not a problem with pip.\n```\n\nScroll to the top, it looks like `jupyter==1.0.0`, `pandas==1.2.2`, `numpy==1.18.5` are required, and numpy==1.18.5 is where the error comes from.\n\n4.As a result, install `jupyter==1.0.0` and `pandas==1.2.2` first\n\n```shell\npip install jupyter==1.0.0\n```\n\nconda can't be installed by pip, but can by conda\n\n```shell\nconda install pandas=1.2.2\n```\n\n5.numpy==1.18.5 can not be installed by pip or conda\n\nthanks to [tensorflow's wheel](https://github.com/apple/tensorflow_macos/releases/tag/v0.1alpha0), numpy==1.18.5's wheel for mac is included in the addons\n\ndownload and unzip [tensorflow_macos-0.1alpha0.tar.gz](https://github.com/apple/tensorflow_macos/releases/download/v0.1alpha0/tensorflow_macos-0.1alpha0.tar.gz)\n\ngo to the unzipped folder in terminal, and run\n\n```shell\npip install arm64/numpy-1.18.5-cp38-cp38-macosx_11_0_arm64.whl\n```\n\ncheck\n\n```shell\npip list |grep pandas\npip list |grep jupyter\npip list |grep numpy\n```\n\n```shell\npandas               1.2.2\njupyter              1.0.0\njupyter-client       7.1.2\njupyter-console      6.4.0\njupyter-core         4.9.2\njupyterlab-pygments  0.1.2\njupyterlab-widgets   1.0.2\nnumpy                1.18.5\n```\n\n5.install d2l\n\n```shell\npip install d2l\n```\n\nSucess!\n\n### Reference  \n\nhttps://zh-v2.d2l.ai/chapter_installation/index.html\n\nhttps://parthiban-kannan.medium.com/install-tensorflow-on-apple-macbook-m1-release-c1ce7e65cd0\n\nhttps://github.com/apple/tensorflow_macos/issues/48\n\n\n\n\n\n","source":"_posts/install-d2l-moudule-on-apple-m1-chip-for-deep-learning.md","raw":"---\ntitle: install d2l module on apple m1 chip for deep learning\nauthor: Ryan LI\ntoc: true\ndeclare: true\ndate: 2022-02-28 22:30:11\ntags:\n  - m1 mac\n  - deep learning\n---\n\n> [d2l](https://pypi.org/project/d2l/) is a small python module wheel that needed when read the useful deep learning book \"[dive into deeplearning](https://d2l.ai/)\", which provide interactive code examples implemented with [MXNet](https://mxnet.apache.org/versions/1.9.0/), PyTorch, and Tensorflow. \n>\n> But it took me ton's of time installing this module on the new M1 MacBook Air. Actually its easy, just to record this.\n\n<!-- more -->\n\n### How to install\n\n1.install miniforge\n\nalready did, easy.\n\n2.create a new environment with python=3.8\n\nm1 Mac officially support python>=3.9, but 3.8 can be installed.\n\n```shell\nconda create -n d2l python=3.8\nconda info --env\nconda activate d2l\n```\n\n3.install torch\n\ntorch==1.8.1 and torchvision==0.9.1 is recommended and tested in the book, but [# macOS is not currently supported for lts](https://pytorch.org/). \n\nSo the most convenient choice for mac is pytorch==1.10.2, torchvision==0.2.2\n\n```python\nconda install pytorch torchvision -c pytorch\n```\n\n#4.try install d2l directly\n\n```shell\nclear\npip install d2l==0.17.3\n```\n\nthousands lines of terrifying error will come out:\n\n```shell\n$ pip install d2l==0.17.3\nCollecting d2l==0.17.3\n  Using cached d2l-0.17.3-py3-none-any.whl (82 kB)\nCollecting jupyter==1.0.0\n  Using cached jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\nCollecting numpy==1.18.5\n  Using cached numpy-1.18.5.zip (5.4 MB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nCollecting pandas==1.2.2\n  Using cached pandas-1.2.2.tar.gz (5.5 MB)\n  Installing build dependencies ... error\n  error: subprocess-exited-with-error\n  \n  × pip subprocess to install build dependencies did not run successfully.\n  │ exit code: 1\n  ╰─> [3659 lines of output]\n      Ignoring numpy: markers 'python_version == \"3.7\" and platform_system != \"AIX\"' don't match your environment\n      Ignoring numpy: markers 'python_version == \"3.7\" and platform_system == \"AIX\"' don't match your environment\n      Ignoring numpy: markers 'python_version == \"3.8\" and platform_system == \"AIX\"' don't match your environment\n      Ignoring numpy: markers 'python_version >= \"3.9\"' don't match your environment\n      Collecting setuptools\n        Using cached setuptools-60.9.3-py3-none-any.whl (1.1 MB)\n      Collecting wheel\n        Using cached wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n      Collecting Cython<3,>=0.29.21\n        Using cached Cython-0.29.28-py2.py3-none-any.whl (983 kB)\n      Collecting numpy==1.17.3\n        Using cached numpy-1.17.3.zip (6.4 MB)\n        Preparing metadata (setup.py): started\n        Preparing metadata (setup.py): finished with status 'done'\n      Building wheels for collected packages: numpy\n        Building wheel for numpy (setup.py): started\n        Building wheel for numpy (setup.py): finished with status 'error'\n        error: subprocess-exited-with-error\n      \n        × python setup.py bdist_wheel did not run successfully.\n        │ exit code: 1\n        ╰─> [3286 lines of output]\n            Running from numpy source directory.\n            blas_opt_info:\n            blas_mkl_info:\n            customize UnixCCompiler\n              libraries mkl_rt not found in ['/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib', '/usr/local/lib', '/usr/lib']\n              NOT AVAILABLE\n      \n            blis_info:\n            customize UnixCCompiler\n              libraries blis not found in ['/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib', '/usr/local/lib', '/usr/lib']\n              NOT AVAILABLE\n      \n            openblas_info:\n            customize UnixCCompiler\n            customize UnixCCompiler\n              libraries openblas not found in ['/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib', '/usr/local/lib', '/usr/lib']\n              NOT AVAILABLE\n      \n            atlas_3_10_blas_threads_info:\n            Setting PTATLAS=ATLAS\n            customize UnixCCompiler\n              libraries tatlas not found in ['/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib', '/usr/local/lib', '/usr/lib']\n              NOT AVAILABLE\n      \n            atlas_3_10_blas_info:\n            customize UnixCCompiler\n              libraries satlas not found in ['/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib', '/usr/local/lib', '/usr/lib']\n              NOT AVAILABLE\n      \n            atlas_blas_threads_info:\n            Setting PTATLAS=ATLAS\n            customize UnixCCompiler\n              libraries ptf77blas,ptcblas,atlas not found in ['/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib', '/usr/local/lib', '/usr/lib']\n              NOT AVAILABLE\n      \n            atlas_blas_info:\n            customize UnixCCompiler\n              libraries f77blas,cblas,atlas not found in ['/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib', '/usr/local/lib', '/usr/lib']\n              NOT AVAILABLE\n      \n            accelerate_info:\n            customize UnixCCompiler\n              libraries accelerate not found in ['/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib', '/usr/local/lib', '/usr/lib']\n            Library accelerate was not found. Ignoring\n            customize UnixCCompiler\n              libraries veclib not found in ['/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib', '/usr/local/lib', '/usr/lib']\n            Library veclib was not found. Ignoring\n              FOUND:\n                extra_compile_args = ['-faltivec', '-I/System/Library/Frameworks/vecLib.framework/Headers']\n                extra_link_args = ['-Wl,-framework', '-Wl,Accelerate']\n                define_macros = [('NO_ATLAS_INFO', 3), ('HAVE_CBLAS', None)]\n      \n              FOUND:\n                extra_compile_args = ['-faltivec', '-I/System/Library/Frameworks/vecLib.framework/Headers']\n                extra_link_args = ['-Wl,-framework', '-Wl,Accelerate']\n                define_macros = [('NO_ATLAS_INFO', 3), ('HAVE_CBLAS', None)]\n      \n            /bin/sh: svnversion: command not found\n            non-existing path in 'numpy/distutils': 'site.cfg'\n            lapack_opt_info:\n            lapack_mkl_info:\n            customize UnixCCompiler\n              libraries mkl_rt not found in ['/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib', '/usr/local/lib', '/usr/lib']\n              NOT AVAILABLE\n      \n            openblas_lapack_info:\n            customize UnixCCompiler\n            customize UnixCCompiler\n              libraries openblas not found in ['/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib', '/usr/local/lib', '/usr/lib']\n              NOT AVAILABLE\n      \n            openblas_clapack_info:\n            customize UnixCCompiler\n            customize UnixCCompiler\n              libraries openblas,lapack not found in ['/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib', '/usr/local/lib', '/usr/lib']\n              NOT AVAILABLE\n      \n            flame_info:\n            customize UnixCCompiler\n              libraries flame not found in ['/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib', '/usr/local/lib', '/usr/lib']\n              NOT AVAILABLE\n      \n            atlas_3_10_threads_info:\n            Setting PTATLAS=ATLAS\n            customize UnixCCompiler\n              libraries lapack_atlas not found in /opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib\n...\n...\n...\n\t\n\t\t\t\t\t\tNone - nothing done with h_files = ['build/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath/npy_math_internal.h']\n            building library \"npysort\" sources\n              adding 'build/src.macosx-11.0-arm64-3.8/numpy/core/src/common' to include_dirs.\n            None - nothing done with h_files = ['build/src.macosx-11.0-arm64-3.8/numpy/core/src/common/npy_sort.h', 'build/src.macosx-11.0-arm64-3.8/numpy/core/src/common/npy_partition.h', 'build/src.macosx-11.0-arm64-3.8/numpy/core/src/common/npy_binsearch.h']\n            building extension \"numpy.core._dummy\" sources\n              adding 'build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/config.h' to sources.\n              adding 'build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/_numpyconfig.h' to sources.\n            executing numpy/core/code_generators/generate_numpy_api.py\n              adding 'build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/__multiarray_api.h' to sources.\n            numpy.core - nothing done with h_files = ['build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/config.h', 'build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/_numpyconfig.h', 'build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/__multiarray_api.h']\n            building extension \"numpy.core._multiarray_tests\" sources\n            building extension \"numpy.core._multiarray_umath\" sources\n              adding 'build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/config.h' to sources.\n              adding 'build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/_numpyconfig.h' to sources.\n            executing numpy/core/code_generators/generate_numpy_api.py\n              adding 'build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/__multiarray_api.h' to sources.\n            executing numpy/core/code_generators/generate_ufunc_api.py\n              adding 'build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/__ufunc_api.h' to sources.\n              adding 'build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath' to include_dirs.\n              adding 'build/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath' to include_dirs.\n              adding 'build/src.macosx-11.0-arm64-3.8/numpy/core/src/common' to include_dirs.\n            numpy.core - nothing done with h_files = ['build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath/funcs.inc', 'build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath/simd.inc', 'build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath/loops.h', 'build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath/matmul.h', 'build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath/clip.h', 'build/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath/npy_math_internal.h', 'build/src.macosx-11.0-arm64-3.8/numpy/core/src/common/templ_common.h', 'build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/config.h', 'build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/_numpyconfig.h', 'build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/__multiarray_api.h', 'build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/__ufunc_api.h']\n            building extension \"numpy.core._umath_tests\" sources\n            building extension \"numpy.core._rational_tests\" sources\n            building extension \"numpy.core._struct_ufunc_tests\" sources\n            building extension \"numpy.core._operand_flag_tests\" sources\n            building extension \"numpy.fft._pocketfft_internal\" sources\n            building extension \"numpy.linalg.lapack_lite\" sources\n              adding 'numpy/linalg/lapack_lite/python_xerbla.c' to sources.\n            building extension \"numpy.linalg._umath_linalg\" sources\n              adding 'numpy/linalg/lapack_lite/python_xerbla.c' to sources.\n            building extension \"numpy.random.mt19937\" sources\n            building extension \"numpy.random.philox\" sources\n            building extension \"numpy.random.pcg64\" sources\n            building extension \"numpy.random.sfc64\" sources\n            building extension \"numpy.random.common\" sources\n            building extension \"numpy.random.bit_generator\" sources\n            building extension \"numpy.random.generator\" sources\n            building extension \"numpy.random.bounded_integers\" sources\n            building extension \"numpy.random.mtrand\" sources\n            building data_files sources\n            build_src: building npy-pkg config files\n            running build_py\n            copying numpy/version.py -> build/lib.macosx-11.0-arm64-3.8/numpy\n            copying build/src.macosx-11.0-arm64-3.8/numpy/__config__.py -> build/lib.macosx-11.0-arm64-3.8/numpy\n            copying build/src.macosx-11.0-arm64-3.8/numpy/distutils/__config__.py -> build/lib.macosx-11.0-arm64-3.8/numpy/distutils\n            running build_clib\n            customize UnixCCompiler\n            customize UnixCCompiler using build_clib\n            running build_ext\n            customize UnixCCompiler\n            customize UnixCCompiler using build_ext\n            building 'numpy.core._multiarray_umath' extension\n            compiling C sources\n            C compiler: gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/opt/homebrew/Caskroom/miniforge/base/envs/d2d/include -arch arm64 -I/opt/homebrew/Caskroom/miniforge/base/envs/d2d/include -arch arm64\n      \n            compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -DNO_ATLAS_INFO=3 -DHAVE_CBLAS -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/umath -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/common -Inumpy/core/include -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/opt/homebrew/Caskroom/miniforge/base/envs/d2d/include/python3.8 -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath -c'\n            extra options: '-faltivec -I/System/Library/Frameworks/vecLib.framework/Headers'\n            gcc: numpy/core/src/multiarray/alloc.c\n            gcc: numpy/core/src/multiarray/array_assign_scalar.c\n            gcc: numpy/core/src/multiarray/buffer.c\n            gcc: numpy/core/src/multiarray/common.c\n            gcc: numpy/core/src/multiarray/conversion_utils.c\n            gcc: numpy/core/src/multiarray/datetime_strings.c\n            gcc: numpy/core/src/multiarray/descriptor.c\n            gcc: build/src.macosx-11.0-arm64-3.8/numpy/core/src/multiarray/einsum.c\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            gcc: numpy/core/src/multiarray/hashdescr.c\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            gcc: build/src.macosx-11.0-arm64-3.8/numpy/core/src/multiarray/lowlevel_strided_loops.c\n            gcc: numpy/core/src/multiarray/multiarraymodule.c\n            gcc: numpy/core/src/multiarray/nditer_constr.c\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            gcc: numpy/core/src/multiarray/refcount.c\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            gcc: numpy/core/src/multiarray/scalarapi.c\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            gcc: numpy/core/src/multiarray/temp_elide.c\n            gcc: numpy/core/src/multiarray/vdot.c\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            gcc: build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath/loops.c\n            gcc: numpy/core/src/umath/ufunc_object.c\n            gcc: build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath/scalarmath.c\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            gcc: numpy/core/src/npymath/npy_math.c\n            gcc: numpy/core/src/common/npy_longdouble.c\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            gcc: numpy/core/src/npymath/halffloat.c\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            gcc: numpy/core/src/common/numpyos.c\n            gcc: /private/var/folders/5y/pqfqz2md0njg2slq29yxp12w0000gn/T/pip-install-mxyh83f9/numpy_51614e6143884c3bbd246341eeb3b857/numpy/_build_utils/src/apple_sgemv_fix.c\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitlyclang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n      \n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n            error: Command \"gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/opt/homebrew/Caskroom/miniforge/base/envs/d2d/include -arch arm64 -I/opt/homebrew/Caskroom/miniforge/base/envs/d2d/include -arch arm64 -DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -DNO_ATLAS_INFO=3 -DHAVE_CBLAS -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/umath -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/common -Inumpy/core/include -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/opt/homebrew/Caskroom/miniforge/base/envs/d2d/include/python3.8 -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath -c numpy/core/src/multiarray/array_assign_scalar.c -o build/temp.macosx-11.0-arm64-3.8/numpy/core/src/multiarray/array_assign_scalar.o -MMD -MF build/temp.macosx-11.0-arm64-3.8/numpy/core/src/multiarray/array_assign_scalar.o.d -faltivec -I/System/Library/Frameworks/vecLib.framework/Headers\" failed with exit status 1\n            [end of output]\n      \n        note: This error originates from a subprocess, and is likely not a problem with pip.\n      error: legacy-install-failure\n      \n      × Encountered error while trying to install package.\n      ╰─> numpy\n      \n      note: This is an issue with the package mentioned above, not pip.\n      hint: See above for output from the failure.\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: subprocess-exited-with-error\n\n× pip subprocess to install build dependencies did not run successfully.\n│ exit code: 1\n╰─> See above for output.\n\nnote: This error originates from a subprocess, and is likely not a problem with pip.\n```\n\nScroll to the top, it looks like `jupyter==1.0.0`, `pandas==1.2.2`, `numpy==1.18.5` are required, and numpy==1.18.5 is where the error comes from.\n\n4.As a result, install `jupyter==1.0.0` and `pandas==1.2.2` first\n\n```shell\npip install jupyter==1.0.0\n```\n\nconda can't be installed by pip, but can by conda\n\n```shell\nconda install pandas=1.2.2\n```\n\n5.numpy==1.18.5 can not be installed by pip or conda\n\nthanks to [tensorflow's wheel](https://github.com/apple/tensorflow_macos/releases/tag/v0.1alpha0), numpy==1.18.5's wheel for mac is included in the addons\n\ndownload and unzip [tensorflow_macos-0.1alpha0.tar.gz](https://github.com/apple/tensorflow_macos/releases/download/v0.1alpha0/tensorflow_macos-0.1alpha0.tar.gz)\n\ngo to the unzipped folder in terminal, and run\n\n```shell\npip install arm64/numpy-1.18.5-cp38-cp38-macosx_11_0_arm64.whl\n```\n\ncheck\n\n```shell\npip list |grep pandas\npip list |grep jupyter\npip list |grep numpy\n```\n\n```shell\npandas               1.2.2\njupyter              1.0.0\njupyter-client       7.1.2\njupyter-console      6.4.0\njupyter-core         4.9.2\njupyterlab-pygments  0.1.2\njupyterlab-widgets   1.0.2\nnumpy                1.18.5\n```\n\n5.install d2l\n\n```shell\npip install d2l\n```\n\nSucess!\n\n### Reference  \n\nhttps://zh-v2.d2l.ai/chapter_installation/index.html\n\nhttps://parthiban-kannan.medium.com/install-tensorflow-on-apple-macbook-m1-release-c1ce7e65cd0\n\nhttps://github.com/apple/tensorflow_macos/issues/48\n\n\n\n\n\n","slug":"install-d2l-moudule-on-apple-m1-chip-for-deep-learning","published":1,"updated":"2022-04-30T19:30:48.590Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl2n5f409000cp9yb4h2thdq1","content":"<blockquote>\n<p><a href=\"https://pypi.org/project/d2l/\">d2l</a> is a small python module wheel that needed when read the useful deep learning book \"<a href=\"https://d2l.ai/\">dive into deeplearning</a>\", which provide interactive code examples implemented with <a href=\"https://mxnet.apache.org/versions/1.9.0/\">MXNet</a>, PyTorch, and Tensorflow.</p>\n<p>But it took me ton's of time installing this module on the new M1 MacBook Air. Actually its easy, just to record this.</p>\n</blockquote>\n<span id=\"more\"></span>\n<h3 id=\"how-to-install\">How to install</h3>\n<p>1.install miniforge</p>\n<p>already did, easy.</p>\n<p>2.create a new environment with python=3.8</p>\n<p>m1 Mac officially support python&gt;=3.9, but 3.8 can be installed.</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs shell\">conda create -n d2l python=3.8\nconda info --env\nconda activate d2l</code></pre></div>\n<p>3.install torch</p>\n<p>torch==1.8.1 and torchvision==0.9.1 is recommended and tested in the book, but <a href=\"https://pytorch.org/\"># macOS is not currently supported for lts</a>.</p>\n<p>So the most convenient choice for mac is pytorch==1.10.2, torchvision==0.2.2</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\">conda install pytorch torchvision -c pytorch</code></pre></div>\n<p>#4.try install d2l directly</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs shell\">clear\npip install d2l==0.17.3</code></pre></div>\n<p>thousands lines of terrifying error will come out:</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs shell\"><span class=\"hljs-meta\">$ </span><span class=\"language-bash\">pip install d2l==0.17.3</span>\nCollecting d2l==0.17.3\n  Using cached d2l-0.17.3-py3-none-any.whl (82 kB)\nCollecting jupyter==1.0.0\n  Using cached jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\nCollecting numpy==1.18.5\n  Using cached numpy-1.18.5.zip (5.4 MB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nCollecting pandas==1.2.2\n  Using cached pandas-1.2.2.tar.gz (5.5 MB)\n  Installing build dependencies ... error\n  error: subprocess-exited-with-error\n  \n  × pip subprocess to install build dependencies did not run successfully.\n  │ exit code: 1\n  ╰─&gt; [3659 lines of output]\n      Ignoring numpy: markers &#x27;python_version == &quot;3.7&quot; and platform_system != &quot;AIX&quot;&#x27; don&#x27;t match your environment\n      Ignoring numpy: markers &#x27;python_version == &quot;3.7&quot; and platform_system == &quot;AIX&quot;&#x27; don&#x27;t match your environment\n      Ignoring numpy: markers &#x27;python_version == &quot;3.8&quot; and platform_system == &quot;AIX&quot;&#x27; don&#x27;t match your environment\n      Ignoring numpy: markers &#x27;python_version &gt;= &quot;3.9&quot;&#x27; don&#x27;t match your environment\n      Collecting setuptools\n        Using cached setuptools-60.9.3-py3-none-any.whl (1.1 MB)\n      Collecting wheel\n        Using cached wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n      Collecting Cython&lt;3,&gt;=0.29.21\n        Using cached Cython-0.29.28-py2.py3-none-any.whl (983 kB)\n      Collecting numpy==1.17.3\n        Using cached numpy-1.17.3.zip (6.4 MB)\n        Preparing metadata (setup.py): started\n        Preparing metadata (setup.py): finished with status &#x27;done&#x27;\n      Building wheels for collected packages: numpy\n        Building wheel for numpy (setup.py): started\n        Building wheel for numpy (setup.py): finished with status &#x27;error&#x27;\n        error: subprocess-exited-with-error\n      \n        × python setup.py bdist_wheel did not run successfully.\n        │ exit code: 1\n        ╰─&gt; [3286 lines of output]\n            Running from numpy source directory.\n            blas_opt_info:\n            blas_mkl_info:\n            customize UnixCCompiler\n              libraries mkl_rt not found in [&#x27;/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib&#x27;, &#x27;/usr/local/lib&#x27;, &#x27;/usr/lib&#x27;]\n              NOT AVAILABLE\n      \n            blis_info:\n            customize UnixCCompiler\n              libraries blis not found in [&#x27;/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib&#x27;, &#x27;/usr/local/lib&#x27;, &#x27;/usr/lib&#x27;]\n              NOT AVAILABLE\n      \n            openblas_info:\n            customize UnixCCompiler\n            customize UnixCCompiler\n              libraries openblas not found in [&#x27;/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib&#x27;, &#x27;/usr/local/lib&#x27;, &#x27;/usr/lib&#x27;]\n              NOT AVAILABLE\n      \n            atlas_3_10_blas_threads_info:\n            Setting PTATLAS=ATLAS\n            customize UnixCCompiler\n              libraries tatlas not found in [&#x27;/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib&#x27;, &#x27;/usr/local/lib&#x27;, &#x27;/usr/lib&#x27;]\n              NOT AVAILABLE\n      \n            atlas_3_10_blas_info:\n            customize UnixCCompiler\n              libraries satlas not found in [&#x27;/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib&#x27;, &#x27;/usr/local/lib&#x27;, &#x27;/usr/lib&#x27;]\n              NOT AVAILABLE\n      \n            atlas_blas_threads_info:\n            Setting PTATLAS=ATLAS\n            customize UnixCCompiler\n              libraries ptf77blas,ptcblas,atlas not found in [&#x27;/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib&#x27;, &#x27;/usr/local/lib&#x27;, &#x27;/usr/lib&#x27;]\n              NOT AVAILABLE\n      \n            atlas_blas_info:\n            customize UnixCCompiler\n              libraries f77blas,cblas,atlas not found in [&#x27;/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib&#x27;, &#x27;/usr/local/lib&#x27;, &#x27;/usr/lib&#x27;]\n              NOT AVAILABLE\n      \n            accelerate_info:\n            customize UnixCCompiler\n              libraries accelerate not found in [&#x27;/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib&#x27;, &#x27;/usr/local/lib&#x27;, &#x27;/usr/lib&#x27;]\n            Library accelerate was not found. Ignoring\n            customize UnixCCompiler\n              libraries veclib not found in [&#x27;/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib&#x27;, &#x27;/usr/local/lib&#x27;, &#x27;/usr/lib&#x27;]\n            Library veclib was not found. Ignoring\n              FOUND:\n                extra_compile_args = [&#x27;-faltivec&#x27;, &#x27;-I/System/Library/Frameworks/vecLib.framework/Headers&#x27;]\n                extra_link_args = [&#x27;-Wl,-framework&#x27;, &#x27;-Wl,Accelerate&#x27;]\n                define_macros = [(&#x27;NO_ATLAS_INFO&#x27;, 3), (&#x27;HAVE_CBLAS&#x27;, None)]\n      \n              FOUND:\n                extra_compile_args = [&#x27;-faltivec&#x27;, &#x27;-I/System/Library/Frameworks/vecLib.framework/Headers&#x27;]\n                extra_link_args = [&#x27;-Wl,-framework&#x27;, &#x27;-Wl,Accelerate&#x27;]\n                define_macros = [(&#x27;NO_ATLAS_INFO&#x27;, 3), (&#x27;HAVE_CBLAS&#x27;, None)]\n      \n            /bin/sh: svnversion: command not found\n            non-existing path in &#x27;numpy/distutils&#x27;: &#x27;site.cfg&#x27;\n            lapack_opt_info:\n            lapack_mkl_info:\n            customize UnixCCompiler\n              libraries mkl_rt not found in [&#x27;/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib&#x27;, &#x27;/usr/local/lib&#x27;, &#x27;/usr/lib&#x27;]\n              NOT AVAILABLE\n      \n            openblas_lapack_info:\n            customize UnixCCompiler\n            customize UnixCCompiler\n              libraries openblas not found in [&#x27;/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib&#x27;, &#x27;/usr/local/lib&#x27;, &#x27;/usr/lib&#x27;]\n              NOT AVAILABLE\n      \n            openblas_clapack_info:\n            customize UnixCCompiler\n            customize UnixCCompiler\n              libraries openblas,lapack not found in [&#x27;/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib&#x27;, &#x27;/usr/local/lib&#x27;, &#x27;/usr/lib&#x27;]\n              NOT AVAILABLE\n      \n            flame_info:\n            customize UnixCCompiler\n              libraries flame not found in [&#x27;/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib&#x27;, &#x27;/usr/local/lib&#x27;, &#x27;/usr/lib&#x27;]\n              NOT AVAILABLE\n      \n            atlas_3_10_threads_info:\n            Setting PTATLAS=ATLAS\n            customize UnixCCompiler\n              libraries lapack_atlas not found in /opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib\n...\n...\n...\n\t\n\t\t\t\t\t\tNone - nothing done with h_files = [&#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath/npy_math_internal.h&#x27;]\n            building library &quot;npysort&quot; sources\n              adding &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/common&#x27; to include_dirs.\n            None - nothing done with h_files = [&#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/common/npy_sort.h&#x27;, &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/common/npy_partition.h&#x27;, &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/common/npy_binsearch.h&#x27;]\n            building extension &quot;numpy.core._dummy&quot; sources\n              adding &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/config.h&#x27; to sources.\n              adding &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/_numpyconfig.h&#x27; to sources.\n            executing numpy/core/code_generators/generate_numpy_api.py\n              adding &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/__multiarray_api.h&#x27; to sources.\n            numpy.core - nothing done with h_files = [&#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/config.h&#x27;, &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/_numpyconfig.h&#x27;, &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/__multiarray_api.h&#x27;]\n            building extension &quot;numpy.core._multiarray_tests&quot; sources\n            building extension &quot;numpy.core._multiarray_umath&quot; sources\n              adding &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/config.h&#x27; to sources.\n              adding &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/_numpyconfig.h&#x27; to sources.\n            executing numpy/core/code_generators/generate_numpy_api.py\n              adding &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/__multiarray_api.h&#x27; to sources.\n            executing numpy/core/code_generators/generate_ufunc_api.py\n              adding &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/__ufunc_api.h&#x27; to sources.\n              adding &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath&#x27; to include_dirs.\n              adding &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath&#x27; to include_dirs.\n              adding &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/common&#x27; to include_dirs.\n            numpy.core - nothing done with h_files = [&#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath/funcs.inc&#x27;, &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath/simd.inc&#x27;, &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath/loops.h&#x27;, &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath/matmul.h&#x27;, &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath/clip.h&#x27;, &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath/npy_math_internal.h&#x27;, &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/common/templ_common.h&#x27;, &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/config.h&#x27;, &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/_numpyconfig.h&#x27;, &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/__multiarray_api.h&#x27;, &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/__ufunc_api.h&#x27;]\n            building extension &quot;numpy.core._umath_tests&quot; sources\n            building extension &quot;numpy.core._rational_tests&quot; sources\n            building extension &quot;numpy.core._struct_ufunc_tests&quot; sources\n            building extension &quot;numpy.core._operand_flag_tests&quot; sources\n            building extension &quot;numpy.fft._pocketfft_internal&quot; sources\n            building extension &quot;numpy.linalg.lapack_lite&quot; sources\n              adding &#x27;numpy/linalg/lapack_lite/python_xerbla.c&#x27; to sources.\n            building extension &quot;numpy.linalg._umath_linalg&quot; sources\n              adding &#x27;numpy/linalg/lapack_lite/python_xerbla.c&#x27; to sources.\n            building extension &quot;numpy.random.mt19937&quot; sources\n            building extension &quot;numpy.random.philox&quot; sources\n            building extension &quot;numpy.random.pcg64&quot; sources\n            building extension &quot;numpy.random.sfc64&quot; sources\n            building extension &quot;numpy.random.common&quot; sources\n            building extension &quot;numpy.random.bit_generator&quot; sources\n            building extension &quot;numpy.random.generator&quot; sources\n            building extension &quot;numpy.random.bounded_integers&quot; sources\n            building extension &quot;numpy.random.mtrand&quot; sources\n            building data_files sources\n            build_src: building npy-pkg config files\n            running build_py\n            copying numpy/version.py -&gt; build/lib.macosx-11.0-arm64-3.8/numpy\n            copying build/src.macosx-11.0-arm64-3.8/numpy/__config__.py -&gt; build/lib.macosx-11.0-arm64-3.8/numpy\n            copying build/src.macosx-11.0-arm64-3.8/numpy/distutils/__config__.py -&gt; build/lib.macosx-11.0-arm64-3.8/numpy/distutils\n            running build_clib\n            customize UnixCCompiler\n            customize UnixCCompiler using build_clib\n            running build_ext\n            customize UnixCCompiler\n            customize UnixCCompiler using build_ext\n            building &#x27;numpy.core._multiarray_umath&#x27; extension\n            compiling C sources\n            C compiler: gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/opt/homebrew/Caskroom/miniforge/base/envs/d2d/include -arch arm64 -I/opt/homebrew/Caskroom/miniforge/base/envs/d2d/include -arch arm64\n      \n            compile options: &#x27;-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -DNO_ATLAS_INFO=3 -DHAVE_CBLAS -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/umath -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/common -Inumpy/core/include -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/opt/homebrew/Caskroom/miniforge/base/envs/d2d/include/python3.8 -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath -c&#x27;\n            extra options: &#x27;-faltivec -I/System/Library/Frameworks/vecLib.framework/Headers&#x27;\n            gcc: numpy/core/src/multiarray/alloc.c\n            gcc: numpy/core/src/multiarray/array_assign_scalar.c\n            gcc: numpy/core/src/multiarray/buffer.c\n            gcc: numpy/core/src/multiarray/common.c\n            gcc: numpy/core/src/multiarray/conversion_utils.c\n            gcc: numpy/core/src/multiarray/datetime_strings.c\n            gcc: numpy/core/src/multiarray/descriptor.c\n            gcc: build/src.macosx-11.0-arm64-3.8/numpy/core/src/multiarray/einsum.c\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            gcc: numpy/core/src/multiarray/hashdescr.c\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            gcc: build/src.macosx-11.0-arm64-3.8/numpy/core/src/multiarray/lowlevel_strided_loops.c\n            gcc: numpy/core/src/multiarray/multiarraymodule.c\n            gcc: numpy/core/src/multiarray/nditer_constr.c\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            gcc: numpy/core/src/multiarray/refcount.c\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            gcc: numpy/core/src/multiarray/scalarapi.c\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            gcc: numpy/core/src/multiarray/temp_elide.c\n            gcc: numpy/core/src/multiarray/vdot.c\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            gcc: build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath/loops.c\n            gcc: numpy/core/src/umath/ufunc_object.c\n            gcc: build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath/scalarmath.c\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            gcc: numpy/core/src/npymath/npy_math.c\n            gcc: numpy/core/src/common/npy_longdouble.c\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            gcc: numpy/core/src/npymath/halffloat.c\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            gcc: numpy/core/src/common/numpyos.c\n            gcc: /private/var/folders/5y/pqfqz2md0njg2slq29yxp12w0000gn/T/pip-install-mxyh83f9/numpy_51614e6143884c3bbd246341eeb3b857/numpy/_build_utils/src/apple_sgemv_fix.c\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitlyclang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n      \n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            error: Command &quot;gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/opt/homebrew/Caskroom/miniforge/base/envs/d2d/include -arch arm64 -I/opt/homebrew/Caskroom/miniforge/base/envs/d2d/include -arch arm64 -DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -DNO_ATLAS_INFO=3 -DHAVE_CBLAS -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/umath -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/common -Inumpy/core/include -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/opt/homebrew/Caskroom/miniforge/base/envs/d2d/include/python3.8 -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath -c numpy/core/src/multiarray/array_assign_scalar.c -o build/temp.macosx-11.0-arm64-3.8/numpy/core/src/multiarray/array_assign_scalar.o -MMD -MF build/temp.macosx-11.0-arm64-3.8/numpy/core/src/multiarray/array_assign_scalar.o.d -faltivec -I/System/Library/Frameworks/vecLib.framework/Headers&quot; failed with exit status 1\n            [end of output]\n      \n        note: This error originates from a subprocess, and is likely not a problem with pip.\n      error: legacy-install-failure\n      \n      × Encountered error while trying to install package.\n      ╰─&gt; numpy\n      \n      note: This is an issue with the package mentioned above, not pip.\n      hint: See above for output from the failure.\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: subprocess-exited-with-error\n\n× pip subprocess to install build dependencies did not run successfully.\n│ exit code: 1\n╰─&gt; See above for output.\n\nnote: This error originates from a subprocess, and is likely not a problem with pip.</code></pre></div>\n<p>Scroll to the top, it looks like <code>jupyter==1.0.0</code>, <code>pandas==1.2.2</code>, <code>numpy==1.18.5</code> are required, and numpy==1.18.5 is where the error comes from.</p>\n<p>4.As a result, install <code>jupyter==1.0.0</code> and <code>pandas==1.2.2</code> first</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs shell\">pip install jupyter==1.0.0</code></pre></div>\n<p>conda can't be installed by pip, but can by conda</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs shell\">conda install pandas=1.2.2</code></pre></div>\n<p>5.numpy==1.18.5 can not be installed by pip or conda</p>\n<p>thanks to <a href=\"https://github.com/apple/tensorflow_macos/releases/tag/v0.1alpha0\">tensorflow's wheel</a>, numpy==1.18.5's wheel for mac is included in the addons</p>\n<p>download and unzip <a href=\"https://github.com/apple/tensorflow_macos/releases/download/v0.1alpha0/tensorflow_macos-0.1alpha0.tar.gz\">tensorflow_macos-0.1alpha0.tar.gz</a></p>\n<p>go to the unzipped folder in terminal, and run</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs shell\">pip install arm64/numpy-1.18.5-cp38-cp38-macosx_11_0_arm64.whl</code></pre></div>\n<p>check</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs shell\">pip list |grep pandas\npip list |grep jupyter\npip list |grep numpy</code></pre></div>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs shell\">pandas               1.2.2\njupyter              1.0.0\njupyter-client       7.1.2\njupyter-console      6.4.0\njupyter-core         4.9.2\njupyterlab-pygments  0.1.2\njupyterlab-widgets   1.0.2\nnumpy                1.18.5</code></pre></div>\n<p>5.install d2l</p>\n<div class=\"hljs code-wrapper\"><pre><code class=\"hljs shell\">pip install d2l</code></pre></div>\n<p>Sucess!</p>\n<h3 id=\"reference\">Reference</h3>\n<p>https://zh-v2.d2l.ai/chapter_installation/index.html</p>\n<p>https://parthiban-kannan.medium.com/install-tensorflow-on-apple-macbook-m1-release-c1ce7e65cd0</p>\n<p>https://github.com/apple/tensorflow_macos/issues/48</p>\n","site":{"data":{}},"excerpt":"<blockquote>\n<p><a href=\"https://pypi.org/project/d2l/\">d2l</a> is a small python module wheel that needed when read the useful deep learning book \"<a href=\"https://d2l.ai/\">dive into deeplearning</a>\", which provide interactive code examples implemented with <a href=\"https://mxnet.apache.org/versions/1.9.0/\">MXNet</a>, PyTorch, and Tensorflow.</p>\n<p>But it took me ton's of time installing this module on the new M1 MacBook Air. Actually its easy, just to record this.</p>\n</blockquote>","more":"<h3 id=\"how-to-install\">How to install</h3>\n<p>1.install miniforge</p>\n<p>already did, easy.</p>\n<p>2.create a new environment with python=3.8</p>\n<p>m1 Mac officially support python&gt;=3.9, but 3.8 can be installed.</p>\n<pre><code class=\"hljs shell\">conda create -n d2l python=3.8\nconda info --env\nconda activate d2l</code></pre>\n<p>3.install torch</p>\n<p>torch==1.8.1 and torchvision==0.9.1 is recommended and tested in the book, but <a href=\"https://pytorch.org/\"># macOS is not currently supported for lts</a>.</p>\n<p>So the most convenient choice for mac is pytorch==1.10.2, torchvision==0.2.2</p>\n<pre><code class=\"hljs python\">conda install pytorch torchvision -c pytorch</code></pre>\n<p>#4.try install d2l directly</p>\n<pre><code class=\"hljs shell\">clear\npip install d2l==0.17.3</code></pre>\n<p>thousands lines of terrifying error will come out:</p>\n<pre><code class=\"hljs shell\"><span class=\"hljs-meta\">$ </span><span class=\"language-bash\">pip install d2l==0.17.3</span>\nCollecting d2l==0.17.3\n  Using cached d2l-0.17.3-py3-none-any.whl (82 kB)\nCollecting jupyter==1.0.0\n  Using cached jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\nCollecting numpy==1.18.5\n  Using cached numpy-1.18.5.zip (5.4 MB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nCollecting pandas==1.2.2\n  Using cached pandas-1.2.2.tar.gz (5.5 MB)\n  Installing build dependencies ... error\n  error: subprocess-exited-with-error\n  \n  × pip subprocess to install build dependencies did not run successfully.\n  │ exit code: 1\n  ╰─&gt; [3659 lines of output]\n      Ignoring numpy: markers &#x27;python_version == &quot;3.7&quot; and platform_system != &quot;AIX&quot;&#x27; don&#x27;t match your environment\n      Ignoring numpy: markers &#x27;python_version == &quot;3.7&quot; and platform_system == &quot;AIX&quot;&#x27; don&#x27;t match your environment\n      Ignoring numpy: markers &#x27;python_version == &quot;3.8&quot; and platform_system == &quot;AIX&quot;&#x27; don&#x27;t match your environment\n      Ignoring numpy: markers &#x27;python_version &gt;= &quot;3.9&quot;&#x27; don&#x27;t match your environment\n      Collecting setuptools\n        Using cached setuptools-60.9.3-py3-none-any.whl (1.1 MB)\n      Collecting wheel\n        Using cached wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n      Collecting Cython&lt;3,&gt;=0.29.21\n        Using cached Cython-0.29.28-py2.py3-none-any.whl (983 kB)\n      Collecting numpy==1.17.3\n        Using cached numpy-1.17.3.zip (6.4 MB)\n        Preparing metadata (setup.py): started\n        Preparing metadata (setup.py): finished with status &#x27;done&#x27;\n      Building wheels for collected packages: numpy\n        Building wheel for numpy (setup.py): started\n        Building wheel for numpy (setup.py): finished with status &#x27;error&#x27;\n        error: subprocess-exited-with-error\n      \n        × python setup.py bdist_wheel did not run successfully.\n        │ exit code: 1\n        ╰─&gt; [3286 lines of output]\n            Running from numpy source directory.\n            blas_opt_info:\n            blas_mkl_info:\n            customize UnixCCompiler\n              libraries mkl_rt not found in [&#x27;/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib&#x27;, &#x27;/usr/local/lib&#x27;, &#x27;/usr/lib&#x27;]\n              NOT AVAILABLE\n      \n            blis_info:\n            customize UnixCCompiler\n              libraries blis not found in [&#x27;/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib&#x27;, &#x27;/usr/local/lib&#x27;, &#x27;/usr/lib&#x27;]\n              NOT AVAILABLE\n      \n            openblas_info:\n            customize UnixCCompiler\n            customize UnixCCompiler\n              libraries openblas not found in [&#x27;/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib&#x27;, &#x27;/usr/local/lib&#x27;, &#x27;/usr/lib&#x27;]\n              NOT AVAILABLE\n      \n            atlas_3_10_blas_threads_info:\n            Setting PTATLAS=ATLAS\n            customize UnixCCompiler\n              libraries tatlas not found in [&#x27;/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib&#x27;, &#x27;/usr/local/lib&#x27;, &#x27;/usr/lib&#x27;]\n              NOT AVAILABLE\n      \n            atlas_3_10_blas_info:\n            customize UnixCCompiler\n              libraries satlas not found in [&#x27;/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib&#x27;, &#x27;/usr/local/lib&#x27;, &#x27;/usr/lib&#x27;]\n              NOT AVAILABLE\n      \n            atlas_blas_threads_info:\n            Setting PTATLAS=ATLAS\n            customize UnixCCompiler\n              libraries ptf77blas,ptcblas,atlas not found in [&#x27;/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib&#x27;, &#x27;/usr/local/lib&#x27;, &#x27;/usr/lib&#x27;]\n              NOT AVAILABLE\n      \n            atlas_blas_info:\n            customize UnixCCompiler\n              libraries f77blas,cblas,atlas not found in [&#x27;/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib&#x27;, &#x27;/usr/local/lib&#x27;, &#x27;/usr/lib&#x27;]\n              NOT AVAILABLE\n      \n            accelerate_info:\n            customize UnixCCompiler\n              libraries accelerate not found in [&#x27;/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib&#x27;, &#x27;/usr/local/lib&#x27;, &#x27;/usr/lib&#x27;]\n            Library accelerate was not found. Ignoring\n            customize UnixCCompiler\n              libraries veclib not found in [&#x27;/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib&#x27;, &#x27;/usr/local/lib&#x27;, &#x27;/usr/lib&#x27;]\n            Library veclib was not found. Ignoring\n              FOUND:\n                extra_compile_args = [&#x27;-faltivec&#x27;, &#x27;-I/System/Library/Frameworks/vecLib.framework/Headers&#x27;]\n                extra_link_args = [&#x27;-Wl,-framework&#x27;, &#x27;-Wl,Accelerate&#x27;]\n                define_macros = [(&#x27;NO_ATLAS_INFO&#x27;, 3), (&#x27;HAVE_CBLAS&#x27;, None)]\n      \n              FOUND:\n                extra_compile_args = [&#x27;-faltivec&#x27;, &#x27;-I/System/Library/Frameworks/vecLib.framework/Headers&#x27;]\n                extra_link_args = [&#x27;-Wl,-framework&#x27;, &#x27;-Wl,Accelerate&#x27;]\n                define_macros = [(&#x27;NO_ATLAS_INFO&#x27;, 3), (&#x27;HAVE_CBLAS&#x27;, None)]\n      \n            /bin/sh: svnversion: command not found\n            non-existing path in &#x27;numpy/distutils&#x27;: &#x27;site.cfg&#x27;\n            lapack_opt_info:\n            lapack_mkl_info:\n            customize UnixCCompiler\n              libraries mkl_rt not found in [&#x27;/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib&#x27;, &#x27;/usr/local/lib&#x27;, &#x27;/usr/lib&#x27;]\n              NOT AVAILABLE\n      \n            openblas_lapack_info:\n            customize UnixCCompiler\n            customize UnixCCompiler\n              libraries openblas not found in [&#x27;/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib&#x27;, &#x27;/usr/local/lib&#x27;, &#x27;/usr/lib&#x27;]\n              NOT AVAILABLE\n      \n            openblas_clapack_info:\n            customize UnixCCompiler\n            customize UnixCCompiler\n              libraries openblas,lapack not found in [&#x27;/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib&#x27;, &#x27;/usr/local/lib&#x27;, &#x27;/usr/lib&#x27;]\n              NOT AVAILABLE\n      \n            flame_info:\n            customize UnixCCompiler\n              libraries flame not found in [&#x27;/opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib&#x27;, &#x27;/usr/local/lib&#x27;, &#x27;/usr/lib&#x27;]\n              NOT AVAILABLE\n      \n            atlas_3_10_threads_info:\n            Setting PTATLAS=ATLAS\n            customize UnixCCompiler\n              libraries lapack_atlas not found in /opt/homebrew/Caskroom/miniforge/base/envs/d2d/lib\n...\n...\n...\n\t\n\t\t\t\t\t\tNone - nothing done with h_files = [&#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath/npy_math_internal.h&#x27;]\n            building library &quot;npysort&quot; sources\n              adding &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/common&#x27; to include_dirs.\n            None - nothing done with h_files = [&#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/common/npy_sort.h&#x27;, &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/common/npy_partition.h&#x27;, &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/common/npy_binsearch.h&#x27;]\n            building extension &quot;numpy.core._dummy&quot; sources\n              adding &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/config.h&#x27; to sources.\n              adding &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/_numpyconfig.h&#x27; to sources.\n            executing numpy/core/code_generators/generate_numpy_api.py\n              adding &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/__multiarray_api.h&#x27; to sources.\n            numpy.core - nothing done with h_files = [&#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/config.h&#x27;, &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/_numpyconfig.h&#x27;, &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/__multiarray_api.h&#x27;]\n            building extension &quot;numpy.core._multiarray_tests&quot; sources\n            building extension &quot;numpy.core._multiarray_umath&quot; sources\n              adding &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/config.h&#x27; to sources.\n              adding &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/_numpyconfig.h&#x27; to sources.\n            executing numpy/core/code_generators/generate_numpy_api.py\n              adding &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/__multiarray_api.h&#x27; to sources.\n            executing numpy/core/code_generators/generate_ufunc_api.py\n              adding &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/__ufunc_api.h&#x27; to sources.\n              adding &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath&#x27; to include_dirs.\n              adding &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath&#x27; to include_dirs.\n              adding &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/common&#x27; to include_dirs.\n            numpy.core - nothing done with h_files = [&#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath/funcs.inc&#x27;, &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath/simd.inc&#x27;, &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath/loops.h&#x27;, &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath/matmul.h&#x27;, &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath/clip.h&#x27;, &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath/npy_math_internal.h&#x27;, &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/src/common/templ_common.h&#x27;, &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/config.h&#x27;, &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/_numpyconfig.h&#x27;, &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/__multiarray_api.h&#x27;, &#x27;build/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy/__ufunc_api.h&#x27;]\n            building extension &quot;numpy.core._umath_tests&quot; sources\n            building extension &quot;numpy.core._rational_tests&quot; sources\n            building extension &quot;numpy.core._struct_ufunc_tests&quot; sources\n            building extension &quot;numpy.core._operand_flag_tests&quot; sources\n            building extension &quot;numpy.fft._pocketfft_internal&quot; sources\n            building extension &quot;numpy.linalg.lapack_lite&quot; sources\n              adding &#x27;numpy/linalg/lapack_lite/python_xerbla.c&#x27; to sources.\n            building extension &quot;numpy.linalg._umath_linalg&quot; sources\n              adding &#x27;numpy/linalg/lapack_lite/python_xerbla.c&#x27; to sources.\n            building extension &quot;numpy.random.mt19937&quot; sources\n            building extension &quot;numpy.random.philox&quot; sources\n            building extension &quot;numpy.random.pcg64&quot; sources\n            building extension &quot;numpy.random.sfc64&quot; sources\n            building extension &quot;numpy.random.common&quot; sources\n            building extension &quot;numpy.random.bit_generator&quot; sources\n            building extension &quot;numpy.random.generator&quot; sources\n            building extension &quot;numpy.random.bounded_integers&quot; sources\n            building extension &quot;numpy.random.mtrand&quot; sources\n            building data_files sources\n            build_src: building npy-pkg config files\n            running build_py\n            copying numpy/version.py -&gt; build/lib.macosx-11.0-arm64-3.8/numpy\n            copying build/src.macosx-11.0-arm64-3.8/numpy/__config__.py -&gt; build/lib.macosx-11.0-arm64-3.8/numpy\n            copying build/src.macosx-11.0-arm64-3.8/numpy/distutils/__config__.py -&gt; build/lib.macosx-11.0-arm64-3.8/numpy/distutils\n            running build_clib\n            customize UnixCCompiler\n            customize UnixCCompiler using build_clib\n            running build_ext\n            customize UnixCCompiler\n            customize UnixCCompiler using build_ext\n            building &#x27;numpy.core._multiarray_umath&#x27; extension\n            compiling C sources\n            C compiler: gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/opt/homebrew/Caskroom/miniforge/base/envs/d2d/include -arch arm64 -I/opt/homebrew/Caskroom/miniforge/base/envs/d2d/include -arch arm64\n      \n            compile options: &#x27;-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -DNO_ATLAS_INFO=3 -DHAVE_CBLAS -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/umath -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/common -Inumpy/core/include -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/opt/homebrew/Caskroom/miniforge/base/envs/d2d/include/python3.8 -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath -c&#x27;\n            extra options: &#x27;-faltivec -I/System/Library/Frameworks/vecLib.framework/Headers&#x27;\n            gcc: numpy/core/src/multiarray/alloc.c\n            gcc: numpy/core/src/multiarray/array_assign_scalar.c\n            gcc: numpy/core/src/multiarray/buffer.c\n            gcc: numpy/core/src/multiarray/common.c\n            gcc: numpy/core/src/multiarray/conversion_utils.c\n            gcc: numpy/core/src/multiarray/datetime_strings.c\n            gcc: numpy/core/src/multiarray/descriptor.c\n            gcc: build/src.macosx-11.0-arm64-3.8/numpy/core/src/multiarray/einsum.c\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            gcc: numpy/core/src/multiarray/hashdescr.c\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            gcc: build/src.macosx-11.0-arm64-3.8/numpy/core/src/multiarray/lowlevel_strided_loops.c\n            gcc: numpy/core/src/multiarray/multiarraymodule.c\n            gcc: numpy/core/src/multiarray/nditer_constr.c\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            gcc: numpy/core/src/multiarray/refcount.c\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            gcc: numpy/core/src/multiarray/scalarapi.c\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            gcc: numpy/core/src/multiarray/temp_elide.c\n            gcc: numpy/core/src/multiarray/vdot.c\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            gcc: build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath/loops.c\n            gcc: numpy/core/src/umath/ufunc_object.c\n            gcc: build/src.macosx-11.0-arm64-3.8/numpy/core/src/umath/scalarmath.c\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            gcc: numpy/core/src/npymath/npy_math.c\n            gcc: numpy/core/src/common/npy_longdouble.c\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            gcc: numpy/core/src/npymath/halffloat.c\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            gcc: numpy/core/src/common/numpyos.c\n            gcc: /private/var/folders/5y/pqfqz2md0njg2slq29yxp12w0000gn/T/pip-install-mxyh83f9/numpy_51614e6143884c3bbd246341eeb3b857/numpy/_build_utils/src/apple_sgemv_fix.c\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitlyclang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n      \n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            clang: error: the clang compiler does not support &#x27;faltivec&#x27;, please use -maltivec and include altivec.h explicitly\n            error: Command &quot;gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/opt/homebrew/Caskroom/miniforge/base/envs/d2d/include -arch arm64 -I/opt/homebrew/Caskroom/miniforge/base/envs/d2d/include -arch arm64 -DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -DNO_ATLAS_INFO=3 -DHAVE_CBLAS -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/umath -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/common -Inumpy/core/include -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/opt/homebrew/Caskroom/miniforge/base/envs/d2d/include/python3.8 -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-11.0-arm64-3.8/numpy/core/src/npymath -c numpy/core/src/multiarray/array_assign_scalar.c -o build/temp.macosx-11.0-arm64-3.8/numpy/core/src/multiarray/array_assign_scalar.o -MMD -MF build/temp.macosx-11.0-arm64-3.8/numpy/core/src/multiarray/array_assign_scalar.o.d -faltivec -I/System/Library/Frameworks/vecLib.framework/Headers&quot; failed with exit status 1\n            [end of output]\n      \n        note: This error originates from a subprocess, and is likely not a problem with pip.\n      error: legacy-install-failure\n      \n      × Encountered error while trying to install package.\n      ╰─&gt; numpy\n      \n      note: This is an issue with the package mentioned above, not pip.\n      hint: See above for output from the failure.\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: subprocess-exited-with-error\n\n× pip subprocess to install build dependencies did not run successfully.\n│ exit code: 1\n╰─&gt; See above for output.\n\nnote: This error originates from a subprocess, and is likely not a problem with pip.</code></pre>\n<p>Scroll to the top, it looks like <code>jupyter==1.0.0</code>, <code>pandas==1.2.2</code>, <code>numpy==1.18.5</code> are required, and numpy==1.18.5 is where the error comes from.</p>\n<p>4.As a result, install <code>jupyter==1.0.0</code> and <code>pandas==1.2.2</code> first</p>\n<pre><code class=\"hljs shell\">pip install jupyter==1.0.0</code></pre>\n<p>conda can't be installed by pip, but can by conda</p>\n<pre><code class=\"hljs shell\">conda install pandas=1.2.2</code></pre>\n<p>5.numpy==1.18.5 can not be installed by pip or conda</p>\n<p>thanks to <a href=\"https://github.com/apple/tensorflow_macos/releases/tag/v0.1alpha0\">tensorflow's wheel</a>, numpy==1.18.5's wheel for mac is included in the addons</p>\n<p>download and unzip <a href=\"https://github.com/apple/tensorflow_macos/releases/download/v0.1alpha0/tensorflow_macos-0.1alpha0.tar.gz\">tensorflow_macos-0.1alpha0.tar.gz</a></p>\n<p>go to the unzipped folder in terminal, and run</p>\n<pre><code class=\"hljs shell\">pip install arm64/numpy-1.18.5-cp38-cp38-macosx_11_0_arm64.whl</code></pre>\n<p>check</p>\n<pre><code class=\"hljs shell\">pip list |grep pandas\npip list |grep jupyter\npip list |grep numpy</code></pre>\n<pre><code class=\"hljs shell\">pandas               1.2.2\njupyter              1.0.0\njupyter-client       7.1.2\njupyter-console      6.4.0\njupyter-core         4.9.2\njupyterlab-pygments  0.1.2\njupyterlab-widgets   1.0.2\nnumpy                1.18.5</code></pre>\n<p>5.install d2l</p>\n<pre><code class=\"hljs shell\">pip install d2l</code></pre>\n<p>Sucess!</p>\n<h3 id=\"reference\">Reference</h3>\n<p>https://zh-v2.d2l.ai/chapter_installation/index.html</p>\n<p>https://parthiban-kannan.medium.com/install-tensorflow-on-apple-macbook-m1-release-c1ce7e65cd0</p>\n<p>https://github.com/apple/tensorflow_macos/issues/48</p>","wordcount":18538},{"title":"paper reading: MAE","author":"Ryan LI","toc":true,"declare":true,"date":"2022-04-26T18:00:38.000Z","_content":"> Published in Dec 2021, this new work by Kaiming He draws a lot of attention from the community. The astounding result of unsupervised transfer learning and the capability of reconstructing highly masked (up to 90%) images might herald a new era in CV. \n\n> This is a [series of paper reading notes](https://daydreamatnight.github.io/2022/04/02/paper-reading-start/), hopefully, to push me to read paper casually and to leave some record of what I've learned.\n\n<!-- more -->\n\nPaper: [Masked autoencoders are scalable vision learners](https://arxiv.org/abs/2111.06377)\n\nUseful link: https://www.bilibili.com/video/BV1sq4y1q77t/\n\n### Abstract\n\nInspired by BERT and ViT, this paper proposes an asymmetric, transformer-based, denoising auto-encoder architecture. The unsupervised pre-training task is to reconstruct highly masked input images. The pre-training time is reduced by 3 times with competitive accuracy. The transfer performance is even better than the supervised pre-training models.\n\n### Conclusion\n\nThe results show that MAE makes scaleable unsupervised pre-training in CV applicable, a similar route to that of NLP. \n\nThey claim the semantic density difference between text and image leads to different masking operations. Besides, the patch masking operation does not separate semantic entities, meaning one masked patch may include more than one piece of semantic information, unlike language. However, the reconstruction results show that the model manages to learn from complex semantics.\n\n### Introduction\n\nAlthough yielding excellent success in NLP, applying scalable unsupervised models in CV is still a challenging problem. But why? i.e. **what makes masked autoencoding different between vision and language? **3 reasons are discussed:\n\n- The architecture difference between convolution and transformer: it's hard to integrate masked embedding or positional embedding to the convolution layer. --addressed by ViT.\n\n- The information density difference between text and image: Unlike high-semantic text, natural signals in images possess heavy spatial redundancy. --addressed by masking a very high portion of random patches.\n- Decoder difference: in NLP, take BERT as an example, a simple linear projection is used as a decoder, while in vision, a simple decoder is not powerful enough to reconstruct the semantic level information -- addressed by substituting linear projection with transformer layers.\n\nThen, the idea of MAE is on the front door. The encoder processes only the unmasked patches, while the lightweight decoder reconstructs the whole image from the encoded latent representation and the [mask] tokens. With a very high masking ratio(e.g. 75%), the pre-training time can be reduced by 3 times.\n\nBesides, the data capacity and generalisation performance are great. SOTA accuracy is achieved with fine tuning on a medium-sized dataset.\n\n### Relate work\n\nWorks in 4 areas are briefly reviewed. \n\n{% markmap 300px %}\n\n- **Masked language modelling:**\n\n  - [BERT](https://arxiv.org/abs/1810.04805)\n\n  - [GPT](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)\n- **Auto-encoding:** \n\n  - [classic autoencoders](https://proceedings.neurips.cc/paper/1993/hash/9e3cfc48eccf81a0d57663e129aef3cb-Abstract.html): PCA, k-means \n\n  - [denoising autoencoders(DAE)](https://dl.acm.org/doi/abs/10.1145/1390156.1390294)\n- **Masked image encoding:** \n\n\t- classic\n\t\t- pioneer work [SDAE](https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf?ref=https://githubhelp.com)\n\t\t-  [context encoder](http://openaccess.thecvf.com/content_cvpr_2016/html/Pathak_Context_Encoders_Feature_CVPR_2016_paper.html)\n\n\t- transformer based: \n\t\t- [iGPT](http://proceedings.mlr.press/v119/chen20s.html)\n\t\t- [ViT](https://arxiv.org/abs/2010.11929)\n\t\t- [BEiT](https://arxiv.org/abs/2106.08254)\n- **Self-supervised learning:** \n  - CNN based: \n    - [Unsupervised learning of visual representations using videos](http://openaccess.thecvf.com/content_iccv_2015/html/Wang_Unsupervised_Learning_of_ICCV_2015_paper.html)\n    - [CFN](https://link.springer.com/chapter/10.1007/978-3-319-46466-4_5)\n    - [Colorful Image Colorization](https://link.springer.com/chapter/10.1007/978-3-319-46487-9_40)\n    - [Unsupervised representation learning by predicting image rotations](https://arxiv.org/abs/1803.07728)\n  - Transformer based: \n    - [ViT](https://arxiv.org/abs/2010.11929)\n  - Contrastive learning based: \n    - [Unsupervised feature learning via non-parametric instance discrimination](http://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.html)\n    - [Representation learning with contrastive predictive coding](https://arxiv.org/abs/1807.03748)\n    - [MOCO](http://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.html)\n\n{%endmarkmap%}\n\n### Approach\n\nThe architecture and the training approach is briefly covered in the sketch below:\n\n<img src=\"MAE architecture.png\" alt=\"MAE architecture\" style=\"zoom:40%;\" />\n\nAdditional details about the architecture: For per-processing, non-overlapped patching and random uniform masking are adopted. The [mask] token is shared, and another position embedding is introduced to the decoder input so that the inputs are different on different masked area. But it is unclear whether the positional embedding is performed only on the [mask] token or on the whole input, i.e. encoded patches + [mask] token. Furthermore, the default decoder has <10% computation per token compared with the encoder.\n\nReconstruction target: The decoder aims to recreate the pixels of masked patches. The loss function is the mean squared error (MSE) between the output and the original image, only on the masked region of course. Besides, a variation reconstructing the normalised pixels shows an improvement in representation quality.\n\n### ImageNet experiments\n\nFirst, the most astonish reconstruction results are shown below:\n\n<img src=\"paper-reading-MAE/MAE result.png\" alt=\"MAE result\" style=\"zoom:50%;\" />\n\n<img src=\"paper-reading-MAE/MAE result2.png\" alt=\"MAE result2\" style=\"zoom:30%;\" />\n\nMask ratio is higher than [BERT](https://arxiv.org/abs/1810.04805)(15%) and [iGPT](http://proceedings.mlr.press/v119/chen20s.html), [ViT](https://arxiv.org/abs/2010.11929) and [BEiT](https://arxiv.org/abs/2106.08254)(25%-50%), \n\n<img src=\"paper-reading-MAE/MAE mask ratio.png\" alt=\"MAE mask ratio\" style=\"zoom:60%;\" />\n\n\n\n### Transfer learning experiments\n\n\n\n### Reference\n\n[Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. *arXiv preprint arXiv:1810.04805*.](https://arxiv.org/abs/1810.04805)\n\n[Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language understanding by generative pre-training.](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)\n\n[Hinton, G. E., & Zemel, R. (1993). Autoencoders, minimum description length and Helmholtz free energy. *Advances in neural information processing systems*, *6*.](https://proceedings.neurips.cc/paper/1993/hash/9e3cfc48eccf81a0d57663e129aef3cb-Abstract.html)\n\n[Vincent, P., Larochelle, H., Bengio, Y., & Manzagol, P. A. (2008, July). Extracting and composing robust features with denoising autoencoders. In *Proceedings of the 25th international conference on Machine learning* (pp. 1096-1103).](https://dl.acm.org/doi/abs/10.1145/1390156.1390294)\n\n[Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P. A., & Bottou, L. (2010). Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. *Journal of machine learning research*, *11*(12).](https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf?ref=https://githubhelp.com)\n\n[Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., & Efros, A. A. (2016). Context encoders: Feature learning by inpainting. In *Proceedings of the IEEE conference on computer vision and pattern recognition* (pp. 2536-2544).](http://openaccess.thecvf.com/content_cvpr_2016/html/Pathak_Context_Encoders_Feature_CVPR_2016_paper.html)\n\n[Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., & Sutskever, I. (2020, November). Generative pretraining from pixels. In *International Conference on Machine Learning* (pp. 1691-1703). PMLR.](http://proceedings.mlr.press/v119/chen20s.html)\n\n[Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. *arXiv preprint arXiv:2010.11929*.](https://arxiv.org/abs/2010.11929)\n\n[Bao, H., Dong, L., & Wei, F. (2021). Beit: Bert pre-training of image transformers. *arXiv preprint arXiv:2106.08254*.](https://arxiv.org/abs/2106.08254)\n\n[Wang, X., & Gupta, A. (2015). Unsupervised learning of visual representations using videos. In *Proceedings of the IEEE international conference on computer vision* (pp. 2794-2802).](http://openaccess.thecvf.com/content_iccv_2015/html/Wang_Unsupervised_Learning_of_ICCV_2015_paper.html)\n\n[Noroozi, M., & Favaro, P. (2016, October). Unsupervised learning of visual representations by solving jigsaw puzzles. In *European conference on computer vision* (pp. 69-84). Springer, Cham.](https://link.springer.com/chapter/10.1007/978-3-319-46466-4_5)\n\n[Zhang, R., Isola, P., & Efros, A. A. (2016, October). Colorful image colorization. In *European conference on computer vision* (pp. 649-666). Springer, Cham.](https://link.springer.com/chapter/10.1007/978-3-319-46487-9_40)\n\n[Gidaris, S., Singh, P., & Komodakis, N. (2018). Unsupervised representation learning by predicting image rotations. *arXiv preprint arXiv:1803.07728*.](https://arxiv.org/abs/1803.07728)\n\n[Wu, Z., Xiong, Y., Yu, S. X., & Lin, D. (2018). Unsupervised feature learning via non-parametric instance discrimination. In *Proceedings of the IEEE conference on computer vision and pattern recognition* (pp. 3733-3742).](http://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.html)\n\n[Oord, A. V. D., Li, Y., & Vinyals, O. (2018). Representation learning with contrastive predictive coding. *arXiv preprint arXiv:1807.03748*.](https://arxiv.org/abs/1807.03748)\n\n[He, K., Fan, H., Wu, Y., Xie, S., & Girshick, R. (2020). Momentum contrast for unsupervised visual representation learning. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition* (pp. 9729-9738).](http://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.html)\n\n","source":"_posts/paper-reading-MAE.md","raw":"---\ntitle: 'paper reading: MAE'\nauthor: Ryan LI\ntoc: true\ndeclare: true\ndate: 2022-04-27 02:00:38\ntags:\n  - paper reading\n  - deep learning\n---\n> Published in Dec 2021, this new work by Kaiming He draws a lot of attention from the community. The astounding result of unsupervised transfer learning and the capability of reconstructing highly masked (up to 90%) images might herald a new era in CV. \n\n> This is a [series of paper reading notes](https://daydreamatnight.github.io/2022/04/02/paper-reading-start/), hopefully, to push me to read paper casually and to leave some record of what I've learned.\n\n<!-- more -->\n\nPaper: [Masked autoencoders are scalable vision learners](https://arxiv.org/abs/2111.06377)\n\nUseful link: https://www.bilibili.com/video/BV1sq4y1q77t/\n\n### Abstract\n\nInspired by BERT and ViT, this paper proposes an asymmetric, transformer-based, denoising auto-encoder architecture. The unsupervised pre-training task is to reconstruct highly masked input images. The pre-training time is reduced by 3 times with competitive accuracy. The transfer performance is even better than the supervised pre-training models.\n\n### Conclusion\n\nThe results show that MAE makes scaleable unsupervised pre-training in CV applicable, a similar route to that of NLP. \n\nThey claim the semantic density difference between text and image leads to different masking operations. Besides, the patch masking operation does not separate semantic entities, meaning one masked patch may include more than one piece of semantic information, unlike language. However, the reconstruction results show that the model manages to learn from complex semantics.\n\n### Introduction\n\nAlthough yielding excellent success in NLP, applying scalable unsupervised models in CV is still a challenging problem. But why? i.e. **what makes masked autoencoding different between vision and language? **3 reasons are discussed:\n\n- The architecture difference between convolution and transformer: it's hard to integrate masked embedding or positional embedding to the convolution layer. --addressed by ViT.\n\n- The information density difference between text and image: Unlike high-semantic text, natural signals in images possess heavy spatial redundancy. --addressed by masking a very high portion of random patches.\n- Decoder difference: in NLP, take BERT as an example, a simple linear projection is used as a decoder, while in vision, a simple decoder is not powerful enough to reconstruct the semantic level information -- addressed by substituting linear projection with transformer layers.\n\nThen, the idea of MAE is on the front door. The encoder processes only the unmasked patches, while the lightweight decoder reconstructs the whole image from the encoded latent representation and the [mask] tokens. With a very high masking ratio(e.g. 75%), the pre-training time can be reduced by 3 times.\n\nBesides, the data capacity and generalisation performance are great. SOTA accuracy is achieved with fine tuning on a medium-sized dataset.\n\n### Relate work\n\nWorks in 4 areas are briefly reviewed. \n\n{% markmap 300px %}\n\n- **Masked language modelling:**\n\n  - [BERT](https://arxiv.org/abs/1810.04805)\n\n  - [GPT](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)\n- **Auto-encoding:** \n\n  - [classic autoencoders](https://proceedings.neurips.cc/paper/1993/hash/9e3cfc48eccf81a0d57663e129aef3cb-Abstract.html): PCA, k-means \n\n  - [denoising autoencoders(DAE)](https://dl.acm.org/doi/abs/10.1145/1390156.1390294)\n- **Masked image encoding:** \n\n\t- classic\n\t\t- pioneer work [SDAE](https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf?ref=https://githubhelp.com)\n\t\t-  [context encoder](http://openaccess.thecvf.com/content_cvpr_2016/html/Pathak_Context_Encoders_Feature_CVPR_2016_paper.html)\n\n\t- transformer based: \n\t\t- [iGPT](http://proceedings.mlr.press/v119/chen20s.html)\n\t\t- [ViT](https://arxiv.org/abs/2010.11929)\n\t\t- [BEiT](https://arxiv.org/abs/2106.08254)\n- **Self-supervised learning:** \n  - CNN based: \n    - [Unsupervised learning of visual representations using videos](http://openaccess.thecvf.com/content_iccv_2015/html/Wang_Unsupervised_Learning_of_ICCV_2015_paper.html)\n    - [CFN](https://link.springer.com/chapter/10.1007/978-3-319-46466-4_5)\n    - [Colorful Image Colorization](https://link.springer.com/chapter/10.1007/978-3-319-46487-9_40)\n    - [Unsupervised representation learning by predicting image rotations](https://arxiv.org/abs/1803.07728)\n  - Transformer based: \n    - [ViT](https://arxiv.org/abs/2010.11929)\n  - Contrastive learning based: \n    - [Unsupervised feature learning via non-parametric instance discrimination](http://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.html)\n    - [Representation learning with contrastive predictive coding](https://arxiv.org/abs/1807.03748)\n    - [MOCO](http://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.html)\n\n{%endmarkmap%}\n\n### Approach\n\nThe architecture and the training approach is briefly covered in the sketch below:\n\n<img src=\"MAE architecture.png\" alt=\"MAE architecture\" style=\"zoom:40%;\" />\n\nAdditional details about the architecture: For per-processing, non-overlapped patching and random uniform masking are adopted. The [mask] token is shared, and another position embedding is introduced to the decoder input so that the inputs are different on different masked area. But it is unclear whether the positional embedding is performed only on the [mask] token or on the whole input, i.e. encoded patches + [mask] token. Furthermore, the default decoder has <10% computation per token compared with the encoder.\n\nReconstruction target: The decoder aims to recreate the pixels of masked patches. The loss function is the mean squared error (MSE) between the output and the original image, only on the masked region of course. Besides, a variation reconstructing the normalised pixels shows an improvement in representation quality.\n\n### ImageNet experiments\n\nFirst, the most astonish reconstruction results are shown below:\n\n<img src=\"paper-reading-MAE/MAE result.png\" alt=\"MAE result\" style=\"zoom:50%;\" />\n\n<img src=\"paper-reading-MAE/MAE result2.png\" alt=\"MAE result2\" style=\"zoom:30%;\" />\n\nMask ratio is higher than [BERT](https://arxiv.org/abs/1810.04805)(15%) and [iGPT](http://proceedings.mlr.press/v119/chen20s.html), [ViT](https://arxiv.org/abs/2010.11929) and [BEiT](https://arxiv.org/abs/2106.08254)(25%-50%), \n\n<img src=\"paper-reading-MAE/MAE mask ratio.png\" alt=\"MAE mask ratio\" style=\"zoom:60%;\" />\n\n\n\n### Transfer learning experiments\n\n\n\n### Reference\n\n[Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. *arXiv preprint arXiv:1810.04805*.](https://arxiv.org/abs/1810.04805)\n\n[Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language understanding by generative pre-training.](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)\n\n[Hinton, G. E., & Zemel, R. (1993). Autoencoders, minimum description length and Helmholtz free energy. *Advances in neural information processing systems*, *6*.](https://proceedings.neurips.cc/paper/1993/hash/9e3cfc48eccf81a0d57663e129aef3cb-Abstract.html)\n\n[Vincent, P., Larochelle, H., Bengio, Y., & Manzagol, P. A. (2008, July). Extracting and composing robust features with denoising autoencoders. In *Proceedings of the 25th international conference on Machine learning* (pp. 1096-1103).](https://dl.acm.org/doi/abs/10.1145/1390156.1390294)\n\n[Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P. A., & Bottou, L. (2010). Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. *Journal of machine learning research*, *11*(12).](https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf?ref=https://githubhelp.com)\n\n[Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., & Efros, A. A. (2016). Context encoders: Feature learning by inpainting. In *Proceedings of the IEEE conference on computer vision and pattern recognition* (pp. 2536-2544).](http://openaccess.thecvf.com/content_cvpr_2016/html/Pathak_Context_Encoders_Feature_CVPR_2016_paper.html)\n\n[Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., & Sutskever, I. (2020, November). Generative pretraining from pixels. In *International Conference on Machine Learning* (pp. 1691-1703). PMLR.](http://proceedings.mlr.press/v119/chen20s.html)\n\n[Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. *arXiv preprint arXiv:2010.11929*.](https://arxiv.org/abs/2010.11929)\n\n[Bao, H., Dong, L., & Wei, F. (2021). Beit: Bert pre-training of image transformers. *arXiv preprint arXiv:2106.08254*.](https://arxiv.org/abs/2106.08254)\n\n[Wang, X., & Gupta, A. (2015). Unsupervised learning of visual representations using videos. In *Proceedings of the IEEE international conference on computer vision* (pp. 2794-2802).](http://openaccess.thecvf.com/content_iccv_2015/html/Wang_Unsupervised_Learning_of_ICCV_2015_paper.html)\n\n[Noroozi, M., & Favaro, P. (2016, October). Unsupervised learning of visual representations by solving jigsaw puzzles. In *European conference on computer vision* (pp. 69-84). Springer, Cham.](https://link.springer.com/chapter/10.1007/978-3-319-46466-4_5)\n\n[Zhang, R., Isola, P., & Efros, A. A. (2016, October). Colorful image colorization. In *European conference on computer vision* (pp. 649-666). Springer, Cham.](https://link.springer.com/chapter/10.1007/978-3-319-46487-9_40)\n\n[Gidaris, S., Singh, P., & Komodakis, N. (2018). Unsupervised representation learning by predicting image rotations. *arXiv preprint arXiv:1803.07728*.](https://arxiv.org/abs/1803.07728)\n\n[Wu, Z., Xiong, Y., Yu, S. X., & Lin, D. (2018). Unsupervised feature learning via non-parametric instance discrimination. In *Proceedings of the IEEE conference on computer vision and pattern recognition* (pp. 3733-3742).](http://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.html)\n\n[Oord, A. V. D., Li, Y., & Vinyals, O. (2018). Representation learning with contrastive predictive coding. *arXiv preprint arXiv:1807.03748*.](https://arxiv.org/abs/1807.03748)\n\n[He, K., Fan, H., Wu, Y., Xie, S., & Girshick, R. (2020). Momentum contrast for unsupervised visual representation learning. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition* (pp. 9729-9738).](http://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.html)\n\n","slug":"paper-reading-MAE","published":1,"updated":"2022-04-30T19:31:11.529Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl2n5f409000ep9yb44de8xtp","content":"<blockquote>\n<p>Published in Dec 2021, this new work by Kaiming He draws a lot of attention from the community. The astounding result of unsupervised transfer learning and the capability of reconstructing highly masked (up to 90%) images might herald a new era in CV.</p>\n</blockquote>\n<blockquote>\n<p>This is a <a href=\"https://daydreamatnight.github.io/2022/04/02/paper-reading-start/\">series of paper reading notes</a>, hopefully, to push me to read paper casually and to leave some record of what I've learned.</p>\n</blockquote>\n<span id=\"more\"></span>\n<p>Paper: <a href=\"https://arxiv.org/abs/2111.06377\">Masked autoencoders are scalable vision learners</a></p>\n<p>Useful link: https://www.bilibili.com/video/BV1sq4y1q77t/</p>\n<h3 id=\"abstract\">Abstract</h3>\n<p>Inspired by BERT and ViT, this paper proposes an asymmetric, transformer-based, denoising auto-encoder architecture. The unsupervised pre-training task is to reconstruct highly masked input images. The pre-training time is reduced by 3 times with competitive accuracy. The transfer performance is even better than the supervised pre-training models.</p>\n<h3 id=\"conclusion\">Conclusion</h3>\n<p>The results show that MAE makes scaleable unsupervised pre-training in CV applicable, a similar route to that of NLP.</p>\n<p>They claim the semantic density difference between text and image leads to different masking operations. Besides, the patch masking operation does not separate semantic entities, meaning one masked patch may include more than one piece of semantic information, unlike language. However, the reconstruction results show that the model manages to learn from complex semantics.</p>\n<h3 id=\"introduction\">Introduction</h3>\n<p>Although yielding excellent success in NLP, applying scalable unsupervised models in CV is still a challenging problem. But why? i.e. <strong>what makes masked autoencoding different between vision and language? </strong>3 reasons are discussed:</p>\n<ul>\n<li><p>The architecture difference between convolution and transformer: it's hard to integrate masked embedding or positional embedding to the convolution layer. --addressed by ViT.</p></li>\n<li><p>The information density difference between text and image: Unlike high-semantic text, natural signals in images possess heavy spatial redundancy. --addressed by masking a very high portion of random patches.</p></li>\n<li><p>Decoder difference: in NLP, take BERT as an example, a simple linear projection is used as a decoder, while in vision, a simple decoder is not powerful enough to reconstruct the semantic level information -- addressed by substituting linear projection with transformer layers.</p></li>\n</ul>\n<p>Then, the idea of MAE is on the front door. The encoder processes only the unmasked patches, while the lightweight decoder reconstructs the whole image from the encoded latent representation and the [mask] tokens. With a very high masking ratio(e.g. 75%), the pre-training time can be reduced by 3 times.</p>\n<p>Besides, the data capacity and generalisation performance are great. SOTA accuracy is achieved with fine tuning on a medium-sized dataset.</p>\n<h3 id=\"relate-work\">Relate work</h3>\n<p>Works in 4 areas are briefly reviewed.</p>\n\n    <div class=\"markmap-container\" style=\"height:300px\">\n      <svg data='{\"t\":\"root\",\"d\":0,\"v\":\"\",\"c\":[{\"t\":\"list_item\",\"d\":2,\"p\":{\"lines\":[0,1]},\"v\":\"<strong>Masked language modelling:</strong>\",\"c\":[{\"t\":\"list_item\",\"d\":4,\"p\":{\"lines\":[2,3]},\"v\":\"<a href=\\\"https://arxiv.org/abs/1810.04805\\\">BERT</a>\"},{\"t\":\"list_item\",\"d\":4,\"p\":{\"lines\":[4,5]},\"v\":\"<a href=\\\"https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf\\\">GPT</a>\"}]},{\"t\":\"list_item\",\"d\":2,\"p\":{\"lines\":[5,6]},\"v\":\"<strong>Auto-encoding:</strong>\",\"c\":[{\"t\":\"list_item\",\"d\":4,\"p\":{\"lines\":[7,8]},\"v\":\"<a href=\\\"https://proceedings.neurips.cc/paper/1993/hash/9e3cfc48eccf81a0d57663e129aef3cb-Abstract.html\\\">classic autoencoders</a>: PCA, k-means\"},{\"t\":\"list_item\",\"d\":4,\"p\":{\"lines\":[9,10]},\"v\":\"<a href=\\\"https://dl.acm.org/doi/abs/10.1145/1390156.1390294\\\">denoising autoencoders(DAE)</a>\"}]},{\"t\":\"list_item\",\"d\":2,\"p\":{\"lines\":[10,11]},\"v\":\"<strong>Masked image encoding:</strong>\",\"c\":[{\"t\":\"list_item\",\"d\":4,\"p\":{\"lines\":[12,13]},\"v\":\"classic\",\"c\":[{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[13,14]},\"v\":\"pioneer work <a href=\\\"https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf?ref=https://githubhelp.com\\\">SDAE</a>\"},{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[14,15]},\"v\":\"<a href=\\\"http://openaccess.thecvf.com/content_cvpr_2016/html/Pathak_Context_Encoders_Feature_CVPR_2016_paper.html\\\">context encoder</a>\"}]},{\"t\":\"list_item\",\"d\":4,\"p\":{\"lines\":[16,17]},\"v\":\"transformer based:\",\"c\":[{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[17,18]},\"v\":\"<a href=\\\"http://proceedings.mlr.press/v119/chen20s.html\\\">iGPT</a>\"},{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[18,19]},\"v\":\"<a href=\\\"https://arxiv.org/abs/2010.11929\\\">ViT</a>\"},{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[19,20]},\"v\":\"<a href=\\\"https://arxiv.org/abs/2106.08254\\\">BEiT</a>\"}]}]},{\"t\":\"list_item\",\"d\":2,\"p\":{\"lines\":[20,21]},\"v\":\"<strong>Self-supervised learning:</strong>\",\"c\":[{\"t\":\"list_item\",\"d\":4,\"p\":{\"lines\":[21,22]},\"v\":\"CNN based:\",\"c\":[{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[22,23]},\"v\":\"<a href=\\\"http://openaccess.thecvf.com/content_iccv_2015/html/Wang_Unsupervised_Learning_of_ICCV_2015_paper.html\\\">Unsupervised learning of visual representations using videos</a>\"},{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[23,24]},\"v\":\"<a href=\\\"https://link.springer.com/chapter/10.1007/978-3-319-46466-4_5\\\">CFN</a>\"},{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[24,25]},\"v\":\"<a href=\\\"https://link.springer.com/chapter/10.1007/978-3-319-46487-9_40\\\">Colorful Image Colorization</a>\"},{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[25,26]},\"v\":\"<a href=\\\"https://arxiv.org/abs/1803.07728\\\">Unsupervised representation learning by predicting image rotations</a>\"}]},{\"t\":\"list_item\",\"d\":4,\"p\":{\"lines\":[26,27]},\"v\":\"Transformer based:\",\"c\":[{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[27,28]},\"v\":\"<a href=\\\"https://arxiv.org/abs/2010.11929\\\">ViT</a>\"}]},{\"t\":\"list_item\",\"d\":4,\"p\":{\"lines\":[28,29]},\"v\":\"Contrastive learning based:\",\"c\":[{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[29,30]},\"v\":\"<a href=\\\"http://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.html\\\">Unsupervised feature learning via non-parametric instance discrimination</a>\"},{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[30,31]},\"v\":\"<a href=\\\"https://arxiv.org/abs/1807.03748\\\">Representation learning with contrastive predictive coding</a>\"},{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[31,32]},\"v\":\"<a href=\\\"http://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.html\\\">MOCO</a>\"}]}]}],\"p\":{}}'></svg>\n    </div>\n  \n<h3 id=\"approach\">Approach</h3>\n<p>The architecture and the training approach is briefly covered in the sketch below:</p>\n<p><img src=\"MAE architecture.png\" srcset=\"/img/loading.gif\" lazyload alt=\"MAE architecture\" style=\"zoom:40%;\" /></p>\n<p>Additional details about the architecture: For per-processing, non-overlapped patching and random uniform masking are adopted. The [mask] token is shared, and another position embedding is introduced to the decoder input so that the inputs are different on different masked area. But it is unclear whether the positional embedding is performed only on the [mask] token or on the whole input, i.e. encoded patches + [mask] token. Furthermore, the default decoder has &lt;10% computation per token compared with the encoder.</p>\n<p>Reconstruction target: The decoder aims to recreate the pixels of masked patches. The loss function is the mean squared error (MSE) between the output and the original image, only on the masked region of course. Besides, a variation reconstructing the normalised pixels shows an improvement in representation quality.</p>\n<h3 id=\"imagenet-experiments\">ImageNet experiments</h3>\n<p>First, the most astonish reconstruction results are shown below:</p>\n<p><img src=\"paper-reading-MAE/MAE result.png\" srcset=\"/img/loading.gif\" lazyload alt=\"MAE result\" style=\"zoom:50%;\" /></p>\n<p><img src=\"paper-reading-MAE/MAE result2.png\" srcset=\"/img/loading.gif\" lazyload alt=\"MAE result2\" style=\"zoom:30%;\" /></p>\n<p>Mask ratio is higher than <a href=\"https://arxiv.org/abs/1810.04805\">BERT</a>(15%) and <a href=\"http://proceedings.mlr.press/v119/chen20s.html\">iGPT</a>, <a href=\"https://arxiv.org/abs/2010.11929\">ViT</a> and <a href=\"https://arxiv.org/abs/2106.08254\">BEiT</a>(25%-50%),</p>\n<p><img src=\"paper-reading-MAE/MAE mask ratio.png\" srcset=\"/img/loading.gif\" lazyload alt=\"MAE mask ratio\" style=\"zoom:60%;\" /></p>\n<h3 id=\"transfer-learning-experiments\">Transfer learning experiments</h3>\n<h3 id=\"reference\">Reference</h3>\n<p><a href=\"https://arxiv.org/abs/1810.04805\">Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. <em>arXiv preprint arXiv:1810.04805</em>.</a></p>\n<p><a href=\"https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf\">Radford, A., Narasimhan, K., Salimans, T., &amp; Sutskever, I. (2018). Improving language understanding by generative pre-training.</a></p>\n<p><a href=\"https://proceedings.neurips.cc/paper/1993/hash/9e3cfc48eccf81a0d57663e129aef3cb-Abstract.html\">Hinton, G. E., &amp; Zemel, R. (1993). Autoencoders, minimum description length and Helmholtz free energy. <em>Advances in neural information processing systems</em>, <em>6</em>.</a></p>\n<p><a href=\"https://dl.acm.org/doi/abs/10.1145/1390156.1390294\">Vincent, P., Larochelle, H., Bengio, Y., &amp; Manzagol, P. A. (2008, July). Extracting and composing robust features with denoising autoencoders. In <em>Proceedings of the 25th international conference on Machine learning</em> (pp. 1096-1103).</a></p>\n<p><a href=\"https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf?ref=https://githubhelp.com\">Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P. A., &amp; Bottou, L. (2010). Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. <em>Journal of machine learning research</em>, <em>11</em>(12).</a></p>\n<p><a href=\"http://openaccess.thecvf.com/content_cvpr_2016/html/Pathak_Context_Encoders_Feature_CVPR_2016_paper.html\">Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., &amp; Efros, A. A. (2016). Context encoders: Feature learning by inpainting. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> (pp. 2536-2544).</a></p>\n<p><a href=\"http://proceedings.mlr.press/v119/chen20s.html\">Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., &amp; Sutskever, I. (2020, November). Generative pretraining from pixels. In <em>International Conference on Machine Learning</em> (pp. 1691-1703). PMLR.</a></p>\n<p><a href=\"https://arxiv.org/abs/2010.11929\">Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... &amp; Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. <em>arXiv preprint arXiv:2010.11929</em>.</a></p>\n<p><a href=\"https://arxiv.org/abs/2106.08254\">Bao, H., Dong, L., &amp; Wei, F. (2021). Beit: Bert pre-training of image transformers. <em>arXiv preprint arXiv:2106.08254</em>.</a></p>\n<p><a href=\"http://openaccess.thecvf.com/content_iccv_2015/html/Wang_Unsupervised_Learning_of_ICCV_2015_paper.html\">Wang, X., &amp; Gupta, A. (2015). Unsupervised learning of visual representations using videos. In <em>Proceedings of the IEEE international conference on computer vision</em> (pp. 2794-2802).</a></p>\n<p><a href=\"https://link.springer.com/chapter/10.1007/978-3-319-46466-4_5\">Noroozi, M., &amp; Favaro, P. (2016, October). Unsupervised learning of visual representations by solving jigsaw puzzles. In <em>European conference on computer vision</em> (pp. 69-84). Springer, Cham.</a></p>\n<p><a href=\"https://link.springer.com/chapter/10.1007/978-3-319-46487-9_40\">Zhang, R., Isola, P., &amp; Efros, A. A. (2016, October). Colorful image colorization. In <em>European conference on computer vision</em> (pp. 649-666). Springer, Cham.</a></p>\n<p><a href=\"https://arxiv.org/abs/1803.07728\">Gidaris, S., Singh, P., &amp; Komodakis, N. (2018). Unsupervised representation learning by predicting image rotations. <em>arXiv preprint arXiv:1803.07728</em>.</a></p>\n<p><a href=\"http://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.html\">Wu, Z., Xiong, Y., Yu, S. X., &amp; Lin, D. (2018). Unsupervised feature learning via non-parametric instance discrimination. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> (pp. 3733-3742).</a></p>\n<p><a href=\"https://arxiv.org/abs/1807.03748\">Oord, A. V. D., Li, Y., &amp; Vinyals, O. (2018). Representation learning with contrastive predictive coding. <em>arXiv preprint arXiv:1807.03748</em>.</a></p>\n<p><a href=\"http://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.html\">He, K., Fan, H., Wu, Y., Xie, S., &amp; Girshick, R. (2020). Momentum contrast for unsupervised visual representation learning. In <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em> (pp. 9729-9738).</a></p>\n\n    <style>.markmap-container{display:flex;justify-content:center;margin:0 auto;width:90%;height:500px}.markmap-container svg{width:100%;height:100%}@media(max-width:768px){.markmap-container{height:400px}}</style>\n    <script src=\"https://cdn.jsdelivr.net/npm/d3@6\"></script>\n    <script src=\"https://cdn.jsdelivr.net/npm/markmap-view\"></script>\n    <script> document.querySelectorAll('.markmap-container>svg').forEach(mindmap => markmap.Markmap.create(mindmap, null, JSON.parse(mindmap.getAttribute('data'))))</script>\n  ","site":{"data":{}},"excerpt":"<blockquote>\n<p>Published in Dec 2021, this new work by Kaiming He draws a lot of attention from the community. The astounding result of unsupervised transfer learning and the capability of reconstructing highly masked (up to 90%) images might herald a new era in CV.</p>\n</blockquote>\n<blockquote>\n<p>This is a <a href=\"https://daydreamatnight.github.io/2022/04/02/paper-reading-start/\">series of paper reading notes</a>, hopefully, to push me to read paper casually and to leave some record of what I've learned.</p>\n</blockquote>","more":"<p>Paper: <a href=\"https://arxiv.org/abs/2111.06377\">Masked autoencoders are scalable vision learners</a></p>\n<p>Useful link: https://www.bilibili.com/video/BV1sq4y1q77t/</p>\n<h3 id=\"abstract\">Abstract</h3>\n<p>Inspired by BERT and ViT, this paper proposes an asymmetric, transformer-based, denoising auto-encoder architecture. The unsupervised pre-training task is to reconstruct highly masked input images. The pre-training time is reduced by 3 times with competitive accuracy. The transfer performance is even better than the supervised pre-training models.</p>\n<h3 id=\"conclusion\">Conclusion</h3>\n<p>The results show that MAE makes scaleable unsupervised pre-training in CV applicable, a similar route to that of NLP.</p>\n<p>They claim the semantic density difference between text and image leads to different masking operations. Besides, the patch masking operation does not separate semantic entities, meaning one masked patch may include more than one piece of semantic information, unlike language. However, the reconstruction results show that the model manages to learn from complex semantics.</p>\n<h3 id=\"introduction\">Introduction</h3>\n<p>Although yielding excellent success in NLP, applying scalable unsupervised models in CV is still a challenging problem. But why? i.e. <strong>what makes masked autoencoding different between vision and language? </strong>3 reasons are discussed:</p>\n<ul>\n<li><p>The architecture difference between convolution and transformer: it's hard to integrate masked embedding or positional embedding to the convolution layer. --addressed by ViT.</p></li>\n<li><p>The information density difference between text and image: Unlike high-semantic text, natural signals in images possess heavy spatial redundancy. --addressed by masking a very high portion of random patches.</p></li>\n<li><p>Decoder difference: in NLP, take BERT as an example, a simple linear projection is used as a decoder, while in vision, a simple decoder is not powerful enough to reconstruct the semantic level information -- addressed by substituting linear projection with transformer layers.</p></li>\n</ul>\n<p>Then, the idea of MAE is on the front door. The encoder processes only the unmasked patches, while the lightweight decoder reconstructs the whole image from the encoded latent representation and the [mask] tokens. With a very high masking ratio(e.g. 75%), the pre-training time can be reduced by 3 times.</p>\n<p>Besides, the data capacity and generalisation performance are great. SOTA accuracy is achieved with fine tuning on a medium-sized dataset.</p>\n<h3 id=\"relate-work\">Relate work</h3>\n<p>Works in 4 areas are briefly reviewed.</p>\n\n    <div class=\"markmap-container\" style=\"height:300px\">\n      <svg data='{\"t\":\"root\",\"d\":0,\"v\":\"\",\"c\":[{\"t\":\"list_item\",\"d\":2,\"p\":{\"lines\":[0,1]},\"v\":\"<strong>Masked language modelling:</strong>\",\"c\":[{\"t\":\"list_item\",\"d\":4,\"p\":{\"lines\":[2,3]},\"v\":\"<a href=\\\"https://arxiv.org/abs/1810.04805\\\">BERT</a>\"},{\"t\":\"list_item\",\"d\":4,\"p\":{\"lines\":[4,5]},\"v\":\"<a href=\\\"https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf\\\">GPT</a>\"}]},{\"t\":\"list_item\",\"d\":2,\"p\":{\"lines\":[5,6]},\"v\":\"<strong>Auto-encoding:</strong>\",\"c\":[{\"t\":\"list_item\",\"d\":4,\"p\":{\"lines\":[7,8]},\"v\":\"<a href=\\\"https://proceedings.neurips.cc/paper/1993/hash/9e3cfc48eccf81a0d57663e129aef3cb-Abstract.html\\\">classic autoencoders</a>: PCA, k-means\"},{\"t\":\"list_item\",\"d\":4,\"p\":{\"lines\":[9,10]},\"v\":\"<a href=\\\"https://dl.acm.org/doi/abs/10.1145/1390156.1390294\\\">denoising autoencoders(DAE)</a>\"}]},{\"t\":\"list_item\",\"d\":2,\"p\":{\"lines\":[10,11]},\"v\":\"<strong>Masked image encoding:</strong>\",\"c\":[{\"t\":\"list_item\",\"d\":4,\"p\":{\"lines\":[12,13]},\"v\":\"classic\",\"c\":[{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[13,14]},\"v\":\"pioneer work <a href=\\\"https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf?ref=https://githubhelp.com\\\">SDAE</a>\"},{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[14,15]},\"v\":\"<a href=\\\"http://openaccess.thecvf.com/content_cvpr_2016/html/Pathak_Context_Encoders_Feature_CVPR_2016_paper.html\\\">context encoder</a>\"}]},{\"t\":\"list_item\",\"d\":4,\"p\":{\"lines\":[16,17]},\"v\":\"transformer based:\",\"c\":[{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[17,18]},\"v\":\"<a href=\\\"http://proceedings.mlr.press/v119/chen20s.html\\\">iGPT</a>\"},{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[18,19]},\"v\":\"<a href=\\\"https://arxiv.org/abs/2010.11929\\\">ViT</a>\"},{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[19,20]},\"v\":\"<a href=\\\"https://arxiv.org/abs/2106.08254\\\">BEiT</a>\"}]}]},{\"t\":\"list_item\",\"d\":2,\"p\":{\"lines\":[20,21]},\"v\":\"<strong>Self-supervised learning:</strong>\",\"c\":[{\"t\":\"list_item\",\"d\":4,\"p\":{\"lines\":[21,22]},\"v\":\"CNN based:\",\"c\":[{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[22,23]},\"v\":\"<a href=\\\"http://openaccess.thecvf.com/content_iccv_2015/html/Wang_Unsupervised_Learning_of_ICCV_2015_paper.html\\\">Unsupervised learning of visual representations using videos</a>\"},{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[23,24]},\"v\":\"<a href=\\\"https://link.springer.com/chapter/10.1007/978-3-319-46466-4_5\\\">CFN</a>\"},{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[24,25]},\"v\":\"<a href=\\\"https://link.springer.com/chapter/10.1007/978-3-319-46487-9_40\\\">Colorful Image Colorization</a>\"},{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[25,26]},\"v\":\"<a href=\\\"https://arxiv.org/abs/1803.07728\\\">Unsupervised representation learning by predicting image rotations</a>\"}]},{\"t\":\"list_item\",\"d\":4,\"p\":{\"lines\":[26,27]},\"v\":\"Transformer based:\",\"c\":[{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[27,28]},\"v\":\"<a href=\\\"https://arxiv.org/abs/2010.11929\\\">ViT</a>\"}]},{\"t\":\"list_item\",\"d\":4,\"p\":{\"lines\":[28,29]},\"v\":\"Contrastive learning based:\",\"c\":[{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[29,30]},\"v\":\"<a href=\\\"http://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.html\\\">Unsupervised feature learning via non-parametric instance discrimination</a>\"},{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[30,31]},\"v\":\"<a href=\\\"https://arxiv.org/abs/1807.03748\\\">Representation learning with contrastive predictive coding</a>\"},{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[31,32]},\"v\":\"<a href=\\\"http://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.html\\\">MOCO</a>\"}]}]}],\"p\":{}}'></svg>\n    </div>\n  \n<h3 id=\"approach\">Approach</h3>\n<p>The architecture and the training approach is briefly covered in the sketch below:</p>\n<p><img src=\"MAE architecture.png\" alt=\"MAE architecture\" style=\"zoom:40%;\" /></p>\n<p>Additional details about the architecture: For per-processing, non-overlapped patching and random uniform masking are adopted. The [mask] token is shared, and another position embedding is introduced to the decoder input so that the inputs are different on different masked area. But it is unclear whether the positional embedding is performed only on the [mask] token or on the whole input, i.e. encoded patches + [mask] token. Furthermore, the default decoder has &lt;10% computation per token compared with the encoder.</p>\n<p>Reconstruction target: The decoder aims to recreate the pixels of masked patches. The loss function is the mean squared error (MSE) between the output and the original image, only on the masked region of course. Besides, a variation reconstructing the normalised pixels shows an improvement in representation quality.</p>\n<h3 id=\"imagenet-experiments\">ImageNet experiments</h3>\n<p>First, the most astonish reconstruction results are shown below:</p>\n<p><img src=\"paper-reading-MAE/MAE result.png\" alt=\"MAE result\" style=\"zoom:50%;\" /></p>\n<p><img src=\"paper-reading-MAE/MAE result2.png\" alt=\"MAE result2\" style=\"zoom:30%;\" /></p>\n<p>Mask ratio is higher than <a href=\"https://arxiv.org/abs/1810.04805\">BERT</a>(15%) and <a href=\"http://proceedings.mlr.press/v119/chen20s.html\">iGPT</a>, <a href=\"https://arxiv.org/abs/2010.11929\">ViT</a> and <a href=\"https://arxiv.org/abs/2106.08254\">BEiT</a>(25%-50%),</p>\n<p><img src=\"paper-reading-MAE/MAE mask ratio.png\" alt=\"MAE mask ratio\" style=\"zoom:60%;\" /></p>\n<h3 id=\"transfer-learning-experiments\">Transfer learning experiments</h3>\n<h3 id=\"reference\">Reference</h3>\n<p><a href=\"https://arxiv.org/abs/1810.04805\">Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. <em>arXiv preprint arXiv:1810.04805</em>.</a></p>\n<p><a href=\"https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf\">Radford, A., Narasimhan, K., Salimans, T., &amp; Sutskever, I. (2018). Improving language understanding by generative pre-training.</a></p>\n<p><a href=\"https://proceedings.neurips.cc/paper/1993/hash/9e3cfc48eccf81a0d57663e129aef3cb-Abstract.html\">Hinton, G. E., &amp; Zemel, R. (1993). Autoencoders, minimum description length and Helmholtz free energy. <em>Advances in neural information processing systems</em>, <em>6</em>.</a></p>\n<p><a href=\"https://dl.acm.org/doi/abs/10.1145/1390156.1390294\">Vincent, P., Larochelle, H., Bengio, Y., &amp; Manzagol, P. A. (2008, July). Extracting and composing robust features with denoising autoencoders. In <em>Proceedings of the 25th international conference on Machine learning</em> (pp. 1096-1103).</a></p>\n<p><a href=\"https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf?ref=https://githubhelp.com\">Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P. A., &amp; Bottou, L. (2010). Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. <em>Journal of machine learning research</em>, <em>11</em>(12).</a></p>\n<p><a href=\"http://openaccess.thecvf.com/content_cvpr_2016/html/Pathak_Context_Encoders_Feature_CVPR_2016_paper.html\">Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., &amp; Efros, A. A. (2016). Context encoders: Feature learning by inpainting. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> (pp. 2536-2544).</a></p>\n<p><a href=\"http://proceedings.mlr.press/v119/chen20s.html\">Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., &amp; Sutskever, I. (2020, November). Generative pretraining from pixels. In <em>International Conference on Machine Learning</em> (pp. 1691-1703). PMLR.</a></p>\n<p><a href=\"https://arxiv.org/abs/2010.11929\">Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... &amp; Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. <em>arXiv preprint arXiv:2010.11929</em>.</a></p>\n<p><a href=\"https://arxiv.org/abs/2106.08254\">Bao, H., Dong, L., &amp; Wei, F. (2021). Beit: Bert pre-training of image transformers. <em>arXiv preprint arXiv:2106.08254</em>.</a></p>\n<p><a href=\"http://openaccess.thecvf.com/content_iccv_2015/html/Wang_Unsupervised_Learning_of_ICCV_2015_paper.html\">Wang, X., &amp; Gupta, A. (2015). Unsupervised learning of visual representations using videos. In <em>Proceedings of the IEEE international conference on computer vision</em> (pp. 2794-2802).</a></p>\n<p><a href=\"https://link.springer.com/chapter/10.1007/978-3-319-46466-4_5\">Noroozi, M., &amp; Favaro, P. (2016, October). Unsupervised learning of visual representations by solving jigsaw puzzles. In <em>European conference on computer vision</em> (pp. 69-84). Springer, Cham.</a></p>\n<p><a href=\"https://link.springer.com/chapter/10.1007/978-3-319-46487-9_40\">Zhang, R., Isola, P., &amp; Efros, A. A. (2016, October). Colorful image colorization. In <em>European conference on computer vision</em> (pp. 649-666). Springer, Cham.</a></p>\n<p><a href=\"https://arxiv.org/abs/1803.07728\">Gidaris, S., Singh, P., &amp; Komodakis, N. (2018). Unsupervised representation learning by predicting image rotations. <em>arXiv preprint arXiv:1803.07728</em>.</a></p>\n<p><a href=\"http://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.html\">Wu, Z., Xiong, Y., Yu, S. X., &amp; Lin, D. (2018). Unsupervised feature learning via non-parametric instance discrimination. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> (pp. 3733-3742).</a></p>\n<p><a href=\"https://arxiv.org/abs/1807.03748\">Oord, A. V. D., Li, Y., &amp; Vinyals, O. (2018). Representation learning with contrastive predictive coding. <em>arXiv preprint arXiv:1807.03748</em>.</a></p>\n<p><a href=\"http://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.html\">He, K., Fan, H., Wu, Y., Xie, S., &amp; Girshick, R. (2020). Momentum contrast for unsupervised visual representation learning. In <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em> (pp. 9729-9738).</a></p>","wordcount":6269},{"title":"paper reading: GPT1-3","author":"Ryan LI","toc":true,"declare":true,"date":"2022-04-18T00:58:59.000Z","_content":"> GPT-3 is the most popular generative language model now. With more than 100 billion parameters, the performance is proved to be great and by now there are more than hundreds of works (commercial or academic) built on it, including the famous [GitHub Copilot](https://copilot.github.com/).\n\n> This is a [series of paper reading notes](https://daydreamatnight.github.io/2022/04/02/paper-reading-start/), hopefully, to push me to read paper casually and to leave some record of what I've learned.\n\n<!-- more -->\n\nPaper links:\n\nGPT-1: [Improving language understanding by generative pre-training](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)\n\nGPT-2: [Language models are unsupervised multitask learners](http://www.persagen.com/files/misc/radford2019language.pdf)\n\nGPT-3: [Language models are few-shot learners](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)\n\nUseful links: \n\nhttps://www.bilibili.com/video/BV1AF411b7xQ/\n\n[GPT-3 Demo: 300+ GPT-3 Examples, Demos, Apps](https://gpt3demo.com/)\n\n[GPT-3: Demos, Use-cases, Implications](https://towardsdatascience.com/gpt-3-demos-use-cases-implications-77f86e540dc1)\n\n*Since my own interest is not NLP, I haven't read these paper by myself. Instead, I follow a reference video above directly and make notes together with it.*\n\n## History and Timeline\n\n<img src=\"GPT timeline.png\" alt=\"GPT timeline\" style=\"zoom:50%;\" />\n\nGiven that the GPT series are all developed by the [OpenAI](https://openai.com/) and the Transformer & Bert are developed by Google, it seems there are two companies combating. And it is inevitable to compare these two series. \n\nFrom the perspective of number of cites, it is obviously that OpenAI have catch less attention from the academic world despite the huge cost. But it's not because GPT series are less novel, but because the goal of GPT series is bigger than that of Transformer & Bert. Transformer is originally developed for Machine Translation task only. And Bert simply aims to push the pre-training technic forward. That's the reason why Bert performs better than GPT if the number of parameters are the same. As a result, GPT is harder and more expensive to train a decent model. And the size of model make others hardly to reconstruct it. From the companies' perspective, OpenAI does this because they want to build the strong AI, but Transformer & Bert are developed only by the teams of Google.\n\n## GPT-1\n\n### Abstract\n\nThe whole idea is similar to what CV did in the last several decades, that is pre-training of a model on a massive dataset followed by fine-tuning on a small specialist dataset. Lacking of large labelled data such as ImageNet of 10 million, however, NLP can't do what CV does exactly. The scale of machine translation database might reach 10 million but one piece of image possesses almost 10 times of information than a sentence. So the database is still not big enough.\n\nGPT makes a big step by using unlabelled data for pre-training. And then aero shot on GPT-2 makes another big step. It is fair to say CV led the trend of deep learning in the first 5 years, but recent years, more new thoughts are coming from the NLP field. And these new thoughts have inspired the CV field as well such as [ViT](https://arxiv.org/abs/2010.11929), [CLIP](http://proceedings.mlr.press/v139/radford21a) and [MAE](https://arxiv.org/abs/2111.06377).\n\nBesides, actually it had been a long time since NLP started to use unsupervised pre-training back then. For example, the word embedding model had been used for decades. But, the word embedding can only be seen as a layer, extra layers of model need to be designed to suit for various tasks. With GPT, however, the architecture doesn't need much change, only adjusting the input to suit the tasks is ok.\n\nThe result is good but not as good as BERT, but the novelty of this paper is much better than BERT.\n\n### Introduction\n\nAfter briefly introducing the word embedding, problems of pre-training more than word-level data are presented. For example the type of optimisation objectives and how to transfer the extracted information to the tasks. The main reason of this problem is the variety of NLP tasks. There is no way to suit all the needs together.\n\nThen this paper introduces a semi-supervised method which has been explained many times. But actually GPT and BERT are normally called self-supervised model. Though they are the same to me. Semi-supervised learning is a common concept in the Machine Learning. It refers learning from a mix of massive unlabelled data and a few labelled data. \n\nThen the architecture is described. Interestingly, in order to list the reason of choosing transformer over RNN as backbone. The authors say \n\n> This model choice provides us with a more structured memory for handling long-term dependencies in text, compared to alternatives like recurrent networks, resulting in robust transfer performance across diverse tasks.\n\nBesides, the paper accents the task-specific input adaptions, which is the key of this paper.\n\n### Framework\n\n#### Unsupervised learning\n\nGPT uses a task that giving the data of u<sub>n-k</sub> to u<sub>n-1</sub> to predict the u<sub>n</sub>. So the likelihood function to be maximised is:\n\n<img src=\"GPT loss.png\" alt=\"GPT loss\" style=\"zoom:25%;\" />\n\nBecause the predicting task, the mechanism of the mask multi-head attention of transformer decoder matches the likelihood function perfectly. Because in the first layer of the transformer decoder, the data after u{i} are simply masked to be 0. \n\nAnd the whole pre-training process is like this:\n\n<img src=\"GPT pretraining process.png\" alt=\"GPT pretraining process\" style=\"zoom:30%;\" />\n\nCompared with BERT, the biggest difference is never encoder or decoder, bidirectional or one-way along. The key is the pre-training task they choose, the completion task that Bert use is much easier than GPT's prediction task. Because this is the **difference between interpolation and extrapolation**. Therefore, BERT outperforms GPT on the same number of parameters. But the potential of the GPT series goes far beyond BERT. As a result, it took OpenAI years to develop such an impressive GPT-3 model. On another side, however, GPT's prediction task leads to a different architecture with BERT. And the decoder architecture makes the GPT model hard to be bidirectional from the start. We'll see how it conquer this.\n\n#### Supervised fine-tuning\n\nThe fine-tuning task follows the standard supervise learning process as follows:\n\n<img src=\"GPT fine tune f2.png\" alt=\"GPT fine tune f2\" style=\"zoom:40%;\" /> \n\nwith     <img src=\"GPT fine tune f1.png\" alt=\"GPT fine tune f1\" style=\"zoom:40%;\" />\n\nAnd the authors find it helpful to optimise the L1 and L2 together as:\n\n<img src=\"GPT fine tune loss.png\" alt=\"GPT fine tune loss\" style=\"zoom:40%;\" />\n\n#### Task-specific input transformations\n\nThe last thing to do is how to apply this framework to different tasks. Similar to with BERT, the pre-trained transformer block doesn't have to be changed.\n\n<img src=\"GPT objectives.png\" alt=\"GPT objectives\" style=\"zoom:40%;\" />\n\n### Experiments\n\nThe dataset GPT trained on is [BooksCorpus dataset](https://paperswithcode.com/dataset/bookcorpus), and the model has 12 layers with H<sub>model</sub> = 768, same as BERT<sub>base</sub>. Although the transformer encoder has one layer less than the decoder, GPT and BERT's numbers of parameters are still at the same level. Yet BERT has a 3 times larger BERT<sub>large</sub> model than the base. Because in addition to the BookCorpus dataset, BERT uses one more dataset for pre-training, in total the dataset is 4 times larger than GPT's.\n\n> For the pre-training corpus we use the [BooksCorpus](https://paperswithcode.com/dataset/bookcorpus) (800M words) (Zhu et al., 2015) and [English Wikipedia](https://en.wikipedia.org/wiki/Wikipedia:Database_download) (2,500M words).\n\nAnd unfortunately, the average accuracy of BERT<sub>base</sub> is higher than GPT at this time. Besides with the BERT<sub>large</sub> model , the accuracy can go higher, as shown below in BERT's paper.\n\n<img src=\"BERT result with GPT.png\" alt=\"BERT result with GPT\" style=\"zoom:50%;\" />\n\n## GPT-2\n\nAfter GPT-1 got defeated by BERT in 4 months, of course, GPT-2 aims to fight back and beat BERT to the ground. Considering the decoder path can't shift to the encoder with dignity, the simplest way then is to enlarge the model and the dataset. But what if it's still not working?\n\n### Abstract\n\nAfter developing a new dataset of millions of webpages called WebText, and training on a new1.5B parameter model. The result turns out to be no significant difference with BERT. So they bring out another sell point, zero shot.\n\nActually, the zero-shot behaviour is mentioned in the last section of GPT-1's paper in order to understand more of the unsupervised pre-training mechanism. And in GPT-2, this behaviour is brought front to increase the novelty.\n\n### Introduction\n\n> **Zero-shot learning** (ZSL) is a problem setup in [machine learning](https://en.wikipedia.org/wiki/Machine_learning), where at test time, a learner observes samples from classes, which were not observed during [training](https://en.wikipedia.org/wiki/Machine_learning#Training_models), and needs to predict the class that they belong to. Zero-shot methods generally work by associating observed and non-observed classes through some form of auxiliary information, which encodes observable distinguishing properties of objects.[[1\\]](https://en.wikipedia.org/wiki/Zero-shot_learning#cite_note-1) \n\nThe main-steam approach is one dataset - one task instead of one dataset - multiple tasks because of the generalisation that state of art models lack. Yet multitask learning (trending 2000-2010) represents the idea of training one model with a combination of multiple datasets and different loss functions. So GPT-2 takes the idea of multitask learning and trains the model with the zero-shot setting, under which the downstream tasks can be handled with no collecting of supervised data or fine-tuning. The result is competitive  and promising according to the authors.\n\n### Approach\n\nThe model architectures of GPT1 and GPT2 are pretty much the same. But the input methods are different.\n\nIn detail, recalling that during fine tuning process, the GPT1 introduces extra tokens such as [start], [Delim] and [Extract] to modify the input. But in GPT2 without the supervised fine-tuning process, these extra tokens would cause confusion. As a result, the downstream task input need to be more likely to the natural language when constructing.\n\nAs a result, the authors introduce what we are used to calling it \"prompt\", here are the examples:\n\n> For example, a translation training example can be written as the sequence (**translate to french**, english text, french text). Likewise, a reading comprehension training example can be written as (**answer the question**, document, question, answer).\n\nAnd afterward, 2 ideas for why this would work are discussed. First, if the model is powerful enough, it might be capable of understanding the prompts. Second, in such a big dataset, this kind of data structure exists. Take machine translation as an example, there should be many sentences containing \"translate to French\", English text, and French text. The authors point out some of them below.\n\n| Examples of machine translation                              |\n| :----------------------------------------------------------- |\n| ”I’m not the cleverest man in the world, but like they say in French: Je ne suis pas un imbecile [I’m not a fool]. <br /><br />In a now-deleted post from Aug. 16, Soheil Eid, Tory candidate in the riding of Joliette, wrote in French: ”Mentez mentez, il en restera toujours quelque chose,” which translates as, ”Lie lie and something will always remain.”<br /><br />“I hate the word ‘perfume,”’ Burr says. ‘It’s somewhat better in French: ‘parfum.’<br /><br />If listened carefully at 29:55, a conversation can be heard between two guys in French: “-Comment on fait pour aller de l’autre coté? -Quel autre coté?”, which means “- How do you get to the other side? - What side?”. <br /><br />If this sounds like a bit of a stretch, consider this question in French: As-tu aller au cinéma?, or Did you go to the movies?, which literally translates as Have-you to go to movies/theater? “<br /><br />Brevet Sans Garantie Du Gouvernement”, translated to English: “Patented without government warranty”. |\n\n####  **Training data**\n\nAfter considering the need for larger data and the disadvantage(noise) of the existing dataset, they developed a new dataset. The data is collected by first crawling 45 million links discussed on Reddit, and then extracting all the contents of these pages. The dataset contains 8 million documents, 40 GB of text.\n\n### Experiment\n\nWith 4 level of model, the one-short performances on 4 downstream tasks are shown blow:\n\n<img src=\"GPT2 performance.png\" alt=\"GPT2 performance\" style=\"zoom:50%;\" />\n\nIt can be seen that first 3 tasks, the results are not the best yet not the worst, however, the performance on Question Answering is bad, and other works perform way better than GPT-2. \n\nBut this is not over, because in the figure, there is still room for performance improvement on all 4 tasks as the size of the larger model increases. So here comes GPT-3.\n\n## GPT-3\n\nThe value of an article depends on the topic, effectiveness and novelty. GPT-2 has rather low effectiveness but strong novelty. As a result, GPT-3 aims to promote the effectiveness of its predecessor, while loosing the zero-shot condition to few-shot condition.\n\n### Abstract\n\nThe parameters of GPT-3 are enlarged 10 times to 175 billion.  When applying GPT-3 to downstream tasks, strong performance is achieved without gradient update or fine-tuning. Besides, GPT-3 is capable of generating articles that are indistinguishable from humans' work.\n\nBesides, unlikely to former papers, this is a 63-page technic report. Without limitation of words or page number, it is very detailed, especially for the experiment and discussion sections.\n\n### Introduction\n\nThere are 3 problems of current pre-training + fine-tuning language model: \n\n- A large labeled dataset is still needed for a good result\n- The pre-training model is not really generalisable because of the need for fine-tuning\n- Human doesn't need large fine tuning dataset\n\nWhen introducing their work, the authors try to re-define the concept of \"meta-learning\" and \"in-context learning\". What they really mean are training a huge generalisable model and train without updating the gradient, respectively.\n\nThen 3 evaluation methods are presented:\n\n- Few-shot learning: 10-100 task-related data\n\n- One-shot learning\n- Zero-short learning\n\nAnd the performance with model size is plotted below (1.3B matches the GPT-2 model). Can be seen the final accuracy (few shot - 175B) almost doubled the former GPT-2 accuracy (1.3B zero-short)\n\n<img src=\"GPT3 performance.png\" alt=\"GPT3 performance\" style=\"zoom:50%;\" />\n\n### Approach\n\nIn this section, fine-tuning, few-shot, one-shot, zero-shot learning are explained first in this figure below:\n\n<img src=\"GPT-3 approach.png\" alt=\"GPT-3 approach\" style=\"zoom:50%;\" />\n\nThe right column denotes the traditional fine-tuning process that requires extra few gradient update steps. The left side shows how GPT-3 applies \"in-context learning\" during inference. Basically, the input is divided by 3 parts, task description, example, and prompt. The task description and prompt are ended by \":\", and \"=>\" respectively. The Transformer Decoder extracts the features of the \"context\" and then predicts the next several words starting from the prompt. As a result, the model is able to infer without gradient updates. Yet it comes 2 problems with few-shot:\n\n- Unable to process really long examples. For example, thousands of English-French translation dataset is easy to get access but they can rarely be leveraged to promote the model performance.\n- The inference is one-time thing, meaning the model can't actually learn from previous task description or example. Same samples have to be inputed again an again.\n\nAs a result, the few-shot learning is not commonly used.\n\n#### Model and Architectures\n\nGPT-3 uses the same architecture as GPT-2 yet uses the methods form \"[Sparse Transformer](https://arxiv.org/abs/1904.10509)\" to modify the layers. And 8 different size of models are developed.\n\n<img src=\"GPT3 models.png\" alt=\"GPT3 models\" style=\"zoom:60%;\" /> \n\nYet compared with previous models, GPT-3 is \"fatter\" ,meaning with same *d<sub>model</sub>* (192x of GPT3<sub>small</sub>), *n<sub>layers</sub>* is smaller (only 8x of GPT3<sub>small</sub>). \n\nThe mini-batch size goes up to 3.2M, definitely not mini. Large batch size improves the computational performance and parallelism while reducing the noise in each batch and making the model easier to overfitting. However, this defect is not evident in GPT-3. It is still an open question, now people consider it from two aspects: (1) the internal structure prevents the model from overfitting. (2) Such a large model is able to search for large space and it is more prone to converge to a simpler architecture.\n\nThe learning rate decreases with batch size increasing according to [research1](https://arxiv.org/abs/2001.08361) and [research2](https://arxiv.org/abs/1812.06162), also counter-intuitive. [Former work](https://arxiv.org/abs/1706.02677) shows that batch size should increase linear with the learning rate.\n\n#### Training Dataset\n\nWith a huge model, [Common Craw dataset](https://commoncrawl.org/) has come back as an option. The authors clean this dataset in 3 steps:\n\n- A logistic classification model is built, taking samples of Common Craw dataset as negative and WebText as positive. Such model is used on the whole dataset of Common Craw to predict positive (high quality) or negative. The positive stay and the negative are filtered.\n- [LSH algorithm](https://www.pinecone.io/learn/locality-sensitive-hashing/) is applied on the remaining dataset to filter the similar content.\n- More \"clean\" datasets are mixed in with weight.\n\n<img src=\"GPT3 training data.png\" alt=\"GPT3 training data\" style=\"zoom:50%;\" />\n\n#### Training Process\n\nSpecific training details are not presented. Information so far is [DGX-1 cluster](https://www.nvidia.com/en-us/data-center/dgx-1/) is used.\n\n### Results\n\nThe results are too many so only interesting figures are covered.\n\n<img src=\"GPT3 performance with compute.png\" alt=\"GPT3 performance with compute\" style=\"zoom:50%;\" />\n\nFrom the figure, the power law distribution of performance with compute are found, i.e. in order to decrease the validation loss linearly, the FLOPS need to increase exponentially. This still is a major problem in Machine Learning.\n\n<img src=\"GPT3 result.png\" alt=\"GPT3 result\" style=\"zoom:50%;\" />\n\nThis figure shows the compression of results with Zero-shot SOTO and human. Nothing to comment. Just ... good.\n\n<img src=\"GPT-3 result 2.png\" alt=\"GPT-3 result 2\" style=\"zoom:50%;\" />And on the Open-Domain QA tasks, GPT-3 outperform other models such as google T5. Google T5 can be considered as a model with both encoder and decoder.\n\n<img src=\"GPT3 result 3.png\" alt=\"GPT3 result 3\" style=\"zoom:50%;\" />\n\nAnd few shot learning outperform SOTO fine tuning models.\n\n<img src=\"GPT3 result4.png\" alt=\"GPT3 result4\" style=\"zoom:50%;\" />\n\nIn the machine translation task, it is interesting to see that other language to English is better than English to others.\n\n<img src=\"GPT 3 result 5.png\" alt=\"GPT 3 result 5\" style=\"zoom:50%;\" />\n\nThen a news article is generated with GPT-3 with numbers, years and time that makes the article look legit.\n\n### Limitations\n\n- Text synthesis, the predicted texts are always looping.\n- The bidirectional limit still exist because of the decoder structure.\n- The tokens that learned by GPT3 are equally weighted, yet wasting the model on the meaningless but high frequent function words.\n- No experience in Videos or real-world physical interaction.\n- The interpretation is still low. It's nearly impossible to know how does such a big model works.\n\n### Broader Impact\n\n- Can be used for fraud and crimes\n- Gender difference and race\n- Energy consuming \n\n<img src=\"GPT3 gender.png\" alt=\"GPT3 gender\" style=\"zoom:50%;\" />\n\n<img src=\"GPT3 race.png\" alt=\"GPT3 race\" style=\"zoom:50%;\" />\n\n\n\n## Reference\n\n[Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. *arXiv preprint arXiv:2010.11929*.]()\n\n[Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... & Sutskever, I. (2021, July). Learning transferable visual models from natural language supervision. In *International Conference on Machine Learning* (pp. 8748-8763). PMLR.](http://proceedings.mlr.press/v139/radford21a)\n\n[He, K., Chen, X., Xie, S., Li, Y., Dollár, P., & Girshick, R. (2021). Masked autoencoders are scalable vision learners. *arXiv preprint arXiv:2111.06377*.](https://arxiv.org/abs/2111.06377)\n\n[Child, R., Gray, S., Radford, A., & Sutskever, I. (2019). Generating long sequences with sparse transformers. *arXiv preprint arXiv:1904.10509*.](https://arxiv.org/abs/1904.10509)\n\n[Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., ... & Amodei, D. (2020). Scaling laws for neural language models. *arXiv preprint arXiv:2001.08361*.](https://arxiv.org/abs/2001.08361)\n\n[McCandlish, S., Kaplan, J., Amodei, D., & Team, O. D. (2018). An empirical model of large-batch training. *arXiv preprint arXiv:1812.06162*.](https://arxiv.org/abs/1812.06162)\n\n[Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., ... & He, K. (2017). Accurate, large minibatch sgd: Training imagenet in 1 hour. *arXiv preprint arXiv:1706.02677*.](https://arxiv.org/abs/1706.02677)\n","source":"_posts/paper-reading-GPT1-3.md","raw":"---\ntitle: 'paper reading: GPT1-3'\nauthor: Ryan LI\ntoc: true\ndeclare: true\ndate: 2022-04-18 08:58:59\ntags:\n  - paper reading\n  - deep learning\n---\n> GPT-3 is the most popular generative language model now. With more than 100 billion parameters, the performance is proved to be great and by now there are more than hundreds of works (commercial or academic) built on it, including the famous [GitHub Copilot](https://copilot.github.com/).\n\n> This is a [series of paper reading notes](https://daydreamatnight.github.io/2022/04/02/paper-reading-start/), hopefully, to push me to read paper casually and to leave some record of what I've learned.\n\n<!-- more -->\n\nPaper links:\n\nGPT-1: [Improving language understanding by generative pre-training](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)\n\nGPT-2: [Language models are unsupervised multitask learners](http://www.persagen.com/files/misc/radford2019language.pdf)\n\nGPT-3: [Language models are few-shot learners](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)\n\nUseful links: \n\nhttps://www.bilibili.com/video/BV1AF411b7xQ/\n\n[GPT-3 Demo: 300+ GPT-3 Examples, Demos, Apps](https://gpt3demo.com/)\n\n[GPT-3: Demos, Use-cases, Implications](https://towardsdatascience.com/gpt-3-demos-use-cases-implications-77f86e540dc1)\n\n*Since my own interest is not NLP, I haven't read these paper by myself. Instead, I follow a reference video above directly and make notes together with it.*\n\n## History and Timeline\n\n<img src=\"GPT timeline.png\" alt=\"GPT timeline\" style=\"zoom:50%;\" />\n\nGiven that the GPT series are all developed by the [OpenAI](https://openai.com/) and the Transformer & Bert are developed by Google, it seems there are two companies combating. And it is inevitable to compare these two series. \n\nFrom the perspective of number of cites, it is obviously that OpenAI have catch less attention from the academic world despite the huge cost. But it's not because GPT series are less novel, but because the goal of GPT series is bigger than that of Transformer & Bert. Transformer is originally developed for Machine Translation task only. And Bert simply aims to push the pre-training technic forward. That's the reason why Bert performs better than GPT if the number of parameters are the same. As a result, GPT is harder and more expensive to train a decent model. And the size of model make others hardly to reconstruct it. From the companies' perspective, OpenAI does this because they want to build the strong AI, but Transformer & Bert are developed only by the teams of Google.\n\n## GPT-1\n\n### Abstract\n\nThe whole idea is similar to what CV did in the last several decades, that is pre-training of a model on a massive dataset followed by fine-tuning on a small specialist dataset. Lacking of large labelled data such as ImageNet of 10 million, however, NLP can't do what CV does exactly. The scale of machine translation database might reach 10 million but one piece of image possesses almost 10 times of information than a sentence. So the database is still not big enough.\n\nGPT makes a big step by using unlabelled data for pre-training. And then aero shot on GPT-2 makes another big step. It is fair to say CV led the trend of deep learning in the first 5 years, but recent years, more new thoughts are coming from the NLP field. And these new thoughts have inspired the CV field as well such as [ViT](https://arxiv.org/abs/2010.11929), [CLIP](http://proceedings.mlr.press/v139/radford21a) and [MAE](https://arxiv.org/abs/2111.06377).\n\nBesides, actually it had been a long time since NLP started to use unsupervised pre-training back then. For example, the word embedding model had been used for decades. But, the word embedding can only be seen as a layer, extra layers of model need to be designed to suit for various tasks. With GPT, however, the architecture doesn't need much change, only adjusting the input to suit the tasks is ok.\n\nThe result is good but not as good as BERT, but the novelty of this paper is much better than BERT.\n\n### Introduction\n\nAfter briefly introducing the word embedding, problems of pre-training more than word-level data are presented. For example the type of optimisation objectives and how to transfer the extracted information to the tasks. The main reason of this problem is the variety of NLP tasks. There is no way to suit all the needs together.\n\nThen this paper introduces a semi-supervised method which has been explained many times. But actually GPT and BERT are normally called self-supervised model. Though they are the same to me. Semi-supervised learning is a common concept in the Machine Learning. It refers learning from a mix of massive unlabelled data and a few labelled data. \n\nThen the architecture is described. Interestingly, in order to list the reason of choosing transformer over RNN as backbone. The authors say \n\n> This model choice provides us with a more structured memory for handling long-term dependencies in text, compared to alternatives like recurrent networks, resulting in robust transfer performance across diverse tasks.\n\nBesides, the paper accents the task-specific input adaptions, which is the key of this paper.\n\n### Framework\n\n#### Unsupervised learning\n\nGPT uses a task that giving the data of u<sub>n-k</sub> to u<sub>n-1</sub> to predict the u<sub>n</sub>. So the likelihood function to be maximised is:\n\n<img src=\"GPT loss.png\" alt=\"GPT loss\" style=\"zoom:25%;\" />\n\nBecause the predicting task, the mechanism of the mask multi-head attention of transformer decoder matches the likelihood function perfectly. Because in the first layer of the transformer decoder, the data after u{i} are simply masked to be 0. \n\nAnd the whole pre-training process is like this:\n\n<img src=\"GPT pretraining process.png\" alt=\"GPT pretraining process\" style=\"zoom:30%;\" />\n\nCompared with BERT, the biggest difference is never encoder or decoder, bidirectional or one-way along. The key is the pre-training task they choose, the completion task that Bert use is much easier than GPT's prediction task. Because this is the **difference between interpolation and extrapolation**. Therefore, BERT outperforms GPT on the same number of parameters. But the potential of the GPT series goes far beyond BERT. As a result, it took OpenAI years to develop such an impressive GPT-3 model. On another side, however, GPT's prediction task leads to a different architecture with BERT. And the decoder architecture makes the GPT model hard to be bidirectional from the start. We'll see how it conquer this.\n\n#### Supervised fine-tuning\n\nThe fine-tuning task follows the standard supervise learning process as follows:\n\n<img src=\"GPT fine tune f2.png\" alt=\"GPT fine tune f2\" style=\"zoom:40%;\" /> \n\nwith     <img src=\"GPT fine tune f1.png\" alt=\"GPT fine tune f1\" style=\"zoom:40%;\" />\n\nAnd the authors find it helpful to optimise the L1 and L2 together as:\n\n<img src=\"GPT fine tune loss.png\" alt=\"GPT fine tune loss\" style=\"zoom:40%;\" />\n\n#### Task-specific input transformations\n\nThe last thing to do is how to apply this framework to different tasks. Similar to with BERT, the pre-trained transformer block doesn't have to be changed.\n\n<img src=\"GPT objectives.png\" alt=\"GPT objectives\" style=\"zoom:40%;\" />\n\n### Experiments\n\nThe dataset GPT trained on is [BooksCorpus dataset](https://paperswithcode.com/dataset/bookcorpus), and the model has 12 layers with H<sub>model</sub> = 768, same as BERT<sub>base</sub>. Although the transformer encoder has one layer less than the decoder, GPT and BERT's numbers of parameters are still at the same level. Yet BERT has a 3 times larger BERT<sub>large</sub> model than the base. Because in addition to the BookCorpus dataset, BERT uses one more dataset for pre-training, in total the dataset is 4 times larger than GPT's.\n\n> For the pre-training corpus we use the [BooksCorpus](https://paperswithcode.com/dataset/bookcorpus) (800M words) (Zhu et al., 2015) and [English Wikipedia](https://en.wikipedia.org/wiki/Wikipedia:Database_download) (2,500M words).\n\nAnd unfortunately, the average accuracy of BERT<sub>base</sub> is higher than GPT at this time. Besides with the BERT<sub>large</sub> model , the accuracy can go higher, as shown below in BERT's paper.\n\n<img src=\"BERT result with GPT.png\" alt=\"BERT result with GPT\" style=\"zoom:50%;\" />\n\n## GPT-2\n\nAfter GPT-1 got defeated by BERT in 4 months, of course, GPT-2 aims to fight back and beat BERT to the ground. Considering the decoder path can't shift to the encoder with dignity, the simplest way then is to enlarge the model and the dataset. But what if it's still not working?\n\n### Abstract\n\nAfter developing a new dataset of millions of webpages called WebText, and training on a new1.5B parameter model. The result turns out to be no significant difference with BERT. So they bring out another sell point, zero shot.\n\nActually, the zero-shot behaviour is mentioned in the last section of GPT-1's paper in order to understand more of the unsupervised pre-training mechanism. And in GPT-2, this behaviour is brought front to increase the novelty.\n\n### Introduction\n\n> **Zero-shot learning** (ZSL) is a problem setup in [machine learning](https://en.wikipedia.org/wiki/Machine_learning), where at test time, a learner observes samples from classes, which were not observed during [training](https://en.wikipedia.org/wiki/Machine_learning#Training_models), and needs to predict the class that they belong to. Zero-shot methods generally work by associating observed and non-observed classes through some form of auxiliary information, which encodes observable distinguishing properties of objects.[[1\\]](https://en.wikipedia.org/wiki/Zero-shot_learning#cite_note-1) \n\nThe main-steam approach is one dataset - one task instead of one dataset - multiple tasks because of the generalisation that state of art models lack. Yet multitask learning (trending 2000-2010) represents the idea of training one model with a combination of multiple datasets and different loss functions. So GPT-2 takes the idea of multitask learning and trains the model with the zero-shot setting, under which the downstream tasks can be handled with no collecting of supervised data or fine-tuning. The result is competitive  and promising according to the authors.\n\n### Approach\n\nThe model architectures of GPT1 and GPT2 are pretty much the same. But the input methods are different.\n\nIn detail, recalling that during fine tuning process, the GPT1 introduces extra tokens such as [start], [Delim] and [Extract] to modify the input. But in GPT2 without the supervised fine-tuning process, these extra tokens would cause confusion. As a result, the downstream task input need to be more likely to the natural language when constructing.\n\nAs a result, the authors introduce what we are used to calling it \"prompt\", here are the examples:\n\n> For example, a translation training example can be written as the sequence (**translate to french**, english text, french text). Likewise, a reading comprehension training example can be written as (**answer the question**, document, question, answer).\n\nAnd afterward, 2 ideas for why this would work are discussed. First, if the model is powerful enough, it might be capable of understanding the prompts. Second, in such a big dataset, this kind of data structure exists. Take machine translation as an example, there should be many sentences containing \"translate to French\", English text, and French text. The authors point out some of them below.\n\n| Examples of machine translation                              |\n| :----------------------------------------------------------- |\n| ”I’m not the cleverest man in the world, but like they say in French: Je ne suis pas un imbecile [I’m not a fool]. <br /><br />In a now-deleted post from Aug. 16, Soheil Eid, Tory candidate in the riding of Joliette, wrote in French: ”Mentez mentez, il en restera toujours quelque chose,” which translates as, ”Lie lie and something will always remain.”<br /><br />“I hate the word ‘perfume,”’ Burr says. ‘It’s somewhat better in French: ‘parfum.’<br /><br />If listened carefully at 29:55, a conversation can be heard between two guys in French: “-Comment on fait pour aller de l’autre coté? -Quel autre coté?”, which means “- How do you get to the other side? - What side?”. <br /><br />If this sounds like a bit of a stretch, consider this question in French: As-tu aller au cinéma?, or Did you go to the movies?, which literally translates as Have-you to go to movies/theater? “<br /><br />Brevet Sans Garantie Du Gouvernement”, translated to English: “Patented without government warranty”. |\n\n####  **Training data**\n\nAfter considering the need for larger data and the disadvantage(noise) of the existing dataset, they developed a new dataset. The data is collected by first crawling 45 million links discussed on Reddit, and then extracting all the contents of these pages. The dataset contains 8 million documents, 40 GB of text.\n\n### Experiment\n\nWith 4 level of model, the one-short performances on 4 downstream tasks are shown blow:\n\n<img src=\"GPT2 performance.png\" alt=\"GPT2 performance\" style=\"zoom:50%;\" />\n\nIt can be seen that first 3 tasks, the results are not the best yet not the worst, however, the performance on Question Answering is bad, and other works perform way better than GPT-2. \n\nBut this is not over, because in the figure, there is still room for performance improvement on all 4 tasks as the size of the larger model increases. So here comes GPT-3.\n\n## GPT-3\n\nThe value of an article depends on the topic, effectiveness and novelty. GPT-2 has rather low effectiveness but strong novelty. As a result, GPT-3 aims to promote the effectiveness of its predecessor, while loosing the zero-shot condition to few-shot condition.\n\n### Abstract\n\nThe parameters of GPT-3 are enlarged 10 times to 175 billion.  When applying GPT-3 to downstream tasks, strong performance is achieved without gradient update or fine-tuning. Besides, GPT-3 is capable of generating articles that are indistinguishable from humans' work.\n\nBesides, unlikely to former papers, this is a 63-page technic report. Without limitation of words or page number, it is very detailed, especially for the experiment and discussion sections.\n\n### Introduction\n\nThere are 3 problems of current pre-training + fine-tuning language model: \n\n- A large labeled dataset is still needed for a good result\n- The pre-training model is not really generalisable because of the need for fine-tuning\n- Human doesn't need large fine tuning dataset\n\nWhen introducing their work, the authors try to re-define the concept of \"meta-learning\" and \"in-context learning\". What they really mean are training a huge generalisable model and train without updating the gradient, respectively.\n\nThen 3 evaluation methods are presented:\n\n- Few-shot learning: 10-100 task-related data\n\n- One-shot learning\n- Zero-short learning\n\nAnd the performance with model size is plotted below (1.3B matches the GPT-2 model). Can be seen the final accuracy (few shot - 175B) almost doubled the former GPT-2 accuracy (1.3B zero-short)\n\n<img src=\"GPT3 performance.png\" alt=\"GPT3 performance\" style=\"zoom:50%;\" />\n\n### Approach\n\nIn this section, fine-tuning, few-shot, one-shot, zero-shot learning are explained first in this figure below:\n\n<img src=\"GPT-3 approach.png\" alt=\"GPT-3 approach\" style=\"zoom:50%;\" />\n\nThe right column denotes the traditional fine-tuning process that requires extra few gradient update steps. The left side shows how GPT-3 applies \"in-context learning\" during inference. Basically, the input is divided by 3 parts, task description, example, and prompt. The task description and prompt are ended by \":\", and \"=>\" respectively. The Transformer Decoder extracts the features of the \"context\" and then predicts the next several words starting from the prompt. As a result, the model is able to infer without gradient updates. Yet it comes 2 problems with few-shot:\n\n- Unable to process really long examples. For example, thousands of English-French translation dataset is easy to get access but they can rarely be leveraged to promote the model performance.\n- The inference is one-time thing, meaning the model can't actually learn from previous task description or example. Same samples have to be inputed again an again.\n\nAs a result, the few-shot learning is not commonly used.\n\n#### Model and Architectures\n\nGPT-3 uses the same architecture as GPT-2 yet uses the methods form \"[Sparse Transformer](https://arxiv.org/abs/1904.10509)\" to modify the layers. And 8 different size of models are developed.\n\n<img src=\"GPT3 models.png\" alt=\"GPT3 models\" style=\"zoom:60%;\" /> \n\nYet compared with previous models, GPT-3 is \"fatter\" ,meaning with same *d<sub>model</sub>* (192x of GPT3<sub>small</sub>), *n<sub>layers</sub>* is smaller (only 8x of GPT3<sub>small</sub>). \n\nThe mini-batch size goes up to 3.2M, definitely not mini. Large batch size improves the computational performance and parallelism while reducing the noise in each batch and making the model easier to overfitting. However, this defect is not evident in GPT-3. It is still an open question, now people consider it from two aspects: (1) the internal structure prevents the model from overfitting. (2) Such a large model is able to search for large space and it is more prone to converge to a simpler architecture.\n\nThe learning rate decreases with batch size increasing according to [research1](https://arxiv.org/abs/2001.08361) and [research2](https://arxiv.org/abs/1812.06162), also counter-intuitive. [Former work](https://arxiv.org/abs/1706.02677) shows that batch size should increase linear with the learning rate.\n\n#### Training Dataset\n\nWith a huge model, [Common Craw dataset](https://commoncrawl.org/) has come back as an option. The authors clean this dataset in 3 steps:\n\n- A logistic classification model is built, taking samples of Common Craw dataset as negative and WebText as positive. Such model is used on the whole dataset of Common Craw to predict positive (high quality) or negative. The positive stay and the negative are filtered.\n- [LSH algorithm](https://www.pinecone.io/learn/locality-sensitive-hashing/) is applied on the remaining dataset to filter the similar content.\n- More \"clean\" datasets are mixed in with weight.\n\n<img src=\"GPT3 training data.png\" alt=\"GPT3 training data\" style=\"zoom:50%;\" />\n\n#### Training Process\n\nSpecific training details are not presented. Information so far is [DGX-1 cluster](https://www.nvidia.com/en-us/data-center/dgx-1/) is used.\n\n### Results\n\nThe results are too many so only interesting figures are covered.\n\n<img src=\"GPT3 performance with compute.png\" alt=\"GPT3 performance with compute\" style=\"zoom:50%;\" />\n\nFrom the figure, the power law distribution of performance with compute are found, i.e. in order to decrease the validation loss linearly, the FLOPS need to increase exponentially. This still is a major problem in Machine Learning.\n\n<img src=\"GPT3 result.png\" alt=\"GPT3 result\" style=\"zoom:50%;\" />\n\nThis figure shows the compression of results with Zero-shot SOTO and human. Nothing to comment. Just ... good.\n\n<img src=\"GPT-3 result 2.png\" alt=\"GPT-3 result 2\" style=\"zoom:50%;\" />And on the Open-Domain QA tasks, GPT-3 outperform other models such as google T5. Google T5 can be considered as a model with both encoder and decoder.\n\n<img src=\"GPT3 result 3.png\" alt=\"GPT3 result 3\" style=\"zoom:50%;\" />\n\nAnd few shot learning outperform SOTO fine tuning models.\n\n<img src=\"GPT3 result4.png\" alt=\"GPT3 result4\" style=\"zoom:50%;\" />\n\nIn the machine translation task, it is interesting to see that other language to English is better than English to others.\n\n<img src=\"GPT 3 result 5.png\" alt=\"GPT 3 result 5\" style=\"zoom:50%;\" />\n\nThen a news article is generated with GPT-3 with numbers, years and time that makes the article look legit.\n\n### Limitations\n\n- Text synthesis, the predicted texts are always looping.\n- The bidirectional limit still exist because of the decoder structure.\n- The tokens that learned by GPT3 are equally weighted, yet wasting the model on the meaningless but high frequent function words.\n- No experience in Videos or real-world physical interaction.\n- The interpretation is still low. It's nearly impossible to know how does such a big model works.\n\n### Broader Impact\n\n- Can be used for fraud and crimes\n- Gender difference and race\n- Energy consuming \n\n<img src=\"GPT3 gender.png\" alt=\"GPT3 gender\" style=\"zoom:50%;\" />\n\n<img src=\"GPT3 race.png\" alt=\"GPT3 race\" style=\"zoom:50%;\" />\n\n\n\n## Reference\n\n[Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. *arXiv preprint arXiv:2010.11929*.]()\n\n[Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... & Sutskever, I. (2021, July). Learning transferable visual models from natural language supervision. In *International Conference on Machine Learning* (pp. 8748-8763). PMLR.](http://proceedings.mlr.press/v139/radford21a)\n\n[He, K., Chen, X., Xie, S., Li, Y., Dollár, P., & Girshick, R. (2021). Masked autoencoders are scalable vision learners. *arXiv preprint arXiv:2111.06377*.](https://arxiv.org/abs/2111.06377)\n\n[Child, R., Gray, S., Radford, A., & Sutskever, I. (2019). Generating long sequences with sparse transformers. *arXiv preprint arXiv:1904.10509*.](https://arxiv.org/abs/1904.10509)\n\n[Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., ... & Amodei, D. (2020). Scaling laws for neural language models. *arXiv preprint arXiv:2001.08361*.](https://arxiv.org/abs/2001.08361)\n\n[McCandlish, S., Kaplan, J., Amodei, D., & Team, O. D. (2018). An empirical model of large-batch training. *arXiv preprint arXiv:1812.06162*.](https://arxiv.org/abs/1812.06162)\n\n[Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., ... & He, K. (2017). Accurate, large minibatch sgd: Training imagenet in 1 hour. *arXiv preprint arXiv:1706.02677*.](https://arxiv.org/abs/1706.02677)\n","slug":"paper-reading-GPT1-3","published":1,"updated":"2022-04-30T19:31:08.178Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl2n5f40a000hp9ybhg9o1skr","content":"<blockquote>\n<p>GPT-3 is the most popular generative language model now. With more than 100 billion parameters, the performance is proved to be great and by now there are more than hundreds of works (commercial or academic) built on it, including the famous <a href=\"https://copilot.github.com/\">GitHub Copilot</a>.</p>\n</blockquote>\n<blockquote>\n<p>This is a <a href=\"https://daydreamatnight.github.io/2022/04/02/paper-reading-start/\">series of paper reading notes</a>, hopefully, to push me to read paper casually and to leave some record of what I've learned.</p>\n</blockquote>\n<span id=\"more\"></span>\n<p>Paper links:</p>\n<p>GPT-1: <a href=\"https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf\">Improving language understanding by generative pre-training</a></p>\n<p>GPT-2: <a href=\"http://www.persagen.com/files/misc/radford2019language.pdf\">Language models are unsupervised multitask learners</a></p>\n<p>GPT-3: <a href=\"https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html\">Language models are few-shot learners</a></p>\n<p>Useful links:</p>\n<p>https://www.bilibili.com/video/BV1AF411b7xQ/</p>\n<p><a href=\"https://gpt3demo.com/\">GPT-3 Demo: 300+ GPT-3 Examples, Demos, Apps</a></p>\n<p><a href=\"https://towardsdatascience.com/gpt-3-demos-use-cases-implications-77f86e540dc1\">GPT-3: Demos, Use-cases, Implications</a></p>\n<p><em>Since my own interest is not NLP, I haven't read these paper by myself. Instead, I follow a reference video above directly and make notes together with it.</em></p>\n<h2 id=\"history-and-timeline\">History and Timeline</h2>\n<p><img src=\"GPT timeline.png\" srcset=\"/img/loading.gif\" lazyload alt=\"GPT timeline\" style=\"zoom:50%;\" /></p>\n<p>Given that the GPT series are all developed by the <a href=\"https://openai.com/\">OpenAI</a> and the Transformer &amp; Bert are developed by Google, it seems there are two companies combating. And it is inevitable to compare these two series.</p>\n<p>From the perspective of number of cites, it is obviously that OpenAI have catch less attention from the academic world despite the huge cost. But it's not because GPT series are less novel, but because the goal of GPT series is bigger than that of Transformer &amp; Bert. Transformer is originally developed for Machine Translation task only. And Bert simply aims to push the pre-training technic forward. That's the reason why Bert performs better than GPT if the number of parameters are the same. As a result, GPT is harder and more expensive to train a decent model. And the size of model make others hardly to reconstruct it. From the companies' perspective, OpenAI does this because they want to build the strong AI, but Transformer &amp; Bert are developed only by the teams of Google.</p>\n<h2 id=\"gpt-1\">GPT-1</h2>\n<h3 id=\"abstract\">Abstract</h3>\n<p>The whole idea is similar to what CV did in the last several decades, that is pre-training of a model on a massive dataset followed by fine-tuning on a small specialist dataset. Lacking of large labelled data such as ImageNet of 10 million, however, NLP can't do what CV does exactly. The scale of machine translation database might reach 10 million but one piece of image possesses almost 10 times of information than a sentence. So the database is still not big enough.</p>\n<p>GPT makes a big step by using unlabelled data for pre-training. And then aero shot on GPT-2 makes another big step. It is fair to say CV led the trend of deep learning in the first 5 years, but recent years, more new thoughts are coming from the NLP field. And these new thoughts have inspired the CV field as well such as <a href=\"https://arxiv.org/abs/2010.11929\">ViT</a>, <a href=\"http://proceedings.mlr.press/v139/radford21a\">CLIP</a> and <a href=\"https://arxiv.org/abs/2111.06377\">MAE</a>.</p>\n<p>Besides, actually it had been a long time since NLP started to use unsupervised pre-training back then. For example, the word embedding model had been used for decades. But, the word embedding can only be seen as a layer, extra layers of model need to be designed to suit for various tasks. With GPT, however, the architecture doesn't need much change, only adjusting the input to suit the tasks is ok.</p>\n<p>The result is good but not as good as BERT, but the novelty of this paper is much better than BERT.</p>\n<h3 id=\"introduction\">Introduction</h3>\n<p>After briefly introducing the word embedding, problems of pre-training more than word-level data are presented. For example the type of optimisation objectives and how to transfer the extracted information to the tasks. The main reason of this problem is the variety of NLP tasks. There is no way to suit all the needs together.</p>\n<p>Then this paper introduces a semi-supervised method which has been explained many times. But actually GPT and BERT are normally called self-supervised model. Though they are the same to me. Semi-supervised learning is a common concept in the Machine Learning. It refers learning from a mix of massive unlabelled data and a few labelled data.</p>\n<p>Then the architecture is described. Interestingly, in order to list the reason of choosing transformer over RNN as backbone. The authors say</p>\n<blockquote>\n<p>This model choice provides us with a more structured memory for handling long-term dependencies in text, compared to alternatives like recurrent networks, resulting in robust transfer performance across diverse tasks.</p>\n</blockquote>\n<p>Besides, the paper accents the task-specific input adaptions, which is the key of this paper.</p>\n<h3 id=\"framework\">Framework</h3>\n<h4 id=\"unsupervised-learning\">Unsupervised learning</h4>\n<p>GPT uses a task that giving the data of u<sub>n-k</sub> to u<sub>n-1</sub> to predict the u<sub>n</sub>. So the likelihood function to be maximised is:</p>\n<p><img src=\"GPT loss.png\" srcset=\"/img/loading.gif\" lazyload alt=\"GPT loss\" style=\"zoom:25%;\" /></p>\n<p>Because the predicting task, the mechanism of the mask multi-head attention of transformer decoder matches the likelihood function perfectly. Because in the first layer of the transformer decoder, the data after u{i} are simply masked to be 0.</p>\n<p>And the whole pre-training process is like this:</p>\n<p><img src=\"GPT pretraining process.png\" srcset=\"/img/loading.gif\" lazyload alt=\"GPT pretraining process\" style=\"zoom:30%;\" /></p>\n<p>Compared with BERT, the biggest difference is never encoder or decoder, bidirectional or one-way along. The key is the pre-training task they choose, the completion task that Bert use is much easier than GPT's prediction task. Because this is the <strong>difference between interpolation and extrapolation</strong>. Therefore, BERT outperforms GPT on the same number of parameters. But the potential of the GPT series goes far beyond BERT. As a result, it took OpenAI years to develop such an impressive GPT-3 model. On another side, however, GPT's prediction task leads to a different architecture with BERT. And the decoder architecture makes the GPT model hard to be bidirectional from the start. We'll see how it conquer this.</p>\n<h4 id=\"supervised-fine-tuning\">Supervised fine-tuning</h4>\n<p>The fine-tuning task follows the standard supervise learning process as follows:</p>\n<p><img src=\"GPT fine tune f2.png\" srcset=\"/img/loading.gif\" lazyload alt=\"GPT fine tune f2\" style=\"zoom:40%;\" /></p>\n<p>with <img src=\"GPT fine tune f1.png\" srcset=\"/img/loading.gif\" lazyload alt=\"GPT fine tune f1\" style=\"zoom:40%;\" /></p>\n<p>And the authors find it helpful to optimise the L1 and L2 together as:</p>\n<p><img src=\"GPT fine tune loss.png\" srcset=\"/img/loading.gif\" lazyload alt=\"GPT fine tune loss\" style=\"zoom:40%;\" /></p>\n<h4 id=\"task-specific-input-transformations\">Task-specific input transformations</h4>\n<p>The last thing to do is how to apply this framework to different tasks. Similar to with BERT, the pre-trained transformer block doesn't have to be changed.</p>\n<p><img src=\"GPT objectives.png\" srcset=\"/img/loading.gif\" lazyload alt=\"GPT objectives\" style=\"zoom:40%;\" /></p>\n<h3 id=\"experiments\">Experiments</h3>\n<p>The dataset GPT trained on is <a href=\"https://paperswithcode.com/dataset/bookcorpus\">BooksCorpus dataset</a>, and the model has 12 layers with H<sub>model</sub> = 768, same as BERT<sub>base</sub>. Although the transformer encoder has one layer less than the decoder, GPT and BERT's numbers of parameters are still at the same level. Yet BERT has a 3 times larger BERT<sub>large</sub> model than the base. Because in addition to the BookCorpus dataset, BERT uses one more dataset for pre-training, in total the dataset is 4 times larger than GPT's.</p>\n<blockquote>\n<p>For the pre-training corpus we use the <a href=\"https://paperswithcode.com/dataset/bookcorpus\">BooksCorpus</a> (800M words) (Zhu et al., 2015) and <a href=\"https://en.wikipedia.org/wiki/Wikipedia:Database_download\">English Wikipedia</a> (2,500M words).</p>\n</blockquote>\n<p>And unfortunately, the average accuracy of BERT<sub>base</sub> is higher than GPT at this time. Besides with the BERT<sub>large</sub> model , the accuracy can go higher, as shown below in BERT's paper.</p>\n<p><img src=\"BERT result with GPT.png\" srcset=\"/img/loading.gif\" lazyload alt=\"BERT result with GPT\" style=\"zoom:50%;\" /></p>\n<h2 id=\"gpt-2\">GPT-2</h2>\n<p>After GPT-1 got defeated by BERT in 4 months, of course, GPT-2 aims to fight back and beat BERT to the ground. Considering the decoder path can't shift to the encoder with dignity, the simplest way then is to enlarge the model and the dataset. But what if it's still not working?</p>\n<h3 id=\"abstract-1\">Abstract</h3>\n<p>After developing a new dataset of millions of webpages called WebText, and training on a new1.5B parameter model. The result turns out to be no significant difference with BERT. So they bring out another sell point, zero shot.</p>\n<p>Actually, the zero-shot behaviour is mentioned in the last section of GPT-1's paper in order to understand more of the unsupervised pre-training mechanism. And in GPT-2, this behaviour is brought front to increase the novelty.</p>\n<h3 id=\"introduction-1\">Introduction</h3>\n<blockquote>\n<p><strong>Zero-shot learning</strong> (ZSL) is a problem setup in <a href=\"https://en.wikipedia.org/wiki/Machine_learning\">machine learning</a>, where at test time, a learner observes samples from classes, which were not observed during <a href=\"https://en.wikipedia.org/wiki/Machine_learning#Training_models\">training</a>, and needs to predict the class that they belong to. Zero-shot methods generally work by associating observed and non-observed classes through some form of auxiliary information, which encodes observable distinguishing properties of objects.[<a href=\"https://en.wikipedia.org/wiki/Zero-shot_learning#cite_note-1\">1]</a></p>\n</blockquote>\n<p>The main-steam approach is one dataset - one task instead of one dataset - multiple tasks because of the generalisation that state of art models lack. Yet multitask learning (trending 2000-2010) represents the idea of training one model with a combination of multiple datasets and different loss functions. So GPT-2 takes the idea of multitask learning and trains the model with the zero-shot setting, under which the downstream tasks can be handled with no collecting of supervised data or fine-tuning. The result is competitive and promising according to the authors.</p>\n<h3 id=\"approach\">Approach</h3>\n<p>The model architectures of GPT1 and GPT2 are pretty much the same. But the input methods are different.</p>\n<p>In detail, recalling that during fine tuning process, the GPT1 introduces extra tokens such as [start], [Delim] and [Extract] to modify the input. But in GPT2 without the supervised fine-tuning process, these extra tokens would cause confusion. As a result, the downstream task input need to be more likely to the natural language when constructing.</p>\n<p>As a result, the authors introduce what we are used to calling it \"prompt\", here are the examples:</p>\n<blockquote>\n<p>For example, a translation training example can be written as the sequence (<strong>translate to french</strong>, english text, french text). Likewise, a reading comprehension training example can be written as (<strong>answer the question</strong>, document, question, answer).</p>\n</blockquote>\n<p>And afterward, 2 ideas for why this would work are discussed. First, if the model is powerful enough, it might be capable of understanding the prompts. Second, in such a big dataset, this kind of data structure exists. Take machine translation as an example, there should be many sentences containing \"translate to French\", English text, and French text. The authors point out some of them below.</p>\n<table>\n<colgroup>\n<col style=\"width: 100%\" />\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Examples of machine translation</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">”I’m not the cleverest man in the world, but like they say in French: Je ne suis pas un imbecile [I’m not a fool]. <br /><br />In a now-deleted post from Aug. 16, Soheil Eid, Tory candidate in the riding of Joliette, wrote in French: ”Mentez mentez, il en restera toujours quelque chose,” which translates as, ”Lie lie and something will always remain.”<br /><br />“I hate the word ‘perfume,”’ Burr says. ‘It’s somewhat better in French: ‘parfum.’<br /><br />If listened carefully at 29:55, a conversation can be heard between two guys in French: “-Comment on fait pour aller de l’autre coté? -Quel autre coté?”, which means “- How do you get to the other side? - What side?”. <br /><br />If this sounds like a bit of a stretch, consider this question in French: As-tu aller au cinéma?, or Did you go to the movies?, which literally translates as Have-you to go to movies/theater? “<br /><br />Brevet Sans Garantie Du Gouvernement”, translated to English: “Patented without government warranty”.</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"training-data\"><strong>Training data</strong></h4>\n<p>After considering the need for larger data and the disadvantage(noise) of the existing dataset, they developed a new dataset. The data is collected by first crawling 45 million links discussed on Reddit, and then extracting all the contents of these pages. The dataset contains 8 million documents, 40 GB of text.</p>\n<h3 id=\"experiment\">Experiment</h3>\n<p>With 4 level of model, the one-short performances on 4 downstream tasks are shown blow:</p>\n<p><img src=\"GPT2 performance.png\" srcset=\"/img/loading.gif\" lazyload alt=\"GPT2 performance\" style=\"zoom:50%;\" /></p>\n<p>It can be seen that first 3 tasks, the results are not the best yet not the worst, however, the performance on Question Answering is bad, and other works perform way better than GPT-2.</p>\n<p>But this is not over, because in the figure, there is still room for performance improvement on all 4 tasks as the size of the larger model increases. So here comes GPT-3.</p>\n<h2 id=\"gpt-3\">GPT-3</h2>\n<p>The value of an article depends on the topic, effectiveness and novelty. GPT-2 has rather low effectiveness but strong novelty. As a result, GPT-3 aims to promote the effectiveness of its predecessor, while loosing the zero-shot condition to few-shot condition.</p>\n<h3 id=\"abstract-2\">Abstract</h3>\n<p>The parameters of GPT-3 are enlarged 10 times to 175 billion. When applying GPT-3 to downstream tasks, strong performance is achieved without gradient update or fine-tuning. Besides, GPT-3 is capable of generating articles that are indistinguishable from humans' work.</p>\n<p>Besides, unlikely to former papers, this is a 63-page technic report. Without limitation of words or page number, it is very detailed, especially for the experiment and discussion sections.</p>\n<h3 id=\"introduction-2\">Introduction</h3>\n<p>There are 3 problems of current pre-training + fine-tuning language model:</p>\n<ul>\n<li>A large labeled dataset is still needed for a good result</li>\n<li>The pre-training model is not really generalisable because of the need for fine-tuning</li>\n<li>Human doesn't need large fine tuning dataset</li>\n</ul>\n<p>When introducing their work, the authors try to re-define the concept of \"meta-learning\" and \"in-context learning\". What they really mean are training a huge generalisable model and train without updating the gradient, respectively.</p>\n<p>Then 3 evaluation methods are presented:</p>\n<ul>\n<li><p>Few-shot learning: 10-100 task-related data</p></li>\n<li><p>One-shot learning</p></li>\n<li><p>Zero-short learning</p></li>\n</ul>\n<p>And the performance with model size is plotted below (1.3B matches the GPT-2 model). Can be seen the final accuracy (few shot - 175B) almost doubled the former GPT-2 accuracy (1.3B zero-short)</p>\n<p><img src=\"GPT3 performance.png\" srcset=\"/img/loading.gif\" lazyload alt=\"GPT3 performance\" style=\"zoom:50%;\" /></p>\n<h3 id=\"approach-1\">Approach</h3>\n<p>In this section, fine-tuning, few-shot, one-shot, zero-shot learning are explained first in this figure below:</p>\n<p><img src=\"GPT-3 approach.png\" srcset=\"/img/loading.gif\" lazyload alt=\"GPT-3 approach\" style=\"zoom:50%;\" /></p>\n<p>The right column denotes the traditional fine-tuning process that requires extra few gradient update steps. The left side shows how GPT-3 applies \"in-context learning\" during inference. Basically, the input is divided by 3 parts, task description, example, and prompt. The task description and prompt are ended by \":\", and \"=&gt;\" respectively. The Transformer Decoder extracts the features of the \"context\" and then predicts the next several words starting from the prompt. As a result, the model is able to infer without gradient updates. Yet it comes 2 problems with few-shot:</p>\n<ul>\n<li>Unable to process really long examples. For example, thousands of English-French translation dataset is easy to get access but they can rarely be leveraged to promote the model performance.</li>\n<li>The inference is one-time thing, meaning the model can't actually learn from previous task description or example. Same samples have to be inputed again an again.</li>\n</ul>\n<p>As a result, the few-shot learning is not commonly used.</p>\n<h4 id=\"model-and-architectures\">Model and Architectures</h4>\n<p>GPT-3 uses the same architecture as GPT-2 yet uses the methods form \"<a href=\"https://arxiv.org/abs/1904.10509\">Sparse Transformer</a>\" to modify the layers. And 8 different size of models are developed.</p>\n<p><img src=\"GPT3 models.png\" srcset=\"/img/loading.gif\" lazyload alt=\"GPT3 models\" style=\"zoom:60%;\" /></p>\n<p>Yet compared with previous models, GPT-3 is \"fatter\" ,meaning with same <em>d<sub>model</sub></em> (192x of GPT3<sub>small</sub>), <em>n<sub>layers</sub></em> is smaller (only 8x of GPT3<sub>small</sub>).</p>\n<p>The mini-batch size goes up to 3.2M, definitely not mini. Large batch size improves the computational performance and parallelism while reducing the noise in each batch and making the model easier to overfitting. However, this defect is not evident in GPT-3. It is still an open question, now people consider it from two aspects: (1) the internal structure prevents the model from overfitting. (2) Such a large model is able to search for large space and it is more prone to converge to a simpler architecture.</p>\n<p>The learning rate decreases with batch size increasing according to <a href=\"https://arxiv.org/abs/2001.08361\">research1</a> and <a href=\"https://arxiv.org/abs/1812.06162\">research2</a>, also counter-intuitive. <a href=\"https://arxiv.org/abs/1706.02677\">Former work</a> shows that batch size should increase linear with the learning rate.</p>\n<h4 id=\"training-dataset\">Training Dataset</h4>\n<p>With a huge model, <a href=\"https://commoncrawl.org/\">Common Craw dataset</a> has come back as an option. The authors clean this dataset in 3 steps:</p>\n<ul>\n<li>A logistic classification model is built, taking samples of Common Craw dataset as negative and WebText as positive. Such model is used on the whole dataset of Common Craw to predict positive (high quality) or negative. The positive stay and the negative are filtered.</li>\n<li><a href=\"https://www.pinecone.io/learn/locality-sensitive-hashing/\">LSH algorithm</a> is applied on the remaining dataset to filter the similar content.</li>\n<li>More \"clean\" datasets are mixed in with weight.</li>\n</ul>\n<p><img src=\"GPT3 training data.png\" srcset=\"/img/loading.gif\" lazyload alt=\"GPT3 training data\" style=\"zoom:50%;\" /></p>\n<h4 id=\"training-process\">Training Process</h4>\n<p>Specific training details are not presented. Information so far is <a href=\"https://www.nvidia.com/en-us/data-center/dgx-1/\">DGX-1 cluster</a> is used.</p>\n<h3 id=\"results\">Results</h3>\n<p>The results are too many so only interesting figures are covered.</p>\n<p><img src=\"GPT3 performance with compute.png\" srcset=\"/img/loading.gif\" lazyload alt=\"GPT3 performance with compute\" style=\"zoom:50%;\" /></p>\n<p>From the figure, the power law distribution of performance with compute are found, i.e. in order to decrease the validation loss linearly, the FLOPS need to increase exponentially. This still is a major problem in Machine Learning.</p>\n<p><img src=\"GPT3 result.png\" srcset=\"/img/loading.gif\" lazyload alt=\"GPT3 result\" style=\"zoom:50%;\" /></p>\n<p>This figure shows the compression of results with Zero-shot SOTO and human. Nothing to comment. Just ... good.</p>\n<p><img src=\"GPT-3 result 2.png\" srcset=\"/img/loading.gif\" lazyload alt=\"GPT-3 result 2\" style=\"zoom:50%;\" />And on the Open-Domain QA tasks, GPT-3 outperform other models such as google T5. Google T5 can be considered as a model with both encoder and decoder.</p>\n<p><img src=\"GPT3 result 3.png\" srcset=\"/img/loading.gif\" lazyload alt=\"GPT3 result 3\" style=\"zoom:50%;\" /></p>\n<p>And few shot learning outperform SOTO fine tuning models.</p>\n<p><img src=\"GPT3 result4.png\" srcset=\"/img/loading.gif\" lazyload alt=\"GPT3 result4\" style=\"zoom:50%;\" /></p>\n<p>In the machine translation task, it is interesting to see that other language to English is better than English to others.</p>\n<p><img src=\"GPT 3 result 5.png\" srcset=\"/img/loading.gif\" lazyload alt=\"GPT 3 result 5\" style=\"zoom:50%;\" /></p>\n<p>Then a news article is generated with GPT-3 with numbers, years and time that makes the article look legit.</p>\n<h3 id=\"limitations\">Limitations</h3>\n<ul>\n<li>Text synthesis, the predicted texts are always looping.</li>\n<li>The bidirectional limit still exist because of the decoder structure.</li>\n<li>The tokens that learned by GPT3 are equally weighted, yet wasting the model on the meaningless but high frequent function words.</li>\n<li>No experience in Videos or real-world physical interaction.</li>\n<li>The interpretation is still low. It's nearly impossible to know how does such a big model works.</li>\n</ul>\n<h3 id=\"broader-impact\">Broader Impact</h3>\n<ul>\n<li>Can be used for fraud and crimes</li>\n<li>Gender difference and race</li>\n<li>Energy consuming</li>\n</ul>\n<p><img src=\"GPT3 gender.png\" srcset=\"/img/loading.gif\" lazyload alt=\"GPT3 gender\" style=\"zoom:50%;\" /></p>\n<p><img src=\"GPT3 race.png\" srcset=\"/img/loading.gif\" lazyload alt=\"GPT3 race\" style=\"zoom:50%;\" /></p>\n<h2 id=\"reference\">Reference</h2>\n<p><a href=\"\">Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... &amp; Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. <em>arXiv preprint arXiv:2010.11929</em>.</a></p>\n<p><a href=\"http://proceedings.mlr.press/v139/radford21a\">Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... &amp; Sutskever, I. (2021, July). Learning transferable visual models from natural language supervision. In <em>International Conference on Machine Learning</em> (pp. 8748-8763). PMLR.</a></p>\n<p><a href=\"https://arxiv.org/abs/2111.06377\">He, K., Chen, X., Xie, S., Li, Y., Dollár, P., &amp; Girshick, R. (2021). Masked autoencoders are scalable vision learners. <em>arXiv preprint arXiv:2111.06377</em>.</a></p>\n<p><a href=\"https://arxiv.org/abs/1904.10509\">Child, R., Gray, S., Radford, A., &amp; Sutskever, I. (2019). Generating long sequences with sparse transformers. <em>arXiv preprint arXiv:1904.10509</em>.</a></p>\n<p><a href=\"https://arxiv.org/abs/2001.08361\">Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., ... &amp; Amodei, D. (2020). Scaling laws for neural language models. <em>arXiv preprint arXiv:2001.08361</em>.</a></p>\n<p><a href=\"https://arxiv.org/abs/1812.06162\">McCandlish, S., Kaplan, J., Amodei, D., &amp; Team, O. D. (2018). An empirical model of large-batch training. <em>arXiv preprint arXiv:1812.06162</em>.</a></p>\n<p><a href=\"https://arxiv.org/abs/1706.02677\">Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., ... &amp; He, K. (2017). Accurate, large minibatch sgd: Training imagenet in 1 hour. <em>arXiv preprint arXiv:1706.02677</em>.</a></p>\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>GPT-3 is the most popular generative language model now. With more than 100 billion parameters, the performance is proved to be great and by now there are more than hundreds of works (commercial or academic) built on it, including the famous <a href=\"https://copilot.github.com/\">GitHub Copilot</a>.</p>\n</blockquote>\n<blockquote>\n<p>This is a <a href=\"https://daydreamatnight.github.io/2022/04/02/paper-reading-start/\">series of paper reading notes</a>, hopefully, to push me to read paper casually and to leave some record of what I've learned.</p>\n</blockquote>","more":"<p>Paper links:</p>\n<p>GPT-1: <a href=\"https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf\">Improving language understanding by generative pre-training</a></p>\n<p>GPT-2: <a href=\"http://www.persagen.com/files/misc/radford2019language.pdf\">Language models are unsupervised multitask learners</a></p>\n<p>GPT-3: <a href=\"https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html\">Language models are few-shot learners</a></p>\n<p>Useful links:</p>\n<p>https://www.bilibili.com/video/BV1AF411b7xQ/</p>\n<p><a href=\"https://gpt3demo.com/\">GPT-3 Demo: 300+ GPT-3 Examples, Demos, Apps</a></p>\n<p><a href=\"https://towardsdatascience.com/gpt-3-demos-use-cases-implications-77f86e540dc1\">GPT-3: Demos, Use-cases, Implications</a></p>\n<p><em>Since my own interest is not NLP, I haven't read these paper by myself. Instead, I follow a reference video above directly and make notes together with it.</em></p>\n<h2 id=\"history-and-timeline\">History and Timeline</h2>\n<p><img src=\"GPT timeline.png\" alt=\"GPT timeline\" style=\"zoom:50%;\" /></p>\n<p>Given that the GPT series are all developed by the <a href=\"https://openai.com/\">OpenAI</a> and the Transformer &amp; Bert are developed by Google, it seems there are two companies combating. And it is inevitable to compare these two series.</p>\n<p>From the perspective of number of cites, it is obviously that OpenAI have catch less attention from the academic world despite the huge cost. But it's not because GPT series are less novel, but because the goal of GPT series is bigger than that of Transformer &amp; Bert. Transformer is originally developed for Machine Translation task only. And Bert simply aims to push the pre-training technic forward. That's the reason why Bert performs better than GPT if the number of parameters are the same. As a result, GPT is harder and more expensive to train a decent model. And the size of model make others hardly to reconstruct it. From the companies' perspective, OpenAI does this because they want to build the strong AI, but Transformer &amp; Bert are developed only by the teams of Google.</p>\n<h2 id=\"gpt-1\">GPT-1</h2>\n<h3 id=\"abstract\">Abstract</h3>\n<p>The whole idea is similar to what CV did in the last several decades, that is pre-training of a model on a massive dataset followed by fine-tuning on a small specialist dataset. Lacking of large labelled data such as ImageNet of 10 million, however, NLP can't do what CV does exactly. The scale of machine translation database might reach 10 million but one piece of image possesses almost 10 times of information than a sentence. So the database is still not big enough.</p>\n<p>GPT makes a big step by using unlabelled data for pre-training. And then aero shot on GPT-2 makes another big step. It is fair to say CV led the trend of deep learning in the first 5 years, but recent years, more new thoughts are coming from the NLP field. And these new thoughts have inspired the CV field as well such as <a href=\"https://arxiv.org/abs/2010.11929\">ViT</a>, <a href=\"http://proceedings.mlr.press/v139/radford21a\">CLIP</a> and <a href=\"https://arxiv.org/abs/2111.06377\">MAE</a>.</p>\n<p>Besides, actually it had been a long time since NLP started to use unsupervised pre-training back then. For example, the word embedding model had been used for decades. But, the word embedding can only be seen as a layer, extra layers of model need to be designed to suit for various tasks. With GPT, however, the architecture doesn't need much change, only adjusting the input to suit the tasks is ok.</p>\n<p>The result is good but not as good as BERT, but the novelty of this paper is much better than BERT.</p>\n<h3 id=\"introduction\">Introduction</h3>\n<p>After briefly introducing the word embedding, problems of pre-training more than word-level data are presented. For example the type of optimisation objectives and how to transfer the extracted information to the tasks. The main reason of this problem is the variety of NLP tasks. There is no way to suit all the needs together.</p>\n<p>Then this paper introduces a semi-supervised method which has been explained many times. But actually GPT and BERT are normally called self-supervised model. Though they are the same to me. Semi-supervised learning is a common concept in the Machine Learning. It refers learning from a mix of massive unlabelled data and a few labelled data.</p>\n<p>Then the architecture is described. Interestingly, in order to list the reason of choosing transformer over RNN as backbone. The authors say</p>\n<blockquote>\n<p>This model choice provides us with a more structured memory for handling long-term dependencies in text, compared to alternatives like recurrent networks, resulting in robust transfer performance across diverse tasks.</p>\n</blockquote>\n<p>Besides, the paper accents the task-specific input adaptions, which is the key of this paper.</p>\n<h3 id=\"framework\">Framework</h3>\n<h4 id=\"unsupervised-learning\">Unsupervised learning</h4>\n<p>GPT uses a task that giving the data of u<sub>n-k</sub> to u<sub>n-1</sub> to predict the u<sub>n</sub>. So the likelihood function to be maximised is:</p>\n<p><img src=\"GPT loss.png\" alt=\"GPT loss\" style=\"zoom:25%;\" /></p>\n<p>Because the predicting task, the mechanism of the mask multi-head attention of transformer decoder matches the likelihood function perfectly. Because in the first layer of the transformer decoder, the data after u{i} are simply masked to be 0.</p>\n<p>And the whole pre-training process is like this:</p>\n<p><img src=\"GPT pretraining process.png\" alt=\"GPT pretraining process\" style=\"zoom:30%;\" /></p>\n<p>Compared with BERT, the biggest difference is never encoder or decoder, bidirectional or one-way along. The key is the pre-training task they choose, the completion task that Bert use is much easier than GPT's prediction task. Because this is the <strong>difference between interpolation and extrapolation</strong>. Therefore, BERT outperforms GPT on the same number of parameters. But the potential of the GPT series goes far beyond BERT. As a result, it took OpenAI years to develop such an impressive GPT-3 model. On another side, however, GPT's prediction task leads to a different architecture with BERT. And the decoder architecture makes the GPT model hard to be bidirectional from the start. We'll see how it conquer this.</p>\n<h4 id=\"supervised-fine-tuning\">Supervised fine-tuning</h4>\n<p>The fine-tuning task follows the standard supervise learning process as follows:</p>\n<p><img src=\"GPT fine tune f2.png\" alt=\"GPT fine tune f2\" style=\"zoom:40%;\" /></p>\n<p>with <img src=\"GPT fine tune f1.png\" alt=\"GPT fine tune f1\" style=\"zoom:40%;\" /></p>\n<p>And the authors find it helpful to optimise the L1 and L2 together as:</p>\n<p><img src=\"GPT fine tune loss.png\" alt=\"GPT fine tune loss\" style=\"zoom:40%;\" /></p>\n<h4 id=\"task-specific-input-transformations\">Task-specific input transformations</h4>\n<p>The last thing to do is how to apply this framework to different tasks. Similar to with BERT, the pre-trained transformer block doesn't have to be changed.</p>\n<p><img src=\"GPT objectives.png\" alt=\"GPT objectives\" style=\"zoom:40%;\" /></p>\n<h3 id=\"experiments\">Experiments</h3>\n<p>The dataset GPT trained on is <a href=\"https://paperswithcode.com/dataset/bookcorpus\">BooksCorpus dataset</a>, and the model has 12 layers with H<sub>model</sub> = 768, same as BERT<sub>base</sub>. Although the transformer encoder has one layer less than the decoder, GPT and BERT's numbers of parameters are still at the same level. Yet BERT has a 3 times larger BERT<sub>large</sub> model than the base. Because in addition to the BookCorpus dataset, BERT uses one more dataset for pre-training, in total the dataset is 4 times larger than GPT's.</p>\n<blockquote>\n<p>For the pre-training corpus we use the <a href=\"https://paperswithcode.com/dataset/bookcorpus\">BooksCorpus</a> (800M words) (Zhu et al., 2015) and <a href=\"https://en.wikipedia.org/wiki/Wikipedia:Database_download\">English Wikipedia</a> (2,500M words).</p>\n</blockquote>\n<p>And unfortunately, the average accuracy of BERT<sub>base</sub> is higher than GPT at this time. Besides with the BERT<sub>large</sub> model , the accuracy can go higher, as shown below in BERT's paper.</p>\n<p><img src=\"BERT result with GPT.png\" alt=\"BERT result with GPT\" style=\"zoom:50%;\" /></p>\n<h2 id=\"gpt-2\">GPT-2</h2>\n<p>After GPT-1 got defeated by BERT in 4 months, of course, GPT-2 aims to fight back and beat BERT to the ground. Considering the decoder path can't shift to the encoder with dignity, the simplest way then is to enlarge the model and the dataset. But what if it's still not working?</p>\n<h3 id=\"abstract-1\">Abstract</h3>\n<p>After developing a new dataset of millions of webpages called WebText, and training on a new1.5B parameter model. The result turns out to be no significant difference with BERT. So they bring out another sell point, zero shot.</p>\n<p>Actually, the zero-shot behaviour is mentioned in the last section of GPT-1's paper in order to understand more of the unsupervised pre-training mechanism. And in GPT-2, this behaviour is brought front to increase the novelty.</p>\n<h3 id=\"introduction-1\">Introduction</h3>\n<blockquote>\n<p><strong>Zero-shot learning</strong> (ZSL) is a problem setup in <a href=\"https://en.wikipedia.org/wiki/Machine_learning\">machine learning</a>, where at test time, a learner observes samples from classes, which were not observed during <a href=\"https://en.wikipedia.org/wiki/Machine_learning#Training_models\">training</a>, and needs to predict the class that they belong to. Zero-shot methods generally work by associating observed and non-observed classes through some form of auxiliary information, which encodes observable distinguishing properties of objects.[<a href=\"https://en.wikipedia.org/wiki/Zero-shot_learning#cite_note-1\">1]</a></p>\n</blockquote>\n<p>The main-steam approach is one dataset - one task instead of one dataset - multiple tasks because of the generalisation that state of art models lack. Yet multitask learning (trending 2000-2010) represents the idea of training one model with a combination of multiple datasets and different loss functions. So GPT-2 takes the idea of multitask learning and trains the model with the zero-shot setting, under which the downstream tasks can be handled with no collecting of supervised data or fine-tuning. The result is competitive and promising according to the authors.</p>\n<h3 id=\"approach\">Approach</h3>\n<p>The model architectures of GPT1 and GPT2 are pretty much the same. But the input methods are different.</p>\n<p>In detail, recalling that during fine tuning process, the GPT1 introduces extra tokens such as [start], [Delim] and [Extract] to modify the input. But in GPT2 without the supervised fine-tuning process, these extra tokens would cause confusion. As a result, the downstream task input need to be more likely to the natural language when constructing.</p>\n<p>As a result, the authors introduce what we are used to calling it \"prompt\", here are the examples:</p>\n<blockquote>\n<p>For example, a translation training example can be written as the sequence (<strong>translate to french</strong>, english text, french text). Likewise, a reading comprehension training example can be written as (<strong>answer the question</strong>, document, question, answer).</p>\n</blockquote>\n<p>And afterward, 2 ideas for why this would work are discussed. First, if the model is powerful enough, it might be capable of understanding the prompts. Second, in such a big dataset, this kind of data structure exists. Take machine translation as an example, there should be many sentences containing \"translate to French\", English text, and French text. The authors point out some of them below.</p>\n<table>\n<colgroup>\n<col style=\"width: 100%\" />\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Examples of machine translation</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">”I’m not the cleverest man in the world, but like they say in French: Je ne suis pas un imbecile [I’m not a fool]. <br /><br />In a now-deleted post from Aug. 16, Soheil Eid, Tory candidate in the riding of Joliette, wrote in French: ”Mentez mentez, il en restera toujours quelque chose,” which translates as, ”Lie lie and something will always remain.”<br /><br />“I hate the word ‘perfume,”’ Burr says. ‘It’s somewhat better in French: ‘parfum.’<br /><br />If listened carefully at 29:55, a conversation can be heard between two guys in French: “-Comment on fait pour aller de l’autre coté? -Quel autre coté?”, which means “- How do you get to the other side? - What side?”. <br /><br />If this sounds like a bit of a stretch, consider this question in French: As-tu aller au cinéma?, or Did you go to the movies?, which literally translates as Have-you to go to movies/theater? “<br /><br />Brevet Sans Garantie Du Gouvernement”, translated to English: “Patented without government warranty”.</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"training-data\"><strong>Training data</strong></h4>\n<p>After considering the need for larger data and the disadvantage(noise) of the existing dataset, they developed a new dataset. The data is collected by first crawling 45 million links discussed on Reddit, and then extracting all the contents of these pages. The dataset contains 8 million documents, 40 GB of text.</p>\n<h3 id=\"experiment\">Experiment</h3>\n<p>With 4 level of model, the one-short performances on 4 downstream tasks are shown blow:</p>\n<p><img src=\"GPT2 performance.png\" alt=\"GPT2 performance\" style=\"zoom:50%;\" /></p>\n<p>It can be seen that first 3 tasks, the results are not the best yet not the worst, however, the performance on Question Answering is bad, and other works perform way better than GPT-2.</p>\n<p>But this is not over, because in the figure, there is still room for performance improvement on all 4 tasks as the size of the larger model increases. So here comes GPT-3.</p>\n<h2 id=\"gpt-3\">GPT-3</h2>\n<p>The value of an article depends on the topic, effectiveness and novelty. GPT-2 has rather low effectiveness but strong novelty. As a result, GPT-3 aims to promote the effectiveness of its predecessor, while loosing the zero-shot condition to few-shot condition.</p>\n<h3 id=\"abstract-2\">Abstract</h3>\n<p>The parameters of GPT-3 are enlarged 10 times to 175 billion. When applying GPT-3 to downstream tasks, strong performance is achieved without gradient update or fine-tuning. Besides, GPT-3 is capable of generating articles that are indistinguishable from humans' work.</p>\n<p>Besides, unlikely to former papers, this is a 63-page technic report. Without limitation of words or page number, it is very detailed, especially for the experiment and discussion sections.</p>\n<h3 id=\"introduction-2\">Introduction</h3>\n<p>There are 3 problems of current pre-training + fine-tuning language model:</p>\n<ul>\n<li>A large labeled dataset is still needed for a good result</li>\n<li>The pre-training model is not really generalisable because of the need for fine-tuning</li>\n<li>Human doesn't need large fine tuning dataset</li>\n</ul>\n<p>When introducing their work, the authors try to re-define the concept of \"meta-learning\" and \"in-context learning\". What they really mean are training a huge generalisable model and train without updating the gradient, respectively.</p>\n<p>Then 3 evaluation methods are presented:</p>\n<ul>\n<li><p>Few-shot learning: 10-100 task-related data</p></li>\n<li><p>One-shot learning</p></li>\n<li><p>Zero-short learning</p></li>\n</ul>\n<p>And the performance with model size is plotted below (1.3B matches the GPT-2 model). Can be seen the final accuracy (few shot - 175B) almost doubled the former GPT-2 accuracy (1.3B zero-short)</p>\n<p><img src=\"GPT3 performance.png\" alt=\"GPT3 performance\" style=\"zoom:50%;\" /></p>\n<h3 id=\"approach-1\">Approach</h3>\n<p>In this section, fine-tuning, few-shot, one-shot, zero-shot learning are explained first in this figure below:</p>\n<p><img src=\"GPT-3 approach.png\" alt=\"GPT-3 approach\" style=\"zoom:50%;\" /></p>\n<p>The right column denotes the traditional fine-tuning process that requires extra few gradient update steps. The left side shows how GPT-3 applies \"in-context learning\" during inference. Basically, the input is divided by 3 parts, task description, example, and prompt. The task description and prompt are ended by \":\", and \"=&gt;\" respectively. The Transformer Decoder extracts the features of the \"context\" and then predicts the next several words starting from the prompt. As a result, the model is able to infer without gradient updates. Yet it comes 2 problems with few-shot:</p>\n<ul>\n<li>Unable to process really long examples. For example, thousands of English-French translation dataset is easy to get access but they can rarely be leveraged to promote the model performance.</li>\n<li>The inference is one-time thing, meaning the model can't actually learn from previous task description or example. Same samples have to be inputed again an again.</li>\n</ul>\n<p>As a result, the few-shot learning is not commonly used.</p>\n<h4 id=\"model-and-architectures\">Model and Architectures</h4>\n<p>GPT-3 uses the same architecture as GPT-2 yet uses the methods form \"<a href=\"https://arxiv.org/abs/1904.10509\">Sparse Transformer</a>\" to modify the layers. And 8 different size of models are developed.</p>\n<p><img src=\"GPT3 models.png\" alt=\"GPT3 models\" style=\"zoom:60%;\" /></p>\n<p>Yet compared with previous models, GPT-3 is \"fatter\" ,meaning with same <em>d<sub>model</sub></em> (192x of GPT3<sub>small</sub>), <em>n<sub>layers</sub></em> is smaller (only 8x of GPT3<sub>small</sub>).</p>\n<p>The mini-batch size goes up to 3.2M, definitely not mini. Large batch size improves the computational performance and parallelism while reducing the noise in each batch and making the model easier to overfitting. However, this defect is not evident in GPT-3. It is still an open question, now people consider it from two aspects: (1) the internal structure prevents the model from overfitting. (2) Such a large model is able to search for large space and it is more prone to converge to a simpler architecture.</p>\n<p>The learning rate decreases with batch size increasing according to <a href=\"https://arxiv.org/abs/2001.08361\">research1</a> and <a href=\"https://arxiv.org/abs/1812.06162\">research2</a>, also counter-intuitive. <a href=\"https://arxiv.org/abs/1706.02677\">Former work</a> shows that batch size should increase linear with the learning rate.</p>\n<h4 id=\"training-dataset\">Training Dataset</h4>\n<p>With a huge model, <a href=\"https://commoncrawl.org/\">Common Craw dataset</a> has come back as an option. The authors clean this dataset in 3 steps:</p>\n<ul>\n<li>A logistic classification model is built, taking samples of Common Craw dataset as negative and WebText as positive. Such model is used on the whole dataset of Common Craw to predict positive (high quality) or negative. The positive stay and the negative are filtered.</li>\n<li><a href=\"https://www.pinecone.io/learn/locality-sensitive-hashing/\">LSH algorithm</a> is applied on the remaining dataset to filter the similar content.</li>\n<li>More \"clean\" datasets are mixed in with weight.</li>\n</ul>\n<p><img src=\"GPT3 training data.png\" alt=\"GPT3 training data\" style=\"zoom:50%;\" /></p>\n<h4 id=\"training-process\">Training Process</h4>\n<p>Specific training details are not presented. Information so far is <a href=\"https://www.nvidia.com/en-us/data-center/dgx-1/\">DGX-1 cluster</a> is used.</p>\n<h3 id=\"results\">Results</h3>\n<p>The results are too many so only interesting figures are covered.</p>\n<p><img src=\"GPT3 performance with compute.png\" alt=\"GPT3 performance with compute\" style=\"zoom:50%;\" /></p>\n<p>From the figure, the power law distribution of performance with compute are found, i.e. in order to decrease the validation loss linearly, the FLOPS need to increase exponentially. This still is a major problem in Machine Learning.</p>\n<p><img src=\"GPT3 result.png\" alt=\"GPT3 result\" style=\"zoom:50%;\" /></p>\n<p>This figure shows the compression of results with Zero-shot SOTO and human. Nothing to comment. Just ... good.</p>\n<p><img src=\"GPT-3 result 2.png\" alt=\"GPT-3 result 2\" style=\"zoom:50%;\" />And on the Open-Domain QA tasks, GPT-3 outperform other models such as google T5. Google T5 can be considered as a model with both encoder and decoder.</p>\n<p><img src=\"GPT3 result 3.png\" alt=\"GPT3 result 3\" style=\"zoom:50%;\" /></p>\n<p>And few shot learning outperform SOTO fine tuning models.</p>\n<p><img src=\"GPT3 result4.png\" alt=\"GPT3 result4\" style=\"zoom:50%;\" /></p>\n<p>In the machine translation task, it is interesting to see that other language to English is better than English to others.</p>\n<p><img src=\"GPT 3 result 5.png\" alt=\"GPT 3 result 5\" style=\"zoom:50%;\" /></p>\n<p>Then a news article is generated with GPT-3 with numbers, years and time that makes the article look legit.</p>\n<h3 id=\"limitations\">Limitations</h3>\n<ul>\n<li>Text synthesis, the predicted texts are always looping.</li>\n<li>The bidirectional limit still exist because of the decoder structure.</li>\n<li>The tokens that learned by GPT3 are equally weighted, yet wasting the model on the meaningless but high frequent function words.</li>\n<li>No experience in Videos or real-world physical interaction.</li>\n<li>The interpretation is still low. It's nearly impossible to know how does such a big model works.</li>\n</ul>\n<h3 id=\"broader-impact\">Broader Impact</h3>\n<ul>\n<li>Can be used for fraud and crimes</li>\n<li>Gender difference and race</li>\n<li>Energy consuming</li>\n</ul>\n<p><img src=\"GPT3 gender.png\" alt=\"GPT3 gender\" style=\"zoom:50%;\" /></p>\n<p><img src=\"GPT3 race.png\" alt=\"GPT3 race\" style=\"zoom:50%;\" /></p>\n<h2 id=\"reference\">Reference</h2>\n<p><a href=\"\">Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... &amp; Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. <em>arXiv preprint arXiv:2010.11929</em>.</a></p>\n<p><a href=\"http://proceedings.mlr.press/v139/radford21a\">Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... &amp; Sutskever, I. (2021, July). Learning transferable visual models from natural language supervision. In <em>International Conference on Machine Learning</em> (pp. 8748-8763). PMLR.</a></p>\n<p><a href=\"https://arxiv.org/abs/2111.06377\">He, K., Chen, X., Xie, S., Li, Y., Dollár, P., &amp; Girshick, R. (2021). Masked autoencoders are scalable vision learners. <em>arXiv preprint arXiv:2111.06377</em>.</a></p>\n<p><a href=\"https://arxiv.org/abs/1904.10509\">Child, R., Gray, S., Radford, A., &amp; Sutskever, I. (2019). Generating long sequences with sparse transformers. <em>arXiv preprint arXiv:1904.10509</em>.</a></p>\n<p><a href=\"https://arxiv.org/abs/2001.08361\">Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., ... &amp; Amodei, D. (2020). Scaling laws for neural language models. <em>arXiv preprint arXiv:2001.08361</em>.</a></p>\n<p><a href=\"https://arxiv.org/abs/1812.06162\">McCandlish, S., Kaplan, J., Amodei, D., &amp; Team, O. D. (2018). An empirical model of large-batch training. <em>arXiv preprint arXiv:1812.06162</em>.</a></p>\n<p><a href=\"https://arxiv.org/abs/1706.02677\">Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., ... &amp; He, K. (2017). Accurate, large minibatch sgd: Training imagenet in 1 hour. <em>arXiv preprint arXiv:1706.02677</em>.</a></p>","wordcount":15434},{"title":"paper reading: ResNet","author":"Ryan LI","toc":true,"declare":true,"date":"2022-04-09T12:42:00.000Z","_content":"\n> Since its introduction in 2015, ResNet and its variants have accounted for 50% of deep neural networks in use. The idea of \"Residual\" has been proved to be efficient and important to deep NN.\n\n> This is a [series of paper reading notes](https://daydreamatnight.github.io/2022/04/02/paper-reading-start/), hopefully, to push me to read paper casually and to leave some record of what I've learned.\n\n<!-- more -->\n\npaper link: [Deep residual learning for image recognition](http://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html)\n\nuseful link: https://www.bilibili.com/video/BV1Fb4y1h73E\n\n## Notes by sections\n\n### 0. Abstract \n\n> An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\n>\n> Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset\n\nNot only the 1st place results on several tasks but the potential to train a 1000-layer network made this Residual framework a huge attention. \n\nUnfortunately, due to the 8-page limitation of CDPR, and the massive number of results to be presented, there is no room for the conclusion section in this paper. \n\n### Key figure\n\n<img src=\"ResNet figure 4.png\" alt=\"ResNet figure 1\" style=\"zoom:80%;\" />\n\nThis is the key figure showing that the degradation in accuracy caused by the depth in plain networks has been well addressed in ResNet.\n\n### 1. Introduction \n\nThis section first presents the need of training deeper neural network. And the first obstacle that encountered during this process is non-convergence cause by gradient vanishing / explosion, and it has been well addressed by normalised initialisation and intermediate normalisation layers.\n\nAnd the main focus of this paper is the second obstacle i.e. deeper networks have difficulty converging to lower losses and may perform worse than networks with fewer layers. And they addressed this obstacle by introducing the Residual learning framework as shown in the pic below. And this framework is easy to implement  in [caffe](https://caffe.berkeleyvision.org/), the most popular DL framework back to 2015.\n\n<img src=\"ResNet figure 2.png\" alt=\"ResNet figure 2\" style=\"zoom:30%;\" />\n\nAfterwards experiments results are briefly mentioned. \nThe introduction section played as an expanded version of the abstract and the residual method is mainly focused, which is helpful for readers to catch the essence of the whole paper.\n\n### 2. Related work\n\n#### Residual representation\n\nActually the concept of residual is more common in the fields of statistics and machine learning. For example in linear regression, the residual denotes the distance between the estimated and the actual results (residual=y- y ̂ in 2D). And the iterative process of calculating the regression line aims to minimise the mean square of the residual loss. In addition, the well-known GB gradient boosting algorithm for machine learning is also based on the residual loss.\n\nBecause this paper mainly focus on computer vision, so these early work isn't included.\n\n#### Shortcut connections\n\nIt looks like this paper combines these two well-studied approaches with amazing results. First ideas are not necessary to make a paper a classic. Just like the quote on the Google Scholar homepage: *stand on the shoulders of giants.*\n\n### 3. Deep Residual Learning\n\n> Let us consider H(x) as an **underlying mapping** to be fit by a few stacked layers (not necessarily the entire net), with x denoting the inputs to the first of these layers.\n\nFirst is the meaning of H(x), underlying mapping is actually intuitive but it confused me for a while, so here's the answer below:\n\n[What does the phrase 'underlying mapping' mean? - Data Science Stack](https://datascience.stackexchange.com/questions/92617/what-does-the-phrase-underlying-mapping-mean)\n\n> > Functions map domains to ranges. Neural networks learn such functions, so you can think of a neural network as a mapping of input spaces to output spaces. Deep neural networks are stacked with many layers of course, and each of those can be viewed as sub-functions of the network with their own underlying mappings. For example, each layer in a convolution network consists of some convolution layers + some other helper layers such as normalisation and pooling.\n\nNext the paper brings up 2 methods to address the shape difference between X the input and H(x) the output for one particular layer that may occur. Option (A): 0 padding and Option (B): 1*1 convolution to project the channel and pooling with stride to adjust the height and width.\n\n### 4. Experiments\n\n#### Identity vs. Projection Shortcuts\n\n<img src=\"  ResNet table 3.png\" alt=\"ResNet table 3\" style=\"zoom:30%;\" />\n\nAfter introducing the well known architectures, the 2 methods to address the shape difference are also studied. 3 groups are studies,  Option (A): 0 padding,  Option (B): projection when necessary, Option (C): projection to all layers. And because of the increasing of parameters caused by the projection, it is not surprise to discover C is better than B than A. And Option (B) is the winner considering both the performance and the efficiency. \n\n#### Deeper Bottleneck Architectures\n\n<img src=\"  ResNet figure 5.png\" alt=\"ResNet figure 5\" style=\"zoom:30%;\" />\n\nIn ResNet18 and ResNet 34, the standard architecture is fine. But for deeper network, in order to decrease the flops, the bottleneck architectures is deployed. Basically in each residual block, decreasing the number of channel by 1\\*1 convolution first then expend the channel back to the number of input channel. As can be seen in Table 1, the flops of ResNet34 is similar to that in ResNet50. But this is only theory, in practice ResNet50 is obviously more expensive because of the inefficiency of computing 1\\*1 conv.\n\n<img src=\"  ResNet table 1.png\" alt=\"ResNet table 1\" style=\"zoom:80%;\" />\n\n#### Exploring Over 1000 layers\n\n<img src=\"ResNet table 6.png\" alt=\"ResNet table 6\" style=\"zoom:30%;\" />\n\nNetworks with layer numbers from 20 to 1202 are applied on dataset CIFAR-10 (With only output size 32\\*32 compared with >300\\*300 on ImageNet, the networks are slightly modified). And the results shows that even with an aggressive depth on a tiny dataset, no difficulty of optimisation occurs. Yet shown on Table6, the test set classification error goes up compared with the shallower models because of the overfitting.\n\n## Some Reviews\n\nThis is a model with relatively simple idea and the authors have a great writing skill to make this paper neat and clear.\n\nThe main contribution of this paper is introducing the residual block and skipping connection in deep learning.\n\nAlthough the authors intuitively explain and provide some experiments, the explanation is not currently accepted by the mainstream. Today, the reason why ResNet can achieve better results than ordinary networks is mainly because of its property of preventing vanishing gradients. The ordinary network without the residual framework cannot be trained in the later stage of training.\n\nIt's still an open question why the 1000-layer network has low level of overfitting on a simple dataset. The same question is good performance on large networks such as the transformer series. One explanation is that despite the large network size, the intrinsic model complexity is low.\n\n","source":"_posts/paper-reading-ResNet.md","raw":"---\ntitle: 'paper reading: ResNet'\nauthor: Ryan LI\ntoc: true\ndeclare: true\ndate: 2022-04-09 20:42:00\ntags:\n  - paper reading\n  - deep learning\n---\n\n> Since its introduction in 2015, ResNet and its variants have accounted for 50% of deep neural networks in use. The idea of \"Residual\" has been proved to be efficient and important to deep NN.\n\n> This is a [series of paper reading notes](https://daydreamatnight.github.io/2022/04/02/paper-reading-start/), hopefully, to push me to read paper casually and to leave some record of what I've learned.\n\n<!-- more -->\n\npaper link: [Deep residual learning for image recognition](http://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html)\n\nuseful link: https://www.bilibili.com/video/BV1Fb4y1h73E\n\n## Notes by sections\n\n### 0. Abstract \n\n> An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\n>\n> Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset\n\nNot only the 1st place results on several tasks but the potential to train a 1000-layer network made this Residual framework a huge attention. \n\nUnfortunately, due to the 8-page limitation of CDPR, and the massive number of results to be presented, there is no room for the conclusion section in this paper. \n\n### Key figure\n\n<img src=\"ResNet figure 4.png\" alt=\"ResNet figure 1\" style=\"zoom:80%;\" />\n\nThis is the key figure showing that the degradation in accuracy caused by the depth in plain networks has been well addressed in ResNet.\n\n### 1. Introduction \n\nThis section first presents the need of training deeper neural network. And the first obstacle that encountered during this process is non-convergence cause by gradient vanishing / explosion, and it has been well addressed by normalised initialisation and intermediate normalisation layers.\n\nAnd the main focus of this paper is the second obstacle i.e. deeper networks have difficulty converging to lower losses and may perform worse than networks with fewer layers. And they addressed this obstacle by introducing the Residual learning framework as shown in the pic below. And this framework is easy to implement  in [caffe](https://caffe.berkeleyvision.org/), the most popular DL framework back to 2015.\n\n<img src=\"ResNet figure 2.png\" alt=\"ResNet figure 2\" style=\"zoom:30%;\" />\n\nAfterwards experiments results are briefly mentioned. \nThe introduction section played as an expanded version of the abstract and the residual method is mainly focused, which is helpful for readers to catch the essence of the whole paper.\n\n### 2. Related work\n\n#### Residual representation\n\nActually the concept of residual is more common in the fields of statistics and machine learning. For example in linear regression, the residual denotes the distance between the estimated and the actual results (residual=y- y ̂ in 2D). And the iterative process of calculating the regression line aims to minimise the mean square of the residual loss. In addition, the well-known GB gradient boosting algorithm for machine learning is also based on the residual loss.\n\nBecause this paper mainly focus on computer vision, so these early work isn't included.\n\n#### Shortcut connections\n\nIt looks like this paper combines these two well-studied approaches with amazing results. First ideas are not necessary to make a paper a classic. Just like the quote on the Google Scholar homepage: *stand on the shoulders of giants.*\n\n### 3. Deep Residual Learning\n\n> Let us consider H(x) as an **underlying mapping** to be fit by a few stacked layers (not necessarily the entire net), with x denoting the inputs to the first of these layers.\n\nFirst is the meaning of H(x), underlying mapping is actually intuitive but it confused me for a while, so here's the answer below:\n\n[What does the phrase 'underlying mapping' mean? - Data Science Stack](https://datascience.stackexchange.com/questions/92617/what-does-the-phrase-underlying-mapping-mean)\n\n> > Functions map domains to ranges. Neural networks learn such functions, so you can think of a neural network as a mapping of input spaces to output spaces. Deep neural networks are stacked with many layers of course, and each of those can be viewed as sub-functions of the network with their own underlying mappings. For example, each layer in a convolution network consists of some convolution layers + some other helper layers such as normalisation and pooling.\n\nNext the paper brings up 2 methods to address the shape difference between X the input and H(x) the output for one particular layer that may occur. Option (A): 0 padding and Option (B): 1*1 convolution to project the channel and pooling with stride to adjust the height and width.\n\n### 4. Experiments\n\n#### Identity vs. Projection Shortcuts\n\n<img src=\"  ResNet table 3.png\" alt=\"ResNet table 3\" style=\"zoom:30%;\" />\n\nAfter introducing the well known architectures, the 2 methods to address the shape difference are also studied. 3 groups are studies,  Option (A): 0 padding,  Option (B): projection when necessary, Option (C): projection to all layers. And because of the increasing of parameters caused by the projection, it is not surprise to discover C is better than B than A. And Option (B) is the winner considering both the performance and the efficiency. \n\n#### Deeper Bottleneck Architectures\n\n<img src=\"  ResNet figure 5.png\" alt=\"ResNet figure 5\" style=\"zoom:30%;\" />\n\nIn ResNet18 and ResNet 34, the standard architecture is fine. But for deeper network, in order to decrease the flops, the bottleneck architectures is deployed. Basically in each residual block, decreasing the number of channel by 1\\*1 convolution first then expend the channel back to the number of input channel. As can be seen in Table 1, the flops of ResNet34 is similar to that in ResNet50. But this is only theory, in practice ResNet50 is obviously more expensive because of the inefficiency of computing 1\\*1 conv.\n\n<img src=\"  ResNet table 1.png\" alt=\"ResNet table 1\" style=\"zoom:80%;\" />\n\n#### Exploring Over 1000 layers\n\n<img src=\"ResNet table 6.png\" alt=\"ResNet table 6\" style=\"zoom:30%;\" />\n\nNetworks with layer numbers from 20 to 1202 are applied on dataset CIFAR-10 (With only output size 32\\*32 compared with >300\\*300 on ImageNet, the networks are slightly modified). And the results shows that even with an aggressive depth on a tiny dataset, no difficulty of optimisation occurs. Yet shown on Table6, the test set classification error goes up compared with the shallower models because of the overfitting.\n\n## Some Reviews\n\nThis is a model with relatively simple idea and the authors have a great writing skill to make this paper neat and clear.\n\nThe main contribution of this paper is introducing the residual block and skipping connection in deep learning.\n\nAlthough the authors intuitively explain and provide some experiments, the explanation is not currently accepted by the mainstream. Today, the reason why ResNet can achieve better results than ordinary networks is mainly because of its property of preventing vanishing gradients. The ordinary network without the residual framework cannot be trained in the later stage of training.\n\nIt's still an open question why the 1000-layer network has low level of overfitting on a simple dataset. The same question is good performance on large networks such as the transformer series. One explanation is that despite the large network size, the intrinsic model complexity is low.\n\n","slug":"paper-reading-ResNet","published":1,"updated":"2022-04-30T19:31:14.476Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl2n5f40a000jp9yb7z537myw","content":"<blockquote>\n<p>Since its introduction in 2015, ResNet and its variants have accounted for 50% of deep neural networks in use. The idea of \"Residual\" has been proved to be efficient and important to deep NN.</p>\n</blockquote>\n<blockquote>\n<p>This is a <a href=\"https://daydreamatnight.github.io/2022/04/02/paper-reading-start/\">series of paper reading notes</a>, hopefully, to push me to read paper casually and to leave some record of what I've learned.</p>\n</blockquote>\n<span id=\"more\"></span>\n<p>paper link: <a href=\"http://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">Deep residual learning for image recognition</a></p>\n<p>useful link: https://www.bilibili.com/video/BV1Fb4y1h73E</p>\n<h2 id=\"notes-by-sections\">Notes by sections</h2>\n<h3 id=\"abstract\">0. Abstract</h3>\n<blockquote>\n<p>An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.</p>\n<p>Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset</p>\n</blockquote>\n<p>Not only the 1st place results on several tasks but the potential to train a 1000-layer network made this Residual framework a huge attention.</p>\n<p>Unfortunately, due to the 8-page limitation of CDPR, and the massive number of results to be presented, there is no room for the conclusion section in this paper.</p>\n<h3 id=\"key-figure\">Key figure</h3>\n<p><img src=\"ResNet figure 4.png\" srcset=\"/img/loading.gif\" lazyload alt=\"ResNet figure 1\" style=\"zoom:80%;\" /></p>\n<p>This is the key figure showing that the degradation in accuracy caused by the depth in plain networks has been well addressed in ResNet.</p>\n<h3 id=\"introduction\">1. Introduction</h3>\n<p>This section first presents the need of training deeper neural network. And the first obstacle that encountered during this process is non-convergence cause by gradient vanishing / explosion, and it has been well addressed by normalised initialisation and intermediate normalisation layers.</p>\n<p>And the main focus of this paper is the second obstacle i.e. deeper networks have difficulty converging to lower losses and may perform worse than networks with fewer layers. And they addressed this obstacle by introducing the Residual learning framework as shown in the pic below. And this framework is easy to implement in <a href=\"https://caffe.berkeleyvision.org/\">caffe</a>, the most popular DL framework back to 2015.</p>\n<p><img src=\"ResNet figure 2.png\" srcset=\"/img/loading.gif\" lazyload alt=\"ResNet figure 2\" style=\"zoom:30%;\" /></p>\n<p>Afterwards experiments results are briefly mentioned. The introduction section played as an expanded version of the abstract and the residual method is mainly focused, which is helpful for readers to catch the essence of the whole paper.</p>\n<h3 id=\"related-work\">2. Related work</h3>\n<h4 id=\"residual-representation\">Residual representation</h4>\n<p>Actually the concept of residual is more common in the fields of statistics and machine learning. For example in linear regression, the residual denotes the distance between the estimated and the actual results (residual=y- y ̂ in 2D). And the iterative process of calculating the regression line aims to minimise the mean square of the residual loss. In addition, the well-known GB gradient boosting algorithm for machine learning is also based on the residual loss.</p>\n<p>Because this paper mainly focus on computer vision, so these early work isn't included.</p>\n<h4 id=\"shortcut-connections\">Shortcut connections</h4>\n<p>It looks like this paper combines these two well-studied approaches with amazing results. First ideas are not necessary to make a paper a classic. Just like the quote on the Google Scholar homepage: <em>stand on the shoulders of giants.</em></p>\n<h3 id=\"deep-residual-learning\">3. Deep Residual Learning</h3>\n<blockquote>\n<p>Let us consider H(x) as an <strong>underlying mapping</strong> to be fit by a few stacked layers (not necessarily the entire net), with x denoting the inputs to the first of these layers.</p>\n</blockquote>\n<p>First is the meaning of H(x), underlying mapping is actually intuitive but it confused me for a while, so here's the answer below:</p>\n<p><a href=\"https://datascience.stackexchange.com/questions/92617/what-does-the-phrase-underlying-mapping-mean\">What does the phrase 'underlying mapping' mean? - Data Science Stack</a></p>\n<blockquote>\n<blockquote>\n<p>Functions map domains to ranges. Neural networks learn such functions, so you can think of a neural network as a mapping of input spaces to output spaces. Deep neural networks are stacked with many layers of course, and each of those can be viewed as sub-functions of the network with their own underlying mappings. For example, each layer in a convolution network consists of some convolution layers + some other helper layers such as normalisation and pooling.</p>\n</blockquote>\n</blockquote>\n<p>Next the paper brings up 2 methods to address the shape difference between X the input and H(x) the output for one particular layer that may occur. Option (A): 0 padding and Option (B): 1*1 convolution to project the channel and pooling with stride to adjust the height and width.</p>\n<h3 id=\"experiments\">4. Experiments</h3>\n<h4 id=\"identity-vs.-projection-shortcuts\">Identity vs. Projection Shortcuts</h4>\n<p><img src=\"  ResNet table 3.png\" srcset=\"/img/loading.gif\" lazyload alt=\"ResNet table 3\" style=\"zoom:30%;\" /></p>\n<p>After introducing the well known architectures, the 2 methods to address the shape difference are also studied. 3 groups are studies, Option (A): 0 padding, Option (B): projection when necessary, Option (C): projection to all layers. And because of the increasing of parameters caused by the projection, it is not surprise to discover C is better than B than A. And Option (B) is the winner considering both the performance and the efficiency.</p>\n<h4 id=\"deeper-bottleneck-architectures\">Deeper Bottleneck Architectures</h4>\n<p><img src=\"  ResNet figure 5.png\" srcset=\"/img/loading.gif\" lazyload alt=\"ResNet figure 5\" style=\"zoom:30%;\" /></p>\n<p>In ResNet18 and ResNet 34, the standard architecture is fine. But for deeper network, in order to decrease the flops, the bottleneck architectures is deployed. Basically in each residual block, decreasing the number of channel by 1*1 convolution first then expend the channel back to the number of input channel. As can be seen in Table 1, the flops of ResNet34 is similar to that in ResNet50. But this is only theory, in practice ResNet50 is obviously more expensive because of the inefficiency of computing 1*1 conv.</p>\n<p><img src=\"  ResNet table 1.png\" srcset=\"/img/loading.gif\" lazyload alt=\"ResNet table 1\" style=\"zoom:80%;\" /></p>\n<h4 id=\"exploring-over-1000-layers\">Exploring Over 1000 layers</h4>\n<p><img src=\"ResNet table 6.png\" srcset=\"/img/loading.gif\" lazyload alt=\"ResNet table 6\" style=\"zoom:30%;\" /></p>\n<p>Networks with layer numbers from 20 to 1202 are applied on dataset CIFAR-10 (With only output size 32*32 compared with &gt;300*300 on ImageNet, the networks are slightly modified). And the results shows that even with an aggressive depth on a tiny dataset, no difficulty of optimisation occurs. Yet shown on Table6, the test set classification error goes up compared with the shallower models because of the overfitting.</p>\n<h2 id=\"some-reviews\">Some Reviews</h2>\n<p>This is a model with relatively simple idea and the authors have a great writing skill to make this paper neat and clear.</p>\n<p>The main contribution of this paper is introducing the residual block and skipping connection in deep learning.</p>\n<p>Although the authors intuitively explain and provide some experiments, the explanation is not currently accepted by the mainstream. Today, the reason why ResNet can achieve better results than ordinary networks is mainly because of its property of preventing vanishing gradients. The ordinary network without the residual framework cannot be trained in the later stage of training.</p>\n<p>It's still an open question why the 1000-layer network has low level of overfitting on a simple dataset. The same question is good performance on large networks such as the transformer series. One explanation is that despite the large network size, the intrinsic model complexity is low.</p>\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>Since its introduction in 2015, ResNet and its variants have accounted for 50% of deep neural networks in use. The idea of \"Residual\" has been proved to be efficient and important to deep NN.</p>\n</blockquote>\n<blockquote>\n<p>This is a <a href=\"https://daydreamatnight.github.io/2022/04/02/paper-reading-start/\">series of paper reading notes</a>, hopefully, to push me to read paper casually and to leave some record of what I've learned.</p>\n</blockquote>","more":"<p>paper link: <a href=\"http://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">Deep residual learning for image recognition</a></p>\n<p>useful link: https://www.bilibili.com/video/BV1Fb4y1h73E</p>\n<h2 id=\"notes-by-sections\">Notes by sections</h2>\n<h3 id=\"abstract\">0. Abstract</h3>\n<blockquote>\n<p>An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.</p>\n<p>Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset</p>\n</blockquote>\n<p>Not only the 1st place results on several tasks but the potential to train a 1000-layer network made this Residual framework a huge attention.</p>\n<p>Unfortunately, due to the 8-page limitation of CDPR, and the massive number of results to be presented, there is no room for the conclusion section in this paper.</p>\n<h3 id=\"key-figure\">Key figure</h3>\n<p><img src=\"ResNet figure 4.png\" alt=\"ResNet figure 1\" style=\"zoom:80%;\" /></p>\n<p>This is the key figure showing that the degradation in accuracy caused by the depth in plain networks has been well addressed in ResNet.</p>\n<h3 id=\"introduction\">1. Introduction</h3>\n<p>This section first presents the need of training deeper neural network. And the first obstacle that encountered during this process is non-convergence cause by gradient vanishing / explosion, and it has been well addressed by normalised initialisation and intermediate normalisation layers.</p>\n<p>And the main focus of this paper is the second obstacle i.e. deeper networks have difficulty converging to lower losses and may perform worse than networks with fewer layers. And they addressed this obstacle by introducing the Residual learning framework as shown in the pic below. And this framework is easy to implement in <a href=\"https://caffe.berkeleyvision.org/\">caffe</a>, the most popular DL framework back to 2015.</p>\n<p><img src=\"ResNet figure 2.png\" alt=\"ResNet figure 2\" style=\"zoom:30%;\" /></p>\n<p>Afterwards experiments results are briefly mentioned. The introduction section played as an expanded version of the abstract and the residual method is mainly focused, which is helpful for readers to catch the essence of the whole paper.</p>\n<h3 id=\"related-work\">2. Related work</h3>\n<h4 id=\"residual-representation\">Residual representation</h4>\n<p>Actually the concept of residual is more common in the fields of statistics and machine learning. For example in linear regression, the residual denotes the distance between the estimated and the actual results (residual=y- y ̂ in 2D). And the iterative process of calculating the regression line aims to minimise the mean square of the residual loss. In addition, the well-known GB gradient boosting algorithm for machine learning is also based on the residual loss.</p>\n<p>Because this paper mainly focus on computer vision, so these early work isn't included.</p>\n<h4 id=\"shortcut-connections\">Shortcut connections</h4>\n<p>It looks like this paper combines these two well-studied approaches with amazing results. First ideas are not necessary to make a paper a classic. Just like the quote on the Google Scholar homepage: <em>stand on the shoulders of giants.</em></p>\n<h3 id=\"deep-residual-learning\">3. Deep Residual Learning</h3>\n<blockquote>\n<p>Let us consider H(x) as an <strong>underlying mapping</strong> to be fit by a few stacked layers (not necessarily the entire net), with x denoting the inputs to the first of these layers.</p>\n</blockquote>\n<p>First is the meaning of H(x), underlying mapping is actually intuitive but it confused me for a while, so here's the answer below:</p>\n<p><a href=\"https://datascience.stackexchange.com/questions/92617/what-does-the-phrase-underlying-mapping-mean\">What does the phrase 'underlying mapping' mean? - Data Science Stack</a></p>\n<blockquote>\n<blockquote>\n<p>Functions map domains to ranges. Neural networks learn such functions, so you can think of a neural network as a mapping of input spaces to output spaces. Deep neural networks are stacked with many layers of course, and each of those can be viewed as sub-functions of the network with their own underlying mappings. For example, each layer in a convolution network consists of some convolution layers + some other helper layers such as normalisation and pooling.</p>\n</blockquote>\n</blockquote>\n<p>Next the paper brings up 2 methods to address the shape difference between X the input and H(x) the output for one particular layer that may occur. Option (A): 0 padding and Option (B): 1*1 convolution to project the channel and pooling with stride to adjust the height and width.</p>\n<h3 id=\"experiments\">4. Experiments</h3>\n<h4 id=\"identity-vs.-projection-shortcuts\">Identity vs. Projection Shortcuts</h4>\n<p><img src=\"  ResNet table 3.png\" alt=\"ResNet table 3\" style=\"zoom:30%;\" /></p>\n<p>After introducing the well known architectures, the 2 methods to address the shape difference are also studied. 3 groups are studies, Option (A): 0 padding, Option (B): projection when necessary, Option (C): projection to all layers. And because of the increasing of parameters caused by the projection, it is not surprise to discover C is better than B than A. And Option (B) is the winner considering both the performance and the efficiency.</p>\n<h4 id=\"deeper-bottleneck-architectures\">Deeper Bottleneck Architectures</h4>\n<p><img src=\"  ResNet figure 5.png\" alt=\"ResNet figure 5\" style=\"zoom:30%;\" /></p>\n<p>In ResNet18 and ResNet 34, the standard architecture is fine. But for deeper network, in order to decrease the flops, the bottleneck architectures is deployed. Basically in each residual block, decreasing the number of channel by 1*1 convolution first then expend the channel back to the number of input channel. As can be seen in Table 1, the flops of ResNet34 is similar to that in ResNet50. But this is only theory, in practice ResNet50 is obviously more expensive because of the inefficiency of computing 1*1 conv.</p>\n<p><img src=\"  ResNet table 1.png\" alt=\"ResNet table 1\" style=\"zoom:80%;\" /></p>\n<h4 id=\"exploring-over-1000-layers\">Exploring Over 1000 layers</h4>\n<p><img src=\"ResNet table 6.png\" alt=\"ResNet table 6\" style=\"zoom:30%;\" /></p>\n<p>Networks with layer numbers from 20 to 1202 are applied on dataset CIFAR-10 (With only output size 32*32 compared with &gt;300*300 on ImageNet, the networks are slightly modified). And the results shows that even with an aggressive depth on a tiny dataset, no difficulty of optimisation occurs. Yet shown on Table6, the test set classification error goes up compared with the shallower models because of the overfitting.</p>\n<h2 id=\"some-reviews\">Some Reviews</h2>\n<p>This is a model with relatively simple idea and the authors have a great writing skill to make this paper neat and clear.</p>\n<p>The main contribution of this paper is introducing the residual block and skipping connection in deep learning.</p>\n<p>Although the authors intuitively explain and provide some experiments, the explanation is not currently accepted by the mainstream. Today, the reason why ResNet can achieve better results than ordinary networks is mainly because of its property of preventing vanishing gradients. The ordinary network without the residual framework cannot be trained in the later stage of training.</p>\n<p>It's still an open question why the 1000-layer network has low level of overfitting on a simple dataset. The same question is good performance on large networks such as the transformer series. One explanation is that despite the large network size, the intrinsic model complexity is low.</p>","wordcount":5514},{"title":"paper reading: bert","author":"Ryan LI","toc":true,"declare":true,"date":"2022-04-15T10:01:32.000Z","_content":">The BERT is the most important achievement in the NLP field in the last 4 years. It makes the transfer learning of NLP tasks possible and the transformer framework dominant the NLP field.\n\n> This is a [series of paper reading notes](https://daydreamatnight.github.io/2022/04/02/paper-reading-start/), hopefully, to push me to read paper casually and to leave some record of what I've learned.\n\n<!-- more -->\n\nPaper link: [Bert: Pre-training of deep bidirectional transformers for language understanding](https://arxiv.org/abs/1810.04805)\n\nUseful links: https://www.bilibili.com/video/BV1PL411M7eQ\n\n​\t\t\t\t\thttps://youtu.be/UYPa347-DdE\n\n​\t\t\t\t\t[Deep contextualized word representations - arXiv](https://arxiv.org/abs/1802.05365)\n\n## Notes by sections\n\n### 0. Abstract \n\n*The name of BERT might come from one of its important related work, ELMo. And Elmo and Bert are both characters in a TV show Sesame Street.*\n\nThe abstract focus on the two related work, ELMo and GPT. The bidirectional feature is in contrast to the unidirectional GPT model. And the \"without substantial task specific architecture modifications\" feature compares with the RNN-based ELMo model, whose architecture might get modified when training downstream tasks. Yet the ELMo model is bidirectional and GPT model is easy to use.\n\nWhen claiming a model is great, it is good to provide both the absolute accuracy and the relative accuracy compared with others. Just like this paper does.\n\n### 6. Conclusion \n\nTo summarise, this is a classical **A+B** type of research. The idea of BERT is simple, combine the advantages of the bidirectional network with a rather old RNN base (ELMo), and the unidirectional network with a transformer base (GPT). And in detail, 2 pre-training tasks are designed. \n\nThe main contribution of this network is showing that bidirectional information is important.\n\n### 1. Introduction\n\nFrom the history introduction, it turns out BERT is not the first to apply pre-training on NLP. It's been a while. But BERT makes it popular. \n\n### 2. Related work\n\n**2.3 Transfer Learning from Supervised data**\n\nThis is what the CV area does most often. Yet it is not effective in NLP. It is partly because of the lacking of data. Another reason might be the existing labeled data focus only on language inference and machine translation, which are too different from other NLP tasks. So BERT and GPT use unlabelled data to pre-train and prove that unsupervised pre-train on a massive data is more effective than supervised pre-train on a relatively small data set. And, interestingly, this trend in NLP gradually effects the CV world. Nowadays unsupervised fine-tuning is becoming more and more popular in CV area.\n\n### 3. Bert\n\nIn the first part, the pre-training and fine-tuning framework is briefly covered. A paper should be self-consistent, meaning that if a mechanic is fundamental in your area and essential to your work. It is a good habit to briefly introduce it, even if all the people in the area know it.\n\n**Model Architecture**\n\nAnd the architecture is as simple as just take the encoder part of the transformer model.\n\n> number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.\n>\n> BERTBASE(L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE(L=24, H=1024, A=16, Total Parameters=340M).\n\nBecause the hidden size of each multi-head attention sublayer is set as 64. And in transformer tradition, H = A * 64. So the multi-head number A actually depends on the hidden size H. As a result, the way of calculating the number of parameters is shown below.\n\n<img src=\"BERT learnable paramters.png\" alt=\"BERT learnable paramters\" style=\"zoom:48%;\" />\n\n**Input/Output Representations**\n\nWhy Bert need a pair of sentence to handle downstream tasks such as Machine Translation, unlike its predecessor Transformer?\n\nBecause in Transformer, the input is a pair of sequences, taken by encoder and decoder respectively. But Bert is only an encoder. In English-Chinese Translation task for example, for transformer, the encoder take the English version and the decoder take the Chinese version, for Bert, the English version and Chinese version are glued with a special token [sept] then be inputed to the model. \n\nBesides the [sept] token, another embedding is introduced into the embedding layer, the sequence model. Details are shown below.\n\n<img src=\"BERT segment embedding.png\" alt=\"BERT segment embedding\" style=\"zoom:50%;\" />\n\n**3.1 Pre-training BERT**\n\nThe paper provides an example of each task in Appendix section **Masked LM and the Masking Procedure**, just go for it and have a look if don't understand.\n\n> Next Sentence Prediction The next sentence prediction task can be illustrated in the following examples. \n>\n> Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP] \n>\n> Label = IsNext \n>\n> Input = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are **flight ##less** birds [SEP] \n>\n> Label = NotNext\n\nThe \"flight ##less\" is because of the WordPiece embedding method that Bert uses. \"##\" means this token is split from last token, in this case, flightless is the original token. Because flightless is rarely used, the WordPiece embedding split this word into two.\n\n**3.2 Fine-tuning BERT**\n\nBert's architecture has one advantage over the transformer's, the self-attention allows model look both the two sentences. And the encoder-decoder model can't do that. As a result, the fine-tuning can be a little bit hazy.\n\n### 4 Experiments\n\n**4.2 SQuAD v1.1**\n\n> We fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32.\n\nThis misleads the people for a while. People found when fine-tuning with Bert, the variance of each results are high i.e. the result of fine-tuning is unstable. Then people found it is because 3 epochs are not enough. Besides, the optimiser that the original Bert model used is an incomplete version of Adam which is not stable for small epoch number. And the follow-ups change it into the original Adam.\n\nFrom the processes of the 3 experiments, it can be seen it is easy for BERT to be applied to downstream tasks. Just need to modify the input data in the form of the Bert sequence, and add another output layer. As a result, with BERT, massive number of tasks can be trained under a rather simple architecture.\n\n### 5 Ablation Studies\n\n<img src=\"BERT alibation study 1.png\" alt=\"BERT alibation study 1\" style=\"zoom:30%;\" />\n\nIt is obvious the 4th architecture BiLSTM is from the idea of ELMO. And all the variation lead to a deterioration of the acc, especially in the MRPC task.\n\n> **Microsoft Research Paraphrase Corpus (MRPC) Dataset**\n>\n> Created by Dolan et al. at 2005, the Microsoft Research Paraphrase Corpus (MRPC) Dataset contains pairs of sentences which have been extracted from news sources on the web, along with human annotations indicating whether each pair captures a paraphrase/semantic equivalence relationship., in English language. Containing 5,8 in Text file format.\n\n\n\n<img src=\"model size graph.webp\" alt=\"model size graph\" style=\"zoom:50%;\" />\n\nIn the Effect of model size part, they claim with a large model size, huge improvement can be reached.  And this leads a trend of increasing the model size in NLP, for example, the 100 billon GPT-3, and 500 billion model Megatron-Turing Natural Language Generation (MT-NLG). The boundary of NLP will be push further. \n\n## Reviews\n\nWriting: The biggest sell point in this paper is chosen as the \"bidirectional\". From today's view, the contributions of BERT are so more than this. Besides, when say to choose a feature, it is better to discuss both the pros and the cons of the choice. For example, compared with GPT, the encoder is used instead of the decoder. The pros are the bidirectional feature, but the cons is the resulting difficulty in applying on generative tasks such as the machine translation. \n\nBesides, BERT follows a whole ideal path of solving deep learning problems. That is after pre-training on a deep and huge model on a huge unlabelled dataset, the model can be applied to many small tasks with a few steps of fine tuning.\n","source":"_posts/paper-reading-bert.md","raw":"---\ntitle: 'paper reading: bert'\nauthor: Ryan LI\ntoc: true\ndeclare: true\ndate: 2022-04-15 18:01:32\ntags:\n  - paper reading\n  - deep learning\n---\n>The BERT is the most important achievement in the NLP field in the last 4 years. It makes the transfer learning of NLP tasks possible and the transformer framework dominant the NLP field.\n\n> This is a [series of paper reading notes](https://daydreamatnight.github.io/2022/04/02/paper-reading-start/), hopefully, to push me to read paper casually and to leave some record of what I've learned.\n\n<!-- more -->\n\nPaper link: [Bert: Pre-training of deep bidirectional transformers for language understanding](https://arxiv.org/abs/1810.04805)\n\nUseful links: https://www.bilibili.com/video/BV1PL411M7eQ\n\n​\t\t\t\t\thttps://youtu.be/UYPa347-DdE\n\n​\t\t\t\t\t[Deep contextualized word representations - arXiv](https://arxiv.org/abs/1802.05365)\n\n## Notes by sections\n\n### 0. Abstract \n\n*The name of BERT might come from one of its important related work, ELMo. And Elmo and Bert are both characters in a TV show Sesame Street.*\n\nThe abstract focus on the two related work, ELMo and GPT. The bidirectional feature is in contrast to the unidirectional GPT model. And the \"without substantial task specific architecture modifications\" feature compares with the RNN-based ELMo model, whose architecture might get modified when training downstream tasks. Yet the ELMo model is bidirectional and GPT model is easy to use.\n\nWhen claiming a model is great, it is good to provide both the absolute accuracy and the relative accuracy compared with others. Just like this paper does.\n\n### 6. Conclusion \n\nTo summarise, this is a classical **A+B** type of research. The idea of BERT is simple, combine the advantages of the bidirectional network with a rather old RNN base (ELMo), and the unidirectional network with a transformer base (GPT). And in detail, 2 pre-training tasks are designed. \n\nThe main contribution of this network is showing that bidirectional information is important.\n\n### 1. Introduction\n\nFrom the history introduction, it turns out BERT is not the first to apply pre-training on NLP. It's been a while. But BERT makes it popular. \n\n### 2. Related work\n\n**2.3 Transfer Learning from Supervised data**\n\nThis is what the CV area does most often. Yet it is not effective in NLP. It is partly because of the lacking of data. Another reason might be the existing labeled data focus only on language inference and machine translation, which are too different from other NLP tasks. So BERT and GPT use unlabelled data to pre-train and prove that unsupervised pre-train on a massive data is more effective than supervised pre-train on a relatively small data set. And, interestingly, this trend in NLP gradually effects the CV world. Nowadays unsupervised fine-tuning is becoming more and more popular in CV area.\n\n### 3. Bert\n\nIn the first part, the pre-training and fine-tuning framework is briefly covered. A paper should be self-consistent, meaning that if a mechanic is fundamental in your area and essential to your work. It is a good habit to briefly introduce it, even if all the people in the area know it.\n\n**Model Architecture**\n\nAnd the architecture is as simple as just take the encoder part of the transformer model.\n\n> number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.\n>\n> BERTBASE(L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE(L=24, H=1024, A=16, Total Parameters=340M).\n\nBecause the hidden size of each multi-head attention sublayer is set as 64. And in transformer tradition, H = A * 64. So the multi-head number A actually depends on the hidden size H. As a result, the way of calculating the number of parameters is shown below.\n\n<img src=\"BERT learnable paramters.png\" alt=\"BERT learnable paramters\" style=\"zoom:48%;\" />\n\n**Input/Output Representations**\n\nWhy Bert need a pair of sentence to handle downstream tasks such as Machine Translation, unlike its predecessor Transformer?\n\nBecause in Transformer, the input is a pair of sequences, taken by encoder and decoder respectively. But Bert is only an encoder. In English-Chinese Translation task for example, for transformer, the encoder take the English version and the decoder take the Chinese version, for Bert, the English version and Chinese version are glued with a special token [sept] then be inputed to the model. \n\nBesides the [sept] token, another embedding is introduced into the embedding layer, the sequence model. Details are shown below.\n\n<img src=\"BERT segment embedding.png\" alt=\"BERT segment embedding\" style=\"zoom:50%;\" />\n\n**3.1 Pre-training BERT**\n\nThe paper provides an example of each task in Appendix section **Masked LM and the Masking Procedure**, just go for it and have a look if don't understand.\n\n> Next Sentence Prediction The next sentence prediction task can be illustrated in the following examples. \n>\n> Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP] \n>\n> Label = IsNext \n>\n> Input = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are **flight ##less** birds [SEP] \n>\n> Label = NotNext\n\nThe \"flight ##less\" is because of the WordPiece embedding method that Bert uses. \"##\" means this token is split from last token, in this case, flightless is the original token. Because flightless is rarely used, the WordPiece embedding split this word into two.\n\n**3.2 Fine-tuning BERT**\n\nBert's architecture has one advantage over the transformer's, the self-attention allows model look both the two sentences. And the encoder-decoder model can't do that. As a result, the fine-tuning can be a little bit hazy.\n\n### 4 Experiments\n\n**4.2 SQuAD v1.1**\n\n> We fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32.\n\nThis misleads the people for a while. People found when fine-tuning with Bert, the variance of each results are high i.e. the result of fine-tuning is unstable. Then people found it is because 3 epochs are not enough. Besides, the optimiser that the original Bert model used is an incomplete version of Adam which is not stable for small epoch number. And the follow-ups change it into the original Adam.\n\nFrom the processes of the 3 experiments, it can be seen it is easy for BERT to be applied to downstream tasks. Just need to modify the input data in the form of the Bert sequence, and add another output layer. As a result, with BERT, massive number of tasks can be trained under a rather simple architecture.\n\n### 5 Ablation Studies\n\n<img src=\"BERT alibation study 1.png\" alt=\"BERT alibation study 1\" style=\"zoom:30%;\" />\n\nIt is obvious the 4th architecture BiLSTM is from the idea of ELMO. And all the variation lead to a deterioration of the acc, especially in the MRPC task.\n\n> **Microsoft Research Paraphrase Corpus (MRPC) Dataset**\n>\n> Created by Dolan et al. at 2005, the Microsoft Research Paraphrase Corpus (MRPC) Dataset contains pairs of sentences which have been extracted from news sources on the web, along with human annotations indicating whether each pair captures a paraphrase/semantic equivalence relationship., in English language. Containing 5,8 in Text file format.\n\n\n\n<img src=\"model size graph.webp\" alt=\"model size graph\" style=\"zoom:50%;\" />\n\nIn the Effect of model size part, they claim with a large model size, huge improvement can be reached.  And this leads a trend of increasing the model size in NLP, for example, the 100 billon GPT-3, and 500 billion model Megatron-Turing Natural Language Generation (MT-NLG). The boundary of NLP will be push further. \n\n## Reviews\n\nWriting: The biggest sell point in this paper is chosen as the \"bidirectional\". From today's view, the contributions of BERT are so more than this. Besides, when say to choose a feature, it is better to discuss both the pros and the cons of the choice. For example, compared with GPT, the encoder is used instead of the decoder. The pros are the bidirectional feature, but the cons is the resulting difficulty in applying on generative tasks such as the machine translation. \n\nBesides, BERT follows a whole ideal path of solving deep learning problems. That is after pre-training on a deep and huge model on a huge unlabelled dataset, the model can be applied to many small tasks with a few steps of fine tuning.\n","slug":"paper-reading-bert","published":1,"updated":"2022-04-30T19:31:04.409Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl2n5f40b000mp9yb4ghuhlsz","content":"<blockquote>\n<p>The BERT is the most important achievement in the NLP field in the last 4 years. It makes the transfer learning of NLP tasks possible and the transformer framework dominant the NLP field.</p>\n</blockquote>\n<blockquote>\n<p>This is a <a href=\"https://daydreamatnight.github.io/2022/04/02/paper-reading-start/\">series of paper reading notes</a>, hopefully, to push me to read paper casually and to leave some record of what I've learned.</p>\n</blockquote>\n<span id=\"more\"></span>\n<p>Paper link: <a href=\"https://arxiv.org/abs/1810.04805\">Bert: Pre-training of deep bidirectional transformers for language understanding</a></p>\n<p>Useful links: https://www.bilibili.com/video/BV1PL411M7eQ</p>\n<p>​ https://youtu.be/UYPa347-DdE</p>\n<p>​ <a href=\"https://arxiv.org/abs/1802.05365\">Deep contextualized word representations - arXiv</a></p>\n<h2 id=\"notes-by-sections\">Notes by sections</h2>\n<h3 id=\"abstract\">0. Abstract</h3>\n<p><em>The name of BERT might come from one of its important related work, ELMo. And Elmo and Bert are both characters in a TV show Sesame Street.</em></p>\n<p>The abstract focus on the two related work, ELMo and GPT. The bidirectional feature is in contrast to the unidirectional GPT model. And the \"without substantial task specific architecture modifications\" feature compares with the RNN-based ELMo model, whose architecture might get modified when training downstream tasks. Yet the ELMo model is bidirectional and GPT model is easy to use.</p>\n<p>When claiming a model is great, it is good to provide both the absolute accuracy and the relative accuracy compared with others. Just like this paper does.</p>\n<h3 id=\"conclusion\">6. Conclusion</h3>\n<p>To summarise, this is a classical <strong>A+B</strong> type of research. The idea of BERT is simple, combine the advantages of the bidirectional network with a rather old RNN base (ELMo), and the unidirectional network with a transformer base (GPT). And in detail, 2 pre-training tasks are designed.</p>\n<p>The main contribution of this network is showing that bidirectional information is important.</p>\n<h3 id=\"introduction\">1. Introduction</h3>\n<p>From the history introduction, it turns out BERT is not the first to apply pre-training on NLP. It's been a while. But BERT makes it popular.</p>\n<h3 id=\"related-work\">2. Related work</h3>\n<p><strong>2.3 Transfer Learning from Supervised data</strong></p>\n<p>This is what the CV area does most often. Yet it is not effective in NLP. It is partly because of the lacking of data. Another reason might be the existing labeled data focus only on language inference and machine translation, which are too different from other NLP tasks. So BERT and GPT use unlabelled data to pre-train and prove that unsupervised pre-train on a massive data is more effective than supervised pre-train on a relatively small data set. And, interestingly, this trend in NLP gradually effects the CV world. Nowadays unsupervised fine-tuning is becoming more and more popular in CV area.</p>\n<h3 id=\"bert\">3. Bert</h3>\n<p>In the first part, the pre-training and fine-tuning framework is briefly covered. A paper should be self-consistent, meaning that if a mechanic is fundamental in your area and essential to your work. It is a good habit to briefly introduce it, even if all the people in the area know it.</p>\n<p><strong>Model Architecture</strong></p>\n<p>And the architecture is as simple as just take the encoder part of the transformer model.</p>\n<blockquote>\n<p>number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.</p>\n<p>BERTBASE(L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE(L=24, H=1024, A=16, Total Parameters=340M).</p>\n</blockquote>\n<p>Because the hidden size of each multi-head attention sublayer is set as 64. And in transformer tradition, H = A * 64. So the multi-head number A actually depends on the hidden size H. As a result, the way of calculating the number of parameters is shown below.</p>\n<p><img src=\"BERT learnable paramters.png\" srcset=\"/img/loading.gif\" lazyload alt=\"BERT learnable paramters\" style=\"zoom:48%;\" /></p>\n<p><strong>Input/Output Representations</strong></p>\n<p>Why Bert need a pair of sentence to handle downstream tasks such as Machine Translation, unlike its predecessor Transformer?</p>\n<p>Because in Transformer, the input is a pair of sequences, taken by encoder and decoder respectively. But Bert is only an encoder. In English-Chinese Translation task for example, for transformer, the encoder take the English version and the decoder take the Chinese version, for Bert, the English version and Chinese version are glued with a special token [sept] then be inputed to the model.</p>\n<p>Besides the [sept] token, another embedding is introduced into the embedding layer, the sequence model. Details are shown below.</p>\n<p><img src=\"BERT segment embedding.png\" srcset=\"/img/loading.gif\" lazyload alt=\"BERT segment embedding\" style=\"zoom:50%;\" /></p>\n<p><strong>3.1 Pre-training BERT</strong></p>\n<p>The paper provides an example of each task in Appendix section <strong>Masked LM and the Masking Procedure</strong>, just go for it and have a look if don't understand.</p>\n<blockquote>\n<p>Next Sentence Prediction The next sentence prediction task can be illustrated in the following examples.</p>\n<p>Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]</p>\n<p>Label = IsNext</p>\n<p>Input = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are <strong>flight ##less</strong> birds [SEP]</p>\n<p>Label = NotNext</p>\n</blockquote>\n<p>The \"flight ##less\" is because of the WordPiece embedding method that Bert uses. \"##\" means this token is split from last token, in this case, flightless is the original token. Because flightless is rarely used, the WordPiece embedding split this word into two.</p>\n<p><strong>3.2 Fine-tuning BERT</strong></p>\n<p>Bert's architecture has one advantage over the transformer's, the self-attention allows model look both the two sentences. And the encoder-decoder model can't do that. As a result, the fine-tuning can be a little bit hazy.</p>\n<h3 id=\"experiments\">4 Experiments</h3>\n<p><strong>4.2 SQuAD v1.1</strong></p>\n<blockquote>\n<p>We fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32.</p>\n</blockquote>\n<p>This misleads the people for a while. People found when fine-tuning with Bert, the variance of each results are high i.e. the result of fine-tuning is unstable. Then people found it is because 3 epochs are not enough. Besides, the optimiser that the original Bert model used is an incomplete version of Adam which is not stable for small epoch number. And the follow-ups change it into the original Adam.</p>\n<p>From the processes of the 3 experiments, it can be seen it is easy for BERT to be applied to downstream tasks. Just need to modify the input data in the form of the Bert sequence, and add another output layer. As a result, with BERT, massive number of tasks can be trained under a rather simple architecture.</p>\n<h3 id=\"ablation-studies\">5 Ablation Studies</h3>\n<p><img src=\"BERT alibation study 1.png\" srcset=\"/img/loading.gif\" lazyload alt=\"BERT alibation study 1\" style=\"zoom:30%;\" /></p>\n<p>It is obvious the 4th architecture BiLSTM is from the idea of ELMO. And all the variation lead to a deterioration of the acc, especially in the MRPC task.</p>\n<blockquote>\n<p><strong>Microsoft Research Paraphrase Corpus (MRPC) Dataset</strong></p>\n<p>Created by Dolan et al. at 2005, the Microsoft Research Paraphrase Corpus (MRPC) Dataset contains pairs of sentences which have been extracted from news sources on the web, along with human annotations indicating whether each pair captures a paraphrase/semantic equivalence relationship., in English language. Containing 5,8 in Text file format.</p>\n</blockquote>\n<p><img src=\"model size graph.webp\" srcset=\"/img/loading.gif\" lazyload alt=\"model size graph\" style=\"zoom:50%;\" /></p>\n<p>In the Effect of model size part, they claim with a large model size, huge improvement can be reached. And this leads a trend of increasing the model size in NLP, for example, the 100 billon GPT-3, and 500 billion model Megatron-Turing Natural Language Generation (MT-NLG). The boundary of NLP will be push further.</p>\n<h2 id=\"reviews\">Reviews</h2>\n<p>Writing: The biggest sell point in this paper is chosen as the \"bidirectional\". From today's view, the contributions of BERT are so more than this. Besides, when say to choose a feature, it is better to discuss both the pros and the cons of the choice. For example, compared with GPT, the encoder is used instead of the decoder. The pros are the bidirectional feature, but the cons is the resulting difficulty in applying on generative tasks such as the machine translation.</p>\n<p>Besides, BERT follows a whole ideal path of solving deep learning problems. That is after pre-training on a deep and huge model on a huge unlabelled dataset, the model can be applied to many small tasks with a few steps of fine tuning.</p>\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>The BERT is the most important achievement in the NLP field in the last 4 years. It makes the transfer learning of NLP tasks possible and the transformer framework dominant the NLP field.</p>\n</blockquote>\n<blockquote>\n<p>This is a <a href=\"https://daydreamatnight.github.io/2022/04/02/paper-reading-start/\">series of paper reading notes</a>, hopefully, to push me to read paper casually and to leave some record of what I've learned.</p>\n</blockquote>","more":"<p>Paper link: <a href=\"https://arxiv.org/abs/1810.04805\">Bert: Pre-training of deep bidirectional transformers for language understanding</a></p>\n<p>Useful links: https://www.bilibili.com/video/BV1PL411M7eQ</p>\n<p>​ https://youtu.be/UYPa347-DdE</p>\n<p>​ <a href=\"https://arxiv.org/abs/1802.05365\">Deep contextualized word representations - arXiv</a></p>\n<h2 id=\"notes-by-sections\">Notes by sections</h2>\n<h3 id=\"abstract\">0. Abstract</h3>\n<p><em>The name of BERT might come from one of its important related work, ELMo. And Elmo and Bert are both characters in a TV show Sesame Street.</em></p>\n<p>The abstract focus on the two related work, ELMo and GPT. The bidirectional feature is in contrast to the unidirectional GPT model. And the \"without substantial task specific architecture modifications\" feature compares with the RNN-based ELMo model, whose architecture might get modified when training downstream tasks. Yet the ELMo model is bidirectional and GPT model is easy to use.</p>\n<p>When claiming a model is great, it is good to provide both the absolute accuracy and the relative accuracy compared with others. Just like this paper does.</p>\n<h3 id=\"conclusion\">6. Conclusion</h3>\n<p>To summarise, this is a classical <strong>A+B</strong> type of research. The idea of BERT is simple, combine the advantages of the bidirectional network with a rather old RNN base (ELMo), and the unidirectional network with a transformer base (GPT). And in detail, 2 pre-training tasks are designed.</p>\n<p>The main contribution of this network is showing that bidirectional information is important.</p>\n<h3 id=\"introduction\">1. Introduction</h3>\n<p>From the history introduction, it turns out BERT is not the first to apply pre-training on NLP. It's been a while. But BERT makes it popular.</p>\n<h3 id=\"related-work\">2. Related work</h3>\n<p><strong>2.3 Transfer Learning from Supervised data</strong></p>\n<p>This is what the CV area does most often. Yet it is not effective in NLP. It is partly because of the lacking of data. Another reason might be the existing labeled data focus only on language inference and machine translation, which are too different from other NLP tasks. So BERT and GPT use unlabelled data to pre-train and prove that unsupervised pre-train on a massive data is more effective than supervised pre-train on a relatively small data set. And, interestingly, this trend in NLP gradually effects the CV world. Nowadays unsupervised fine-tuning is becoming more and more popular in CV area.</p>\n<h3 id=\"bert\">3. Bert</h3>\n<p>In the first part, the pre-training and fine-tuning framework is briefly covered. A paper should be self-consistent, meaning that if a mechanic is fundamental in your area and essential to your work. It is a good habit to briefly introduce it, even if all the people in the area know it.</p>\n<p><strong>Model Architecture</strong></p>\n<p>And the architecture is as simple as just take the encoder part of the transformer model.</p>\n<blockquote>\n<p>number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.</p>\n<p>BERTBASE(L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE(L=24, H=1024, A=16, Total Parameters=340M).</p>\n</blockquote>\n<p>Because the hidden size of each multi-head attention sublayer is set as 64. And in transformer tradition, H = A * 64. So the multi-head number A actually depends on the hidden size H. As a result, the way of calculating the number of parameters is shown below.</p>\n<p><img src=\"BERT learnable paramters.png\" alt=\"BERT learnable paramters\" style=\"zoom:48%;\" /></p>\n<p><strong>Input/Output Representations</strong></p>\n<p>Why Bert need a pair of sentence to handle downstream tasks such as Machine Translation, unlike its predecessor Transformer?</p>\n<p>Because in Transformer, the input is a pair of sequences, taken by encoder and decoder respectively. But Bert is only an encoder. In English-Chinese Translation task for example, for transformer, the encoder take the English version and the decoder take the Chinese version, for Bert, the English version and Chinese version are glued with a special token [sept] then be inputed to the model.</p>\n<p>Besides the [sept] token, another embedding is introduced into the embedding layer, the sequence model. Details are shown below.</p>\n<p><img src=\"BERT segment embedding.png\" alt=\"BERT segment embedding\" style=\"zoom:50%;\" /></p>\n<p><strong>3.1 Pre-training BERT</strong></p>\n<p>The paper provides an example of each task in Appendix section <strong>Masked LM and the Masking Procedure</strong>, just go for it and have a look if don't understand.</p>\n<blockquote>\n<p>Next Sentence Prediction The next sentence prediction task can be illustrated in the following examples.</p>\n<p>Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]</p>\n<p>Label = IsNext</p>\n<p>Input = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are <strong>flight ##less</strong> birds [SEP]</p>\n<p>Label = NotNext</p>\n</blockquote>\n<p>The \"flight ##less\" is because of the WordPiece embedding method that Bert uses. \"##\" means this token is split from last token, in this case, flightless is the original token. Because flightless is rarely used, the WordPiece embedding split this word into two.</p>\n<p><strong>3.2 Fine-tuning BERT</strong></p>\n<p>Bert's architecture has one advantage over the transformer's, the self-attention allows model look both the two sentences. And the encoder-decoder model can't do that. As a result, the fine-tuning can be a little bit hazy.</p>\n<h3 id=\"experiments\">4 Experiments</h3>\n<p><strong>4.2 SQuAD v1.1</strong></p>\n<blockquote>\n<p>We fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32.</p>\n</blockquote>\n<p>This misleads the people for a while. People found when fine-tuning with Bert, the variance of each results are high i.e. the result of fine-tuning is unstable. Then people found it is because 3 epochs are not enough. Besides, the optimiser that the original Bert model used is an incomplete version of Adam which is not stable for small epoch number. And the follow-ups change it into the original Adam.</p>\n<p>From the processes of the 3 experiments, it can be seen it is easy for BERT to be applied to downstream tasks. Just need to modify the input data in the form of the Bert sequence, and add another output layer. As a result, with BERT, massive number of tasks can be trained under a rather simple architecture.</p>\n<h3 id=\"ablation-studies\">5 Ablation Studies</h3>\n<p><img src=\"BERT alibation study 1.png\" alt=\"BERT alibation study 1\" style=\"zoom:30%;\" /></p>\n<p>It is obvious the 4th architecture BiLSTM is from the idea of ELMO. And all the variation lead to a deterioration of the acc, especially in the MRPC task.</p>\n<blockquote>\n<p><strong>Microsoft Research Paraphrase Corpus (MRPC) Dataset</strong></p>\n<p>Created by Dolan et al. at 2005, the Microsoft Research Paraphrase Corpus (MRPC) Dataset contains pairs of sentences which have been extracted from news sources on the web, along with human annotations indicating whether each pair captures a paraphrase/semantic equivalence relationship., in English language. Containing 5,8 in Text file format.</p>\n</blockquote>\n<p><img src=\"model size graph.webp\" alt=\"model size graph\" style=\"zoom:50%;\" /></p>\n<p>In the Effect of model size part, they claim with a large model size, huge improvement can be reached. And this leads a trend of increasing the model size in NLP, for example, the 100 billon GPT-3, and 500 billion model Megatron-Turing Natural Language Generation (MT-NLG). The boundary of NLP will be push further.</p>\n<h2 id=\"reviews\">Reviews</h2>\n<p>Writing: The biggest sell point in this paper is chosen as the \"bidirectional\". From today's view, the contributions of BERT are so more than this. Besides, when say to choose a feature, it is better to discuss both the pros and the cons of the choice. For example, compared with GPT, the encoder is used instead of the decoder. The pros are the bidirectional feature, but the cons is the resulting difficulty in applying on generative tasks such as the machine translation.</p>\n<p>Besides, BERT follows a whole ideal path of solving deep learning problems. That is after pre-training on a deep and huge model on a huge unlabelled dataset, the model can be applied to many small tasks with a few steps of fine tuning.</p>","wordcount":6231},{"title":"paper reading: Vision Transformer","author":"Ryan LI","toc":true,"declare":true,"date":"2022-04-21T12:42:10.000Z","_content":"\n> Presented in 2021, the vision transformer model (ViT) is the most influential work in the CV field recent years. Its variants outperform the dominant convolutional networks in almost all CV tasks such as [classification](https://paperswithcode.com/sota/image-classification-on-imagenet) and [object detection](https://paperswithcode.com/sota/object-detection-on-coco). And it breaks the border of CV and NLP, providing new thoughts to CV and multi-model areas.\n\n> This is a [series of paper reading notes](https://daydreamatnight.github.io/2022/04/02/paper-reading-start/), hopefully, to push me to read paper casually and to leave some record of what I've learned.\n\n<!-- more -->\n\nPaper:\n\n[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929v2)\n\nUseful links:\n\nhttps://www.bilibili.com/video/BV15P4y137jb\n\nhttps://theaisummer.com/vision-transformer/\n\n<img src=\"VIT properties.png\" alt=\"VIT properties\" style=\"zoom:50%;\" />\n\nNot only Vision Transformer (ViT) performs better on traditional CV tasks, it has more impressive properties. As shown above, [Naseer et al.](https://proceedings.neurips.cc/paper/2021/hash/c404a5adbf90e09631678b13b05d9d7a-Abstract.html) demonstrate the tasks where ViT shows extra performance over CNN models, even over humans. \n\n## Notes\n\n### Abstract\n\nWhile Transformer-based models such as BERT, GPT series, and T5 nail the NLP tasks, CV tasks remain dominated by CNN-based models. This paper applied a pure transformer encoder (same as BERT) to sequences of cut images and obtains a good classification result, especially with **supervised** pre-training on a large dataset then fine tuning on a mid-size dataset. Besides, fewer computational resources (meaning 2500 days of TPUv3) are need to attain good results, compared with CNN models.\n\n### Conclusion\n\nBesides the paraphrasing part, the conclusion part discusses the future work based on the ViT. And all of them have follow-up works.\n\n- Apply ViT to other CV tasks, given the promising performance of [DETR](https://arxiv.org/pdf/2005.12872.pdf,). Only 1 and a half month later, [ViT-FRCNN](https://arxiv.org/abs/2012.09958) and [SEDR](http://openaccess.thecvf.com/content/CVPR2021/html/Zheng_Rethinking_Semantic_Segmentation_From_a_Sequence-to-Sequence_Perspective_With_Transformers_CVPR_2021_paper.html) mange to apply ViT on detection and segmentation respectively. And after 3 months, [Swin Transformer](https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.html) introduces hierarchical feature to transformer, making ViT more suitable to vision tasks.\n- Self-supervised pre-training, given the great results of BERT and GPT in the NLP field.  Initial explorations in the paper show a gap from the supervised pre-training. One year later,  [MAE](https://arxiv.org/abs/2111.06377) narrows the gap successfully by generative model.\n  - Besides, in the section of self-supervised learning, a contrastive learning is mentioned as well, and [MOCO v3](http://openaccess.thecvf.com/content/ICCV2021/html/Chen_An_Empirical_Study_of_Training_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html) and [DINO](http://openaccess.thecvf.com/content/ICCV2021/html/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html) follow this line.\n- Further scaling up this model. Half year later, same group introduces [Vit-G](https://arxiv.org/pdf/2106.04560v1.pdf) with two billion parameters, attaining new SOTA on ImageNet of 90.45%.\n\n### Introduction\n\nSuccess of Transformer-based models on NLP tasks are firstly reviewed. And it is natural trying to apply such self-attention mechanism to vision. Yet here are one major obstacle:\n\n- How to transfer a 2D picture to a 1D sequence? \n\nOne intuitive thought is to flatten the picture directly and treat each pixel as an element. In this way, a medium size 224\\*224 picture will be converted to a 50,176 long sequence. However, the sequence length is quadratically related to model complexity. Morden hardware only supports input sequence length <1000 of a pure self-attention model. For example BERT only accepts input length of 512. \n\nYet the authors mange to incorporate the original transformer encoder in CV. In order to address the sequence length problem, they split the image into 16*16 patches, each patches denotes a sequence element (token). In this way, a 224\\*224 image can be converted as a sequence length of 16\\*16 with each element sized 14\\*14. Each element then gets linearised through a FC layer before being passed into the transformer encoder.\n\nAnd the afterwards experiments show that the new  model doesn't perform well on mid-size model. One explanation is that transformer model lack the image-related inductive bias of CNN (locality and translation equalisation). Yet with pre-training on large dataset such as JFT-300 and ImagNet-21K, a better result than CNN can be approached.\n\n### Related work\n\n*It is a detailed related work covering all the aspects in the original paper. I just pick few of them.*\n\nAll the related works aim at reducing the sequence length within the limitation caused by self-attention. Some try to combine CNN with self-attention. For example [Wang, X et al.'s work](http://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.html) takes the feature map extracted by CNN as the input of transformer. Others try to replace the CNN with a special variation of self-attention. For example, [Ramachandran et al.'s work](https://proceedings.neurips.cc/paper/2019/hash/3416a75f4cea9109507cacd8e2f2aefc-Abstract.html) replaces all convolutional sublayers of the ResNet-50 model with self-attention layers. To reduce the computational cost, a local region of the image instead of the whole image is used as the receptive field of the self-attention layer, meaning each pixel only attends to its neighbours in a restricted area. In another work line, [Wang, H et.al's work](https://link.springer.com/chapter/10.1007/978-3-030-58548-8_7) factorising 2D self-attention into two 1D self-attentions to significantly reduce computation complexity. \n\nAnd there is a very similar [Cordonnier et al's work](https://arxiv.org/abs/1911.03584) also split the images before the self-attention layer. Yet the patch size is 2*2 and with the dataset only [CIFAR-10](https://paperswithcode.com/sota/image-classification-on-cifar-10). This paper  enlarges the model, apply it in big dataset, and shows the scalability.\n\nAnother related work image GPT([iGPT](http://proceedings.mlr.press/v119/chen20s.html)) trains a GPT-2 scale generative network. Yet the highest accuracy on ImageNet is 72%, way less than 88% of this paper. But in 2021, an afterwards generative network [MAE](https://arxiv.org/abs/2111.06377) shows a competitive result of 87.8%, with good transfer leaning capability on segmentation and object detection as well.\n\nBesides, works exploring transfer learning performance of CNN model on larger datasets such as ImageNet-21k and JFT-300M are mentioned. And this paper studies the transformer instead of the CNN.\n\n### Method\n\nThe whole big idea of the method part is leaving as much as possible the original transformer architecture in order to leverage the good feature and the existing mature efficient implementations of it.  \n\n#### Vision Transformer (VIT)\n\n<img src=\"VIT model overview.png\" alt=\"VIT model overview\" style=\"zoom:60%;\" />\n\n<img src=\"VIT algorithm.png\" alt=\"VIT algorithm\" style=\"zoom:40%;\" />\n\nFrom the overview, and the algorithm it should be called Vision BERT instead of Vision Transformer, given the pure encoder architecture and the extra \"classification token\". Assume a *224\\*224\\*3* image, after patching, the sequence length is *HW/P<sup>2</sup>=16\\*16=196* and the width is *14\\*14\\*3=768*. Given the hidden size of the model *D=768*, through linear projection layer (E), sequence *X [196\\*768]* are multiplied with weight *E [768\\*768]*. The resulting linear output *[196\\*768]* is then contacted with [cls], followed by adding standard 1D learnable positional embedding to be the transformer input *[197\\*768]*. After several transformer blocks, the output size does not change and the [cls] token is projected to a softmax classification layer\n\n#### Ablation experiments\n\nThe pre and post-processing are crucial for ViT given that the middle transformer encoder layers are kept as original. Multiple rounds of ablation experiments are carried out. \n\n- Position embedding schemes, 2D embedding and relative embedding are applied to compare with the standard 1D embedding, and no evident gain is spotted.\n\n  <img src=\"VIT positional embedding ablation.png\" alt=\"VIT positional embedding ablation\" style=\"zoom:30%;\" />\n\n- [cls] token vs average pooling, extra [cls] token is inherited from the Transformer model for text, and traditionally in CV, instead of an additional token, an average polling layer after the output layer is usually used as a classifier. The figure blow shows no both works. But in order to stick the original design as close as possible, [cls] token is applied.\n\n  <img src=\"VIT class token ablation.png\" alt=\"VIT class token ablation\" style=\"zoom:30%;\" />\n\nAnother analysis after the model description:\n\n**Inductive bias:** Less locality, translation equivariance and 2D neighbourhood structure are possessed by ViT, compared with CNN. \n\n**Hybrid Architecture:** CNN can be used as a special embedding, leveraging the inductive bias of the CNN model.\n\n#### Limitation on Fine-Tuning\n\nPre-train ViT at larger and higher resolution datasets is [proved](https://proceedings.neurips.cc/paper/2019/hash/d03a857a23b5285736c4d55e0bb067c8-Abstract.html) to be beneficial. Yet the input sequence lengths are different when training on two datasets with different resolutions, resulting in positional embeddings of different lengths. In this article, a 2D interpolation is applied to transfer a pre-trained positional embedding to another dataset to fine-tune. But the accuracy will loss if the resolution difference is too big.\n\n### Experiments\n\n#### Setup\n\nResNet, ViT, and the hybrid model are evaluated together and ViT wins taking account of the pre-training cost. Besides, a small self-supervision experiment is deployed and sees potential.\n\nTwo scales of ImageNet(1k and 21k) and JFT(303M) are used as pre-training dataset. Only classification tasks are evaluated with popular datasets.\n\n3 scales of ViT are designed with different patch size(inversely proportional to the amount of data). For example, ViT-L/16 means ViT-Large with 16 patches.\n\n<img src=\"ViT variants.png\" alt=\"ViT variants\" style=\"zoom:50%;\" />\n\n#### Comparison to SOTA\n\nAnd the best results are shown below:\n\n<img src=\"ViT results.png\" alt=\"ViT results\" style=\"zoom:60%;\" />\n\n#### Pre-training cost requirements\n\nFigure 3 and 4 shows the performance of the presented models on different sizes of per-training datasets.  ViT preforms competitive only starts from dataset 21k, and very well only on huge dataset.\n\nFigure 5 shows the transfer performances versus pre-training costs on JFT-300M of several models. And it shows that ViT is cheaper than ResNet. Interestingly, the Hybrid model is competitive on low pre-training cost.\n\n<img src=\"ViT ablation 2.png\" alt=\"ViT ablation 2\" style=\"zoom:55%;\" />\n\n#### Inspecting ViT\n\n<img src=\"ViT inspecting.png\" alt=\"ViT inspecting\" style=\"zoom:42%;\" />\n\n**Figure7 Left:** The learned linear projection weight matrix *E [768\\*768]* is inspected by PCA, and the first 28 components(modes) are visualised as embedding filters. They look pretty much similar to the early layer filters(kernels) of CNN (for example, the first layer of a CNN shown by [Brachnmann et.al](https://www.mdpi.com/2073-8994/8/12/144)). This similarity indicates that the linear patch embedding manages to represent the low-dimension structure of each patch. <img src=\"CNN first layer filters.png\" alt=\"CNN first layer filters\" style=\"zoom:40%;\" />\n\n**Figure7 mid:** Position embedding visualisation first shows that the spatial information is captured well by the E matrix, given that the similarity matrix between patches matches well with the distance relationships of patches. Second, patterns across rows (and columns) have similar representations, indicating the embedding layer has successful learned the row-column relationship. Overall, the 1D positional embedding has learned the 2D structure, coherent with the ablation experiment result. \n\n**Figure7 Right:** The receptive fields of the multi-head attention layers are evaluated by the mean attention distance. Compared with CNN whose receptive field increases linearly with the depth, the ViT attends the whole picture from the first layer, leveraging the natural advantage of transformer.\n\n#### Self-supervision\n\nA preliminary exploration on masked patch prediction for self-supervision (mimicking one of the BERT pre-training tasks) has been employed. But the result is not satisfying. And contrastive pre-training are mentioned as a future work.\n\n## Review\n\nThis is a concise-written, fundamental article. Just as presented in the conclusion part, it inspired many flow-up works from any direction in the CV area, such as applying to  more tasks, changing the architecture(tokenisation, transformer block([MLP-mixer](https://proceedings.neurips.cc/paper/2021/hash/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Abstract.html)： changing multi-head attention layers to MLP , [meta-former](https://arxiv.org/abs/2111.11418): substituting multi-head attention layers to average pooling), changing objective function(self-supervised, contrastive learning), and multi modality. \n\nIt's still unclear whether convolution, attention or MLP will win this game.\n\n## Reference\n\n[Naseer, M. M., Ranasinghe, K., Khan, S. H., Hayat, M., Shahbaz Khan, F., & Yang, M. H. (2021). Intriguing properties of vision transformers. *Advances in Neural Information Processing Systems*, *34*.](https://proceedings.neurips.cc/paper/2021/hash/c404a5adbf90e09631678b13b05d9d7a-Abstract.html)\n\n[Beal, J., Kim, E., Tzeng, E., Park, D. H., Zhai, A., & Kislyuk, D. (2020). Toward transformer-based object detection. *arXiv preprint arXiv:2012.09958*.](https://arxiv.org/abs/2012.09958)\n\n[Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., ... & Zhang, L. (2021). Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition* (pp. 6881-6890).](http://openaccess.thecvf.com/content/CVPR2021/html/Zheng_Rethinking_Semantic_Segmentation_From_a_Sequence-to-Sequence_Perspective_With_Transformers_CVPR_2021_paper.html)\n\n[Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. In *Proceedings of the IEEE/CVF International Conference on Computer Vision* (pp. 10012-10022).](https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.html)\n\n[Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., & Zagoruyko, S. (2020, August). End-to-end object detection with transformers. In *European conference on computer vision* (pp. 213-229). Springer, Cham.](https://arxiv.org/pdf/2005.12872.pdf,)\n\n[He, K., Chen, X., Xie, S., Li, Y., Dollár, P., & Girshick, R. (2021). Masked autoencoders are scalable vision learners. *arXiv preprint arXiv:2111.06377*.](https://arxiv.org/abs/2111.06377)\n\n[Chen, X., Xie, S., & He, K. (2021). An empirical study of training self-supervised vision transformers. In *Proceedings of the IEEE/CVF International Conference on Computer Vision* (pp. 9640-9649).](http://openaccess.thecvf.com/content/ICCV2021/html/Chen_An_Empirical_Study_of_Training_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html)\n\n[Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., & Joulin, A. (2021). Emerging properties in self-supervised vision transformers. In *Proceedings of the IEEE/CVF International Conference on Computer Vision* (pp. 9650-9660).](http://openaccess.thecvf.com/content/ICCV2021/html/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html)\n\n[Zhai, X., Kolesnikov, A., Houlsby, N., & Beyer, L. (2021). Scaling Vision Transformers. *ArXiv, abs/2106.04560*.](https://arxiv.org/abs/2106.04560)\n\n[Wang, X., Girshick, R., Gupta, A., & He, K. (2018). Non-local neural networks. In *Proceedings of the IEEE conference on computer vision and pattern recognition* (pp. 7794-7803).](http://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.html)\n\n[Ramachandran, P., Parmar, N., Vaswani, A., Bello, I., Levskaya, A., & Shlens, J. (2019). Stand-alone self-attention in vision models. *Advances in Neural Information Processing Systems*, *32*.](https://proceedings.neurips.cc/paper/2019/hash/3416a75f4cea9109507cacd8e2f2aefc-Abstract.html)\n\n[Wang, H., Zhu, Y., Green, B., Adam, H., Yuille, A., & Chen, L. C. (2020, August). Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. In *European Conference on Computer Vision* (pp. 108-126). Springer, Cham.](https://link.springer.com/chapter/10.1007/978-3-030-58548-8_7)\n\n[Cordonnier, J. B., Loukas, A., & Jaggi, M. (2019). On the relationship between self-attention and convolutional layers. *arXiv preprint arXiv:1911.03584*.](https://arxiv.org/abs/1911.03584)\n\n[Chen, Mark, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. \"Generative pretraining from pixels.\" In *International Conference on Machine Learning*, pp. 1691-1703. PMLR, 2020.](http://proceedings.mlr.press/v119/chen20s.html)\n\n[Touvron, H., Vedaldi, A., Douze, M., & Jégou, H. (2019). Fixing the train-test resolution discrepancy. *Advances in neural information processing systems*, *32*.](https://proceedings.neurips.cc/paper/2019/hash/d03a857a23b5285736c4d55e0bb067c8-Abstract.html)\n\n[Brachmann, A., & Redies, C. (2016). Using convolutional neural network filters to measure left-right mirror symmetry in images. *Symmetry*, *8*(12), 144.](https://www.mdpi.com/2073-8994/8/12/144)\n\n[Tolstikhin, I. O., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., ... & Dosovitskiy, A. (2021). Mlp-mixer: An all-mlp architecture for vision. *Advances in Neural Information Processing Systems*, *34*.](https://proceedings.neurips.cc/paper/2021/hash/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Abstract.html)\n\n[Yu, W., Luo, M., Zhou, P., Si, C., Zhou, Y., Wang, X., ... & Yan, S. (2021). Metaformer is actually what you need for vision. arXiv preprint arXiv:2111.11418.](https://arxiv.org/abs/2111.11418)\n","source":"_posts/paper-reading-Vision-Transformer.md","raw":"---\ntitle: 'paper reading: Vision Transformer'\nauthor: Ryan LI\ntoc: true\ndeclare: true\ndate: 2022-04-21 20:42:10\ntags:\n  - paper reading\n  - deep learning \n---\n\n> Presented in 2021, the vision transformer model (ViT) is the most influential work in the CV field recent years. Its variants outperform the dominant convolutional networks in almost all CV tasks such as [classification](https://paperswithcode.com/sota/image-classification-on-imagenet) and [object detection](https://paperswithcode.com/sota/object-detection-on-coco). And it breaks the border of CV and NLP, providing new thoughts to CV and multi-model areas.\n\n> This is a [series of paper reading notes](https://daydreamatnight.github.io/2022/04/02/paper-reading-start/), hopefully, to push me to read paper casually and to leave some record of what I've learned.\n\n<!-- more -->\n\nPaper:\n\n[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929v2)\n\nUseful links:\n\nhttps://www.bilibili.com/video/BV15P4y137jb\n\nhttps://theaisummer.com/vision-transformer/\n\n<img src=\"VIT properties.png\" alt=\"VIT properties\" style=\"zoom:50%;\" />\n\nNot only Vision Transformer (ViT) performs better on traditional CV tasks, it has more impressive properties. As shown above, [Naseer et al.](https://proceedings.neurips.cc/paper/2021/hash/c404a5adbf90e09631678b13b05d9d7a-Abstract.html) demonstrate the tasks where ViT shows extra performance over CNN models, even over humans. \n\n## Notes\n\n### Abstract\n\nWhile Transformer-based models such as BERT, GPT series, and T5 nail the NLP tasks, CV tasks remain dominated by CNN-based models. This paper applied a pure transformer encoder (same as BERT) to sequences of cut images and obtains a good classification result, especially with **supervised** pre-training on a large dataset then fine tuning on a mid-size dataset. Besides, fewer computational resources (meaning 2500 days of TPUv3) are need to attain good results, compared with CNN models.\n\n### Conclusion\n\nBesides the paraphrasing part, the conclusion part discusses the future work based on the ViT. And all of them have follow-up works.\n\n- Apply ViT to other CV tasks, given the promising performance of [DETR](https://arxiv.org/pdf/2005.12872.pdf,). Only 1 and a half month later, [ViT-FRCNN](https://arxiv.org/abs/2012.09958) and [SEDR](http://openaccess.thecvf.com/content/CVPR2021/html/Zheng_Rethinking_Semantic_Segmentation_From_a_Sequence-to-Sequence_Perspective_With_Transformers_CVPR_2021_paper.html) mange to apply ViT on detection and segmentation respectively. And after 3 months, [Swin Transformer](https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.html) introduces hierarchical feature to transformer, making ViT more suitable to vision tasks.\n- Self-supervised pre-training, given the great results of BERT and GPT in the NLP field.  Initial explorations in the paper show a gap from the supervised pre-training. One year later,  [MAE](https://arxiv.org/abs/2111.06377) narrows the gap successfully by generative model.\n  - Besides, in the section of self-supervised learning, a contrastive learning is mentioned as well, and [MOCO v3](http://openaccess.thecvf.com/content/ICCV2021/html/Chen_An_Empirical_Study_of_Training_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html) and [DINO](http://openaccess.thecvf.com/content/ICCV2021/html/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html) follow this line.\n- Further scaling up this model. Half year later, same group introduces [Vit-G](https://arxiv.org/pdf/2106.04560v1.pdf) with two billion parameters, attaining new SOTA on ImageNet of 90.45%.\n\n### Introduction\n\nSuccess of Transformer-based models on NLP tasks are firstly reviewed. And it is natural trying to apply such self-attention mechanism to vision. Yet here are one major obstacle:\n\n- How to transfer a 2D picture to a 1D sequence? \n\nOne intuitive thought is to flatten the picture directly and treat each pixel as an element. In this way, a medium size 224\\*224 picture will be converted to a 50,176 long sequence. However, the sequence length is quadratically related to model complexity. Morden hardware only supports input sequence length <1000 of a pure self-attention model. For example BERT only accepts input length of 512. \n\nYet the authors mange to incorporate the original transformer encoder in CV. In order to address the sequence length problem, they split the image into 16*16 patches, each patches denotes a sequence element (token). In this way, a 224\\*224 image can be converted as a sequence length of 16\\*16 with each element sized 14\\*14. Each element then gets linearised through a FC layer before being passed into the transformer encoder.\n\nAnd the afterwards experiments show that the new  model doesn't perform well on mid-size model. One explanation is that transformer model lack the image-related inductive bias of CNN (locality and translation equalisation). Yet with pre-training on large dataset such as JFT-300 and ImagNet-21K, a better result than CNN can be approached.\n\n### Related work\n\n*It is a detailed related work covering all the aspects in the original paper. I just pick few of them.*\n\nAll the related works aim at reducing the sequence length within the limitation caused by self-attention. Some try to combine CNN with self-attention. For example [Wang, X et al.'s work](http://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.html) takes the feature map extracted by CNN as the input of transformer. Others try to replace the CNN with a special variation of self-attention. For example, [Ramachandran et al.'s work](https://proceedings.neurips.cc/paper/2019/hash/3416a75f4cea9109507cacd8e2f2aefc-Abstract.html) replaces all convolutional sublayers of the ResNet-50 model with self-attention layers. To reduce the computational cost, a local region of the image instead of the whole image is used as the receptive field of the self-attention layer, meaning each pixel only attends to its neighbours in a restricted area. In another work line, [Wang, H et.al's work](https://link.springer.com/chapter/10.1007/978-3-030-58548-8_7) factorising 2D self-attention into two 1D self-attentions to significantly reduce computation complexity. \n\nAnd there is a very similar [Cordonnier et al's work](https://arxiv.org/abs/1911.03584) also split the images before the self-attention layer. Yet the patch size is 2*2 and with the dataset only [CIFAR-10](https://paperswithcode.com/sota/image-classification-on-cifar-10). This paper  enlarges the model, apply it in big dataset, and shows the scalability.\n\nAnother related work image GPT([iGPT](http://proceedings.mlr.press/v119/chen20s.html)) trains a GPT-2 scale generative network. Yet the highest accuracy on ImageNet is 72%, way less than 88% of this paper. But in 2021, an afterwards generative network [MAE](https://arxiv.org/abs/2111.06377) shows a competitive result of 87.8%, with good transfer leaning capability on segmentation and object detection as well.\n\nBesides, works exploring transfer learning performance of CNN model on larger datasets such as ImageNet-21k and JFT-300M are mentioned. And this paper studies the transformer instead of the CNN.\n\n### Method\n\nThe whole big idea of the method part is leaving as much as possible the original transformer architecture in order to leverage the good feature and the existing mature efficient implementations of it.  \n\n#### Vision Transformer (VIT)\n\n<img src=\"VIT model overview.png\" alt=\"VIT model overview\" style=\"zoom:60%;\" />\n\n<img src=\"VIT algorithm.png\" alt=\"VIT algorithm\" style=\"zoom:40%;\" />\n\nFrom the overview, and the algorithm it should be called Vision BERT instead of Vision Transformer, given the pure encoder architecture and the extra \"classification token\". Assume a *224\\*224\\*3* image, after patching, the sequence length is *HW/P<sup>2</sup>=16\\*16=196* and the width is *14\\*14\\*3=768*. Given the hidden size of the model *D=768*, through linear projection layer (E), sequence *X [196\\*768]* are multiplied with weight *E [768\\*768]*. The resulting linear output *[196\\*768]* is then contacted with [cls], followed by adding standard 1D learnable positional embedding to be the transformer input *[197\\*768]*. After several transformer blocks, the output size does not change and the [cls] token is projected to a softmax classification layer\n\n#### Ablation experiments\n\nThe pre and post-processing are crucial for ViT given that the middle transformer encoder layers are kept as original. Multiple rounds of ablation experiments are carried out. \n\n- Position embedding schemes, 2D embedding and relative embedding are applied to compare with the standard 1D embedding, and no evident gain is spotted.\n\n  <img src=\"VIT positional embedding ablation.png\" alt=\"VIT positional embedding ablation\" style=\"zoom:30%;\" />\n\n- [cls] token vs average pooling, extra [cls] token is inherited from the Transformer model for text, and traditionally in CV, instead of an additional token, an average polling layer after the output layer is usually used as a classifier. The figure blow shows no both works. But in order to stick the original design as close as possible, [cls] token is applied.\n\n  <img src=\"VIT class token ablation.png\" alt=\"VIT class token ablation\" style=\"zoom:30%;\" />\n\nAnother analysis after the model description:\n\n**Inductive bias:** Less locality, translation equivariance and 2D neighbourhood structure are possessed by ViT, compared with CNN. \n\n**Hybrid Architecture:** CNN can be used as a special embedding, leveraging the inductive bias of the CNN model.\n\n#### Limitation on Fine-Tuning\n\nPre-train ViT at larger and higher resolution datasets is [proved](https://proceedings.neurips.cc/paper/2019/hash/d03a857a23b5285736c4d55e0bb067c8-Abstract.html) to be beneficial. Yet the input sequence lengths are different when training on two datasets with different resolutions, resulting in positional embeddings of different lengths. In this article, a 2D interpolation is applied to transfer a pre-trained positional embedding to another dataset to fine-tune. But the accuracy will loss if the resolution difference is too big.\n\n### Experiments\n\n#### Setup\n\nResNet, ViT, and the hybrid model are evaluated together and ViT wins taking account of the pre-training cost. Besides, a small self-supervision experiment is deployed and sees potential.\n\nTwo scales of ImageNet(1k and 21k) and JFT(303M) are used as pre-training dataset. Only classification tasks are evaluated with popular datasets.\n\n3 scales of ViT are designed with different patch size(inversely proportional to the amount of data). For example, ViT-L/16 means ViT-Large with 16 patches.\n\n<img src=\"ViT variants.png\" alt=\"ViT variants\" style=\"zoom:50%;\" />\n\n#### Comparison to SOTA\n\nAnd the best results are shown below:\n\n<img src=\"ViT results.png\" alt=\"ViT results\" style=\"zoom:60%;\" />\n\n#### Pre-training cost requirements\n\nFigure 3 and 4 shows the performance of the presented models on different sizes of per-training datasets.  ViT preforms competitive only starts from dataset 21k, and very well only on huge dataset.\n\nFigure 5 shows the transfer performances versus pre-training costs on JFT-300M of several models. And it shows that ViT is cheaper than ResNet. Interestingly, the Hybrid model is competitive on low pre-training cost.\n\n<img src=\"ViT ablation 2.png\" alt=\"ViT ablation 2\" style=\"zoom:55%;\" />\n\n#### Inspecting ViT\n\n<img src=\"ViT inspecting.png\" alt=\"ViT inspecting\" style=\"zoom:42%;\" />\n\n**Figure7 Left:** The learned linear projection weight matrix *E [768\\*768]* is inspected by PCA, and the first 28 components(modes) are visualised as embedding filters. They look pretty much similar to the early layer filters(kernels) of CNN (for example, the first layer of a CNN shown by [Brachnmann et.al](https://www.mdpi.com/2073-8994/8/12/144)). This similarity indicates that the linear patch embedding manages to represent the low-dimension structure of each patch. <img src=\"CNN first layer filters.png\" alt=\"CNN first layer filters\" style=\"zoom:40%;\" />\n\n**Figure7 mid:** Position embedding visualisation first shows that the spatial information is captured well by the E matrix, given that the similarity matrix between patches matches well with the distance relationships of patches. Second, patterns across rows (and columns) have similar representations, indicating the embedding layer has successful learned the row-column relationship. Overall, the 1D positional embedding has learned the 2D structure, coherent with the ablation experiment result. \n\n**Figure7 Right:** The receptive fields of the multi-head attention layers are evaluated by the mean attention distance. Compared with CNN whose receptive field increases linearly with the depth, the ViT attends the whole picture from the first layer, leveraging the natural advantage of transformer.\n\n#### Self-supervision\n\nA preliminary exploration on masked patch prediction for self-supervision (mimicking one of the BERT pre-training tasks) has been employed. But the result is not satisfying. And contrastive pre-training are mentioned as a future work.\n\n## Review\n\nThis is a concise-written, fundamental article. Just as presented in the conclusion part, it inspired many flow-up works from any direction in the CV area, such as applying to  more tasks, changing the architecture(tokenisation, transformer block([MLP-mixer](https://proceedings.neurips.cc/paper/2021/hash/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Abstract.html)： changing multi-head attention layers to MLP , [meta-former](https://arxiv.org/abs/2111.11418): substituting multi-head attention layers to average pooling), changing objective function(self-supervised, contrastive learning), and multi modality. \n\nIt's still unclear whether convolution, attention or MLP will win this game.\n\n## Reference\n\n[Naseer, M. M., Ranasinghe, K., Khan, S. H., Hayat, M., Shahbaz Khan, F., & Yang, M. H. (2021). Intriguing properties of vision transformers. *Advances in Neural Information Processing Systems*, *34*.](https://proceedings.neurips.cc/paper/2021/hash/c404a5adbf90e09631678b13b05d9d7a-Abstract.html)\n\n[Beal, J., Kim, E., Tzeng, E., Park, D. H., Zhai, A., & Kislyuk, D. (2020). Toward transformer-based object detection. *arXiv preprint arXiv:2012.09958*.](https://arxiv.org/abs/2012.09958)\n\n[Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., ... & Zhang, L. (2021). Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition* (pp. 6881-6890).](http://openaccess.thecvf.com/content/CVPR2021/html/Zheng_Rethinking_Semantic_Segmentation_From_a_Sequence-to-Sequence_Perspective_With_Transformers_CVPR_2021_paper.html)\n\n[Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. In *Proceedings of the IEEE/CVF International Conference on Computer Vision* (pp. 10012-10022).](https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.html)\n\n[Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., & Zagoruyko, S. (2020, August). End-to-end object detection with transformers. In *European conference on computer vision* (pp. 213-229). Springer, Cham.](https://arxiv.org/pdf/2005.12872.pdf,)\n\n[He, K., Chen, X., Xie, S., Li, Y., Dollár, P., & Girshick, R. (2021). Masked autoencoders are scalable vision learners. *arXiv preprint arXiv:2111.06377*.](https://arxiv.org/abs/2111.06377)\n\n[Chen, X., Xie, S., & He, K. (2021). An empirical study of training self-supervised vision transformers. In *Proceedings of the IEEE/CVF International Conference on Computer Vision* (pp. 9640-9649).](http://openaccess.thecvf.com/content/ICCV2021/html/Chen_An_Empirical_Study_of_Training_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html)\n\n[Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., & Joulin, A. (2021). Emerging properties in self-supervised vision transformers. In *Proceedings of the IEEE/CVF International Conference on Computer Vision* (pp. 9650-9660).](http://openaccess.thecvf.com/content/ICCV2021/html/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html)\n\n[Zhai, X., Kolesnikov, A., Houlsby, N., & Beyer, L. (2021). Scaling Vision Transformers. *ArXiv, abs/2106.04560*.](https://arxiv.org/abs/2106.04560)\n\n[Wang, X., Girshick, R., Gupta, A., & He, K. (2018). Non-local neural networks. In *Proceedings of the IEEE conference on computer vision and pattern recognition* (pp. 7794-7803).](http://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.html)\n\n[Ramachandran, P., Parmar, N., Vaswani, A., Bello, I., Levskaya, A., & Shlens, J. (2019). Stand-alone self-attention in vision models. *Advances in Neural Information Processing Systems*, *32*.](https://proceedings.neurips.cc/paper/2019/hash/3416a75f4cea9109507cacd8e2f2aefc-Abstract.html)\n\n[Wang, H., Zhu, Y., Green, B., Adam, H., Yuille, A., & Chen, L. C. (2020, August). Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. In *European Conference on Computer Vision* (pp. 108-126). Springer, Cham.](https://link.springer.com/chapter/10.1007/978-3-030-58548-8_7)\n\n[Cordonnier, J. B., Loukas, A., & Jaggi, M. (2019). On the relationship between self-attention and convolutional layers. *arXiv preprint arXiv:1911.03584*.](https://arxiv.org/abs/1911.03584)\n\n[Chen, Mark, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. \"Generative pretraining from pixels.\" In *International Conference on Machine Learning*, pp. 1691-1703. PMLR, 2020.](http://proceedings.mlr.press/v119/chen20s.html)\n\n[Touvron, H., Vedaldi, A., Douze, M., & Jégou, H. (2019). Fixing the train-test resolution discrepancy. *Advances in neural information processing systems*, *32*.](https://proceedings.neurips.cc/paper/2019/hash/d03a857a23b5285736c4d55e0bb067c8-Abstract.html)\n\n[Brachmann, A., & Redies, C. (2016). Using convolutional neural network filters to measure left-right mirror symmetry in images. *Symmetry*, *8*(12), 144.](https://www.mdpi.com/2073-8994/8/12/144)\n\n[Tolstikhin, I. O., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., ... & Dosovitskiy, A. (2021). Mlp-mixer: An all-mlp architecture for vision. *Advances in Neural Information Processing Systems*, *34*.](https://proceedings.neurips.cc/paper/2021/hash/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Abstract.html)\n\n[Yu, W., Luo, M., Zhou, P., Si, C., Zhou, Y., Wang, X., ... & Yan, S. (2021). Metaformer is actually what you need for vision. arXiv preprint arXiv:2111.11418.](https://arxiv.org/abs/2111.11418)\n","slug":"paper-reading-Vision-Transformer","published":1,"updated":"2022-04-30T19:31:30.325Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl2n5f40b000op9ybba8384ls","content":"<blockquote>\n<p>Presented in 2021, the vision transformer model (ViT) is the most influential work in the CV field recent years. Its variants outperform the dominant convolutional networks in almost all CV tasks such as <a href=\"https://paperswithcode.com/sota/image-classification-on-imagenet\">classification</a> and <a href=\"https://paperswithcode.com/sota/object-detection-on-coco\">object detection</a>. And it breaks the border of CV and NLP, providing new thoughts to CV and multi-model areas.</p>\n</blockquote>\n<blockquote>\n<p>This is a <a href=\"https://daydreamatnight.github.io/2022/04/02/paper-reading-start/\">series of paper reading notes</a>, hopefully, to push me to read paper casually and to leave some record of what I've learned.</p>\n</blockquote>\n<span id=\"more\"></span>\n<p>Paper:</p>\n<p><a href=\"https://arxiv.org/abs/2010.11929v2\">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></p>\n<p>Useful links:</p>\n<p>https://www.bilibili.com/video/BV15P4y137jb</p>\n<p>https://theaisummer.com/vision-transformer/</p>\n<p><img src=\"VIT properties.png\" srcset=\"/img/loading.gif\" lazyload alt=\"VIT properties\" style=\"zoom:50%;\" /></p>\n<p>Not only Vision Transformer (ViT) performs better on traditional CV tasks, it has more impressive properties. As shown above, <a href=\"https://proceedings.neurips.cc/paper/2021/hash/c404a5adbf90e09631678b13b05d9d7a-Abstract.html\">Naseer et al.</a> demonstrate the tasks where ViT shows extra performance over CNN models, even over humans.</p>\n<h2 id=\"notes\">Notes</h2>\n<h3 id=\"abstract\">Abstract</h3>\n<p>While Transformer-based models such as BERT, GPT series, and T5 nail the NLP tasks, CV tasks remain dominated by CNN-based models. This paper applied a pure transformer encoder (same as BERT) to sequences of cut images and obtains a good classification result, especially with <strong>supervised</strong> pre-training on a large dataset then fine tuning on a mid-size dataset. Besides, fewer computational resources (meaning 2500 days of TPUv3) are need to attain good results, compared with CNN models.</p>\n<h3 id=\"conclusion\">Conclusion</h3>\n<p>Besides the paraphrasing part, the conclusion part discusses the future work based on the ViT. And all of them have follow-up works.</p>\n<ul>\n<li>Apply ViT to other CV tasks, given the promising performance of <a href=\"https://arxiv.org/pdf/2005.12872.pdf,\">DETR</a>. Only 1 and a half month later, <a href=\"https://arxiv.org/abs/2012.09958\">ViT-FRCNN</a> and <a href=\"http://openaccess.thecvf.com/content/CVPR2021/html/Zheng_Rethinking_Semantic_Segmentation_From_a_Sequence-to-Sequence_Perspective_With_Transformers_CVPR_2021_paper.html\">SEDR</a> mange to apply ViT on detection and segmentation respectively. And after 3 months, <a href=\"https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.html\">Swin Transformer</a> introduces hierarchical feature to transformer, making ViT more suitable to vision tasks.</li>\n<li>Self-supervised pre-training, given the great results of BERT and GPT in the NLP field. Initial explorations in the paper show a gap from the supervised pre-training. One year later, <a href=\"https://arxiv.org/abs/2111.06377\">MAE</a> narrows the gap successfully by generative model.\n<ul>\n<li>Besides, in the section of self-supervised learning, a contrastive learning is mentioned as well, and <a href=\"http://openaccess.thecvf.com/content/ICCV2021/html/Chen_An_Empirical_Study_of_Training_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html\">MOCO v3</a> and <a href=\"http://openaccess.thecvf.com/content/ICCV2021/html/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html\">DINO</a> follow this line.</li>\n</ul></li>\n<li>Further scaling up this model. Half year later, same group introduces <a href=\"https://arxiv.org/pdf/2106.04560v1.pdf\">Vit-G</a> with two billion parameters, attaining new SOTA on ImageNet of 90.45%.</li>\n</ul>\n<h3 id=\"introduction\">Introduction</h3>\n<p>Success of Transformer-based models on NLP tasks are firstly reviewed. And it is natural trying to apply such self-attention mechanism to vision. Yet here are one major obstacle:</p>\n<ul>\n<li>How to transfer a 2D picture to a 1D sequence?</li>\n</ul>\n<p>One intuitive thought is to flatten the picture directly and treat each pixel as an element. In this way, a medium size 224*224 picture will be converted to a 50,176 long sequence. However, the sequence length is quadratically related to model complexity. Morden hardware only supports input sequence length &lt;1000 of a pure self-attention model. For example BERT only accepts input length of 512.</p>\n<p>Yet the authors mange to incorporate the original transformer encoder in CV. In order to address the sequence length problem, they split the image into 16*16 patches, each patches denotes a sequence element (token). In this way, a 224*224 image can be converted as a sequence length of 16*16 with each element sized 14*14. Each element then gets linearised through a FC layer before being passed into the transformer encoder.</p>\n<p>And the afterwards experiments show that the new model doesn't perform well on mid-size model. One explanation is that transformer model lack the image-related inductive bias of CNN (locality and translation equalisation). Yet with pre-training on large dataset such as JFT-300 and ImagNet-21K, a better result than CNN can be approached.</p>\n<h3 id=\"related-work\">Related work</h3>\n<p><em>It is a detailed related work covering all the aspects in the original paper. I just pick few of them.</em></p>\n<p>All the related works aim at reducing the sequence length within the limitation caused by self-attention. Some try to combine CNN with self-attention. For example <a href=\"http://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.html\">Wang, X et al.'s work</a> takes the feature map extracted by CNN as the input of transformer. Others try to replace the CNN with a special variation of self-attention. For example, <a href=\"https://proceedings.neurips.cc/paper/2019/hash/3416a75f4cea9109507cacd8e2f2aefc-Abstract.html\">Ramachandran et al.'s work</a> replaces all convolutional sublayers of the ResNet-50 model with self-attention layers. To reduce the computational cost, a local region of the image instead of the whole image is used as the receptive field of the self-attention layer, meaning each pixel only attends to its neighbours in a restricted area. In another work line, <a href=\"https://link.springer.com/chapter/10.1007/978-3-030-58548-8_7\">Wang, H et.al's work</a> factorising 2D self-attention into two 1D self-attentions to significantly reduce computation complexity.</p>\n<p>And there is a very similar <a href=\"https://arxiv.org/abs/1911.03584\">Cordonnier et al's work</a> also split the images before the self-attention layer. Yet the patch size is 2*2 and with the dataset only <a href=\"https://paperswithcode.com/sota/image-classification-on-cifar-10\">CIFAR-10</a>. This paper enlarges the model, apply it in big dataset, and shows the scalability.</p>\n<p>Another related work image GPT(<a href=\"http://proceedings.mlr.press/v119/chen20s.html\">iGPT</a>) trains a GPT-2 scale generative network. Yet the highest accuracy on ImageNet is 72%, way less than 88% of this paper. But in 2021, an afterwards generative network <a href=\"https://arxiv.org/abs/2111.06377\">MAE</a> shows a competitive result of 87.8%, with good transfer leaning capability on segmentation and object detection as well.</p>\n<p>Besides, works exploring transfer learning performance of CNN model on larger datasets such as ImageNet-21k and JFT-300M are mentioned. And this paper studies the transformer instead of the CNN.</p>\n<h3 id=\"method\">Method</h3>\n<p>The whole big idea of the method part is leaving as much as possible the original transformer architecture in order to leverage the good feature and the existing mature efficient implementations of it.</p>\n<h4 id=\"vision-transformer-vit\">Vision Transformer (VIT)</h4>\n<p><img src=\"VIT model overview.png\" srcset=\"/img/loading.gif\" lazyload alt=\"VIT model overview\" style=\"zoom:60%;\" /></p>\n<p><img src=\"VIT algorithm.png\" srcset=\"/img/loading.gif\" lazyload alt=\"VIT algorithm\" style=\"zoom:40%;\" /></p>\n<p>From the overview, and the algorithm it should be called Vision BERT instead of Vision Transformer, given the pure encoder architecture and the extra \"classification token\". Assume a <em>224*224*3</em> image, after patching, the sequence length is <em>HW/P<sup>2</sup>=16*16=196</em> and the width is <em>14*14*3=768</em>. Given the hidden size of the model <em>D=768</em>, through linear projection layer (E), sequence <em>X [196*768]</em> are multiplied with weight <em>E [768*768]</em>. The resulting linear output <em>[196*768]</em> is then contacted with [cls], followed by adding standard 1D learnable positional embedding to be the transformer input <em>[197*768]</em>. After several transformer blocks, the output size does not change and the [cls] token is projected to a softmax classification layer</p>\n<h4 id=\"ablation-experiments\">Ablation experiments</h4>\n<p>The pre and post-processing are crucial for ViT given that the middle transformer encoder layers are kept as original. Multiple rounds of ablation experiments are carried out.</p>\n<ul>\n<li><p>Position embedding schemes, 2D embedding and relative embedding are applied to compare with the standard 1D embedding, and no evident gain is spotted.</p>\n<p><img src=\"VIT positional embedding ablation.png\" srcset=\"/img/loading.gif\" lazyload alt=\"VIT positional embedding ablation\" style=\"zoom:30%;\" /></p></li>\n<li><p>[cls] token vs average pooling, extra [cls] token is inherited from the Transformer model for text, and traditionally in CV, instead of an additional token, an average polling layer after the output layer is usually used as a classifier. The figure blow shows no both works. But in order to stick the original design as close as possible, [cls] token is applied.</p>\n<p><img src=\"VIT class token ablation.png\" srcset=\"/img/loading.gif\" lazyload alt=\"VIT class token ablation\" style=\"zoom:30%;\" /></p></li>\n</ul>\n<p>Another analysis after the model description:</p>\n<p><strong>Inductive bias:</strong> Less locality, translation equivariance and 2D neighbourhood structure are possessed by ViT, compared with CNN.</p>\n<p><strong>Hybrid Architecture:</strong> CNN can be used as a special embedding, leveraging the inductive bias of the CNN model.</p>\n<h4 id=\"limitation-on-fine-tuning\">Limitation on Fine-Tuning</h4>\n<p>Pre-train ViT at larger and higher resolution datasets is <a href=\"https://proceedings.neurips.cc/paper/2019/hash/d03a857a23b5285736c4d55e0bb067c8-Abstract.html\">proved</a> to be beneficial. Yet the input sequence lengths are different when training on two datasets with different resolutions, resulting in positional embeddings of different lengths. In this article, a 2D interpolation is applied to transfer a pre-trained positional embedding to another dataset to fine-tune. But the accuracy will loss if the resolution difference is too big.</p>\n<h3 id=\"experiments\">Experiments</h3>\n<h4 id=\"setup\">Setup</h4>\n<p>ResNet, ViT, and the hybrid model are evaluated together and ViT wins taking account of the pre-training cost. Besides, a small self-supervision experiment is deployed and sees potential.</p>\n<p>Two scales of ImageNet(1k and 21k) and JFT(303M) are used as pre-training dataset. Only classification tasks are evaluated with popular datasets.</p>\n<p>3 scales of ViT are designed with different patch size(inversely proportional to the amount of data). For example, ViT-L/16 means ViT-Large with 16 patches.</p>\n<p><img src=\"ViT variants.png\" srcset=\"/img/loading.gif\" lazyload alt=\"ViT variants\" style=\"zoom:50%;\" /></p>\n<h4 id=\"comparison-to-sota\">Comparison to SOTA</h4>\n<p>And the best results are shown below:</p>\n<p><img src=\"ViT results.png\" srcset=\"/img/loading.gif\" lazyload alt=\"ViT results\" style=\"zoom:60%;\" /></p>\n<h4 id=\"pre-training-cost-requirements\">Pre-training cost requirements</h4>\n<p>Figure 3 and 4 shows the performance of the presented models on different sizes of per-training datasets. ViT preforms competitive only starts from dataset 21k, and very well only on huge dataset.</p>\n<p>Figure 5 shows the transfer performances versus pre-training costs on JFT-300M of several models. And it shows that ViT is cheaper than ResNet. Interestingly, the Hybrid model is competitive on low pre-training cost.</p>\n<p><img src=\"ViT ablation 2.png\" srcset=\"/img/loading.gif\" lazyload alt=\"ViT ablation 2\" style=\"zoom:55%;\" /></p>\n<h4 id=\"inspecting-vit\">Inspecting ViT</h4>\n<p><img src=\"ViT inspecting.png\" srcset=\"/img/loading.gif\" lazyload alt=\"ViT inspecting\" style=\"zoom:42%;\" /></p>\n<p><strong>Figure7 Left:</strong> The learned linear projection weight matrix <em>E [768*768]</em> is inspected by PCA, and the first 28 components(modes) are visualised as embedding filters. They look pretty much similar to the early layer filters(kernels) of CNN (for example, the first layer of a CNN shown by <a href=\"https://www.mdpi.com/2073-8994/8/12/144\">Brachnmann et.al</a>). This similarity indicates that the linear patch embedding manages to represent the low-dimension structure of each patch. <img src=\"CNN first layer filters.png\" srcset=\"/img/loading.gif\" lazyload alt=\"CNN first layer filters\" style=\"zoom:40%;\" /></p>\n<p><strong>Figure7 mid:</strong> Position embedding visualisation first shows that the spatial information is captured well by the E matrix, given that the similarity matrix between patches matches well with the distance relationships of patches. Second, patterns across rows (and columns) have similar representations, indicating the embedding layer has successful learned the row-column relationship. Overall, the 1D positional embedding has learned the 2D structure, coherent with the ablation experiment result.</p>\n<p><strong>Figure7 Right:</strong> The receptive fields of the multi-head attention layers are evaluated by the mean attention distance. Compared with CNN whose receptive field increases linearly with the depth, the ViT attends the whole picture from the first layer, leveraging the natural advantage of transformer.</p>\n<h4 id=\"self-supervision\">Self-supervision</h4>\n<p>A preliminary exploration on masked patch prediction for self-supervision (mimicking one of the BERT pre-training tasks) has been employed. But the result is not satisfying. And contrastive pre-training are mentioned as a future work.</p>\n<h2 id=\"review\">Review</h2>\n<p>This is a concise-written, fundamental article. Just as presented in the conclusion part, it inspired many flow-up works from any direction in the CV area, such as applying to more tasks, changing the architecture(tokenisation, transformer block(<a href=\"https://proceedings.neurips.cc/paper/2021/hash/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Abstract.html\">MLP-mixer</a>： changing multi-head attention layers to MLP , <a href=\"https://arxiv.org/abs/2111.11418\">meta-former</a>: substituting multi-head attention layers to average pooling), changing objective function(self-supervised, contrastive learning), and multi modality.</p>\n<p>It's still unclear whether convolution, attention or MLP will win this game.</p>\n<h2 id=\"reference\">Reference</h2>\n<p><a href=\"https://proceedings.neurips.cc/paper/2021/hash/c404a5adbf90e09631678b13b05d9d7a-Abstract.html\">Naseer, M. M., Ranasinghe, K., Khan, S. H., Hayat, M., Shahbaz Khan, F., &amp; Yang, M. H. (2021). Intriguing properties of vision transformers. <em>Advances in Neural Information Processing Systems</em>, <em>34</em>.</a></p>\n<p><a href=\"https://arxiv.org/abs/2012.09958\">Beal, J., Kim, E., Tzeng, E., Park, D. H., Zhai, A., &amp; Kislyuk, D. (2020). Toward transformer-based object detection. <em>arXiv preprint arXiv:2012.09958</em>.</a></p>\n<p><a href=\"http://openaccess.thecvf.com/content/CVPR2021/html/Zheng_Rethinking_Semantic_Segmentation_From_a_Sequence-to-Sequence_Perspective_With_Transformers_CVPR_2021_paper.html\">Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., ... &amp; Zhang, L. (2021). Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em> (pp. 6881-6890).</a></p>\n<p><a href=\"https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.html\">Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... &amp; Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em> (pp. 10012-10022).</a></p>\n<p><a href=\"https://arxiv.org/pdf/2005.12872.pdf,\">Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., &amp; Zagoruyko, S. (2020, August). End-to-end object detection with transformers. In <em>European conference on computer vision</em> (pp. 213-229). Springer, Cham.</a></p>\n<p><a href=\"https://arxiv.org/abs/2111.06377\">He, K., Chen, X., Xie, S., Li, Y., Dollár, P., &amp; Girshick, R. (2021). Masked autoencoders are scalable vision learners. <em>arXiv preprint arXiv:2111.06377</em>.</a></p>\n<p><a href=\"http://openaccess.thecvf.com/content/ICCV2021/html/Chen_An_Empirical_Study_of_Training_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html\">Chen, X., Xie, S., &amp; He, K. (2021). An empirical study of training self-supervised vision transformers. In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em> (pp. 9640-9649).</a></p>\n<p><a href=\"http://openaccess.thecvf.com/content/ICCV2021/html/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html\">Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., &amp; Joulin, A. (2021). Emerging properties in self-supervised vision transformers. In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em> (pp. 9650-9660).</a></p>\n<p><a href=\"https://arxiv.org/abs/2106.04560\">Zhai, X., Kolesnikov, A., Houlsby, N., &amp; Beyer, L. (2021). Scaling Vision Transformers. <em>ArXiv, abs/2106.04560</em>.</a></p>\n<p><a href=\"http://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.html\">Wang, X., Girshick, R., Gupta, A., &amp; He, K. (2018). Non-local neural networks. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> (pp. 7794-7803).</a></p>\n<p><a href=\"https://proceedings.neurips.cc/paper/2019/hash/3416a75f4cea9109507cacd8e2f2aefc-Abstract.html\">Ramachandran, P., Parmar, N., Vaswani, A., Bello, I., Levskaya, A., &amp; Shlens, J. (2019). Stand-alone self-attention in vision models. <em>Advances in Neural Information Processing Systems</em>, <em>32</em>.</a></p>\n<p><a href=\"https://link.springer.com/chapter/10.1007/978-3-030-58548-8_7\">Wang, H., Zhu, Y., Green, B., Adam, H., Yuille, A., &amp; Chen, L. C. (2020, August). Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. In <em>European Conference on Computer Vision</em> (pp. 108-126). Springer, Cham.</a></p>\n<p><a href=\"https://arxiv.org/abs/1911.03584\">Cordonnier, J. B., Loukas, A., &amp; Jaggi, M. (2019). On the relationship between self-attention and convolutional layers. <em>arXiv preprint arXiv:1911.03584</em>.</a></p>\n<p><a href=\"http://proceedings.mlr.press/v119/chen20s.html\">Chen, Mark, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. \"Generative pretraining from pixels.\" In <em>International Conference on Machine Learning</em>, pp. 1691-1703. PMLR, 2020.</a></p>\n<p><a href=\"https://proceedings.neurips.cc/paper/2019/hash/d03a857a23b5285736c4d55e0bb067c8-Abstract.html\">Touvron, H., Vedaldi, A., Douze, M., &amp; Jégou, H. (2019). Fixing the train-test resolution discrepancy. <em>Advances in neural information processing systems</em>, <em>32</em>.</a></p>\n<p><a href=\"https://www.mdpi.com/2073-8994/8/12/144\">Brachmann, A., &amp; Redies, C. (2016). Using convolutional neural network filters to measure left-right mirror symmetry in images. <em>Symmetry</em>, <em>8</em>(12), 144.</a></p>\n<p><a href=\"https://proceedings.neurips.cc/paper/2021/hash/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Abstract.html\">Tolstikhin, I. O., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., ... &amp; Dosovitskiy, A. (2021). Mlp-mixer: An all-mlp architecture for vision. <em>Advances in Neural Information Processing Systems</em>, <em>34</em>.</a></p>\n<p><a href=\"https://arxiv.org/abs/2111.11418\">Yu, W., Luo, M., Zhou, P., Si, C., Zhou, Y., Wang, X., ... &amp; Yan, S. (2021). Metaformer is actually what you need for vision. arXiv preprint arXiv:2111.11418.</a></p>\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>Presented in 2021, the vision transformer model (ViT) is the most influential work in the CV field recent years. Its variants outperform the dominant convolutional networks in almost all CV tasks such as <a href=\"https://paperswithcode.com/sota/image-classification-on-imagenet\">classification</a> and <a href=\"https://paperswithcode.com/sota/object-detection-on-coco\">object detection</a>. And it breaks the border of CV and NLP, providing new thoughts to CV and multi-model areas.</p>\n</blockquote>\n<blockquote>\n<p>This is a <a href=\"https://daydreamatnight.github.io/2022/04/02/paper-reading-start/\">series of paper reading notes</a>, hopefully, to push me to read paper casually and to leave some record of what I've learned.</p>\n</blockquote>","more":"<p>Paper:</p>\n<p><a href=\"https://arxiv.org/abs/2010.11929v2\">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></p>\n<p>Useful links:</p>\n<p>https://www.bilibili.com/video/BV15P4y137jb</p>\n<p>https://theaisummer.com/vision-transformer/</p>\n<p><img src=\"VIT properties.png\" alt=\"VIT properties\" style=\"zoom:50%;\" /></p>\n<p>Not only Vision Transformer (ViT) performs better on traditional CV tasks, it has more impressive properties. As shown above, <a href=\"https://proceedings.neurips.cc/paper/2021/hash/c404a5adbf90e09631678b13b05d9d7a-Abstract.html\">Naseer et al.</a> demonstrate the tasks where ViT shows extra performance over CNN models, even over humans.</p>\n<h2 id=\"notes\">Notes</h2>\n<h3 id=\"abstract\">Abstract</h3>\n<p>While Transformer-based models such as BERT, GPT series, and T5 nail the NLP tasks, CV tasks remain dominated by CNN-based models. This paper applied a pure transformer encoder (same as BERT) to sequences of cut images and obtains a good classification result, especially with <strong>supervised</strong> pre-training on a large dataset then fine tuning on a mid-size dataset. Besides, fewer computational resources (meaning 2500 days of TPUv3) are need to attain good results, compared with CNN models.</p>\n<h3 id=\"conclusion\">Conclusion</h3>\n<p>Besides the paraphrasing part, the conclusion part discusses the future work based on the ViT. And all of them have follow-up works.</p>\n<ul>\n<li>Apply ViT to other CV tasks, given the promising performance of <a href=\"https://arxiv.org/pdf/2005.12872.pdf,\">DETR</a>. Only 1 and a half month later, <a href=\"https://arxiv.org/abs/2012.09958\">ViT-FRCNN</a> and <a href=\"http://openaccess.thecvf.com/content/CVPR2021/html/Zheng_Rethinking_Semantic_Segmentation_From_a_Sequence-to-Sequence_Perspective_With_Transformers_CVPR_2021_paper.html\">SEDR</a> mange to apply ViT on detection and segmentation respectively. And after 3 months, <a href=\"https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.html\">Swin Transformer</a> introduces hierarchical feature to transformer, making ViT more suitable to vision tasks.</li>\n<li>Self-supervised pre-training, given the great results of BERT and GPT in the NLP field. Initial explorations in the paper show a gap from the supervised pre-training. One year later, <a href=\"https://arxiv.org/abs/2111.06377\">MAE</a> narrows the gap successfully by generative model.\n<ul>\n<li>Besides, in the section of self-supervised learning, a contrastive learning is mentioned as well, and <a href=\"http://openaccess.thecvf.com/content/ICCV2021/html/Chen_An_Empirical_Study_of_Training_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html\">MOCO v3</a> and <a href=\"http://openaccess.thecvf.com/content/ICCV2021/html/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html\">DINO</a> follow this line.</li>\n</ul></li>\n<li>Further scaling up this model. Half year later, same group introduces <a href=\"https://arxiv.org/pdf/2106.04560v1.pdf\">Vit-G</a> with two billion parameters, attaining new SOTA on ImageNet of 90.45%.</li>\n</ul>\n<h3 id=\"introduction\">Introduction</h3>\n<p>Success of Transformer-based models on NLP tasks are firstly reviewed. And it is natural trying to apply such self-attention mechanism to vision. Yet here are one major obstacle:</p>\n<ul>\n<li>How to transfer a 2D picture to a 1D sequence?</li>\n</ul>\n<p>One intuitive thought is to flatten the picture directly and treat each pixel as an element. In this way, a medium size 224*224 picture will be converted to a 50,176 long sequence. However, the sequence length is quadratically related to model complexity. Morden hardware only supports input sequence length &lt;1000 of a pure self-attention model. For example BERT only accepts input length of 512.</p>\n<p>Yet the authors mange to incorporate the original transformer encoder in CV. In order to address the sequence length problem, they split the image into 16*16 patches, each patches denotes a sequence element (token). In this way, a 224*224 image can be converted as a sequence length of 16*16 with each element sized 14*14. Each element then gets linearised through a FC layer before being passed into the transformer encoder.</p>\n<p>And the afterwards experiments show that the new model doesn't perform well on mid-size model. One explanation is that transformer model lack the image-related inductive bias of CNN (locality and translation equalisation). Yet with pre-training on large dataset such as JFT-300 and ImagNet-21K, a better result than CNN can be approached.</p>\n<h3 id=\"related-work\">Related work</h3>\n<p><em>It is a detailed related work covering all the aspects in the original paper. I just pick few of them.</em></p>\n<p>All the related works aim at reducing the sequence length within the limitation caused by self-attention. Some try to combine CNN with self-attention. For example <a href=\"http://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.html\">Wang, X et al.'s work</a> takes the feature map extracted by CNN as the input of transformer. Others try to replace the CNN with a special variation of self-attention. For example, <a href=\"https://proceedings.neurips.cc/paper/2019/hash/3416a75f4cea9109507cacd8e2f2aefc-Abstract.html\">Ramachandran et al.'s work</a> replaces all convolutional sublayers of the ResNet-50 model with self-attention layers. To reduce the computational cost, a local region of the image instead of the whole image is used as the receptive field of the self-attention layer, meaning each pixel only attends to its neighbours in a restricted area. In another work line, <a href=\"https://link.springer.com/chapter/10.1007/978-3-030-58548-8_7\">Wang, H et.al's work</a> factorising 2D self-attention into two 1D self-attentions to significantly reduce computation complexity.</p>\n<p>And there is a very similar <a href=\"https://arxiv.org/abs/1911.03584\">Cordonnier et al's work</a> also split the images before the self-attention layer. Yet the patch size is 2*2 and with the dataset only <a href=\"https://paperswithcode.com/sota/image-classification-on-cifar-10\">CIFAR-10</a>. This paper enlarges the model, apply it in big dataset, and shows the scalability.</p>\n<p>Another related work image GPT(<a href=\"http://proceedings.mlr.press/v119/chen20s.html\">iGPT</a>) trains a GPT-2 scale generative network. Yet the highest accuracy on ImageNet is 72%, way less than 88% of this paper. But in 2021, an afterwards generative network <a href=\"https://arxiv.org/abs/2111.06377\">MAE</a> shows a competitive result of 87.8%, with good transfer leaning capability on segmentation and object detection as well.</p>\n<p>Besides, works exploring transfer learning performance of CNN model on larger datasets such as ImageNet-21k and JFT-300M are mentioned. And this paper studies the transformer instead of the CNN.</p>\n<h3 id=\"method\">Method</h3>\n<p>The whole big idea of the method part is leaving as much as possible the original transformer architecture in order to leverage the good feature and the existing mature efficient implementations of it.</p>\n<h4 id=\"vision-transformer-vit\">Vision Transformer (VIT)</h4>\n<p><img src=\"VIT model overview.png\" alt=\"VIT model overview\" style=\"zoom:60%;\" /></p>\n<p><img src=\"VIT algorithm.png\" alt=\"VIT algorithm\" style=\"zoom:40%;\" /></p>\n<p>From the overview, and the algorithm it should be called Vision BERT instead of Vision Transformer, given the pure encoder architecture and the extra \"classification token\". Assume a <em>224*224*3</em> image, after patching, the sequence length is <em>HW/P<sup>2</sup>=16*16=196</em> and the width is <em>14*14*3=768</em>. Given the hidden size of the model <em>D=768</em>, through linear projection layer (E), sequence <em>X [196*768]</em> are multiplied with weight <em>E [768*768]</em>. The resulting linear output <em>[196*768]</em> is then contacted with [cls], followed by adding standard 1D learnable positional embedding to be the transformer input <em>[197*768]</em>. After several transformer blocks, the output size does not change and the [cls] token is projected to a softmax classification layer</p>\n<h4 id=\"ablation-experiments\">Ablation experiments</h4>\n<p>The pre and post-processing are crucial for ViT given that the middle transformer encoder layers are kept as original. Multiple rounds of ablation experiments are carried out.</p>\n<ul>\n<li><p>Position embedding schemes, 2D embedding and relative embedding are applied to compare with the standard 1D embedding, and no evident gain is spotted.</p>\n<p><img src=\"VIT positional embedding ablation.png\" alt=\"VIT positional embedding ablation\" style=\"zoom:30%;\" /></p></li>\n<li><p>[cls] token vs average pooling, extra [cls] token is inherited from the Transformer model for text, and traditionally in CV, instead of an additional token, an average polling layer after the output layer is usually used as a classifier. The figure blow shows no both works. But in order to stick the original design as close as possible, [cls] token is applied.</p>\n<p><img src=\"VIT class token ablation.png\" alt=\"VIT class token ablation\" style=\"zoom:30%;\" /></p></li>\n</ul>\n<p>Another analysis after the model description:</p>\n<p><strong>Inductive bias:</strong> Less locality, translation equivariance and 2D neighbourhood structure are possessed by ViT, compared with CNN.</p>\n<p><strong>Hybrid Architecture:</strong> CNN can be used as a special embedding, leveraging the inductive bias of the CNN model.</p>\n<h4 id=\"limitation-on-fine-tuning\">Limitation on Fine-Tuning</h4>\n<p>Pre-train ViT at larger and higher resolution datasets is <a href=\"https://proceedings.neurips.cc/paper/2019/hash/d03a857a23b5285736c4d55e0bb067c8-Abstract.html\">proved</a> to be beneficial. Yet the input sequence lengths are different when training on two datasets with different resolutions, resulting in positional embeddings of different lengths. In this article, a 2D interpolation is applied to transfer a pre-trained positional embedding to another dataset to fine-tune. But the accuracy will loss if the resolution difference is too big.</p>\n<h3 id=\"experiments\">Experiments</h3>\n<h4 id=\"setup\">Setup</h4>\n<p>ResNet, ViT, and the hybrid model are evaluated together and ViT wins taking account of the pre-training cost. Besides, a small self-supervision experiment is deployed and sees potential.</p>\n<p>Two scales of ImageNet(1k and 21k) and JFT(303M) are used as pre-training dataset. Only classification tasks are evaluated with popular datasets.</p>\n<p>3 scales of ViT are designed with different patch size(inversely proportional to the amount of data). For example, ViT-L/16 means ViT-Large with 16 patches.</p>\n<p><img src=\"ViT variants.png\" alt=\"ViT variants\" style=\"zoom:50%;\" /></p>\n<h4 id=\"comparison-to-sota\">Comparison to SOTA</h4>\n<p>And the best results are shown below:</p>\n<p><img src=\"ViT results.png\" alt=\"ViT results\" style=\"zoom:60%;\" /></p>\n<h4 id=\"pre-training-cost-requirements\">Pre-training cost requirements</h4>\n<p>Figure 3 and 4 shows the performance of the presented models on different sizes of per-training datasets. ViT preforms competitive only starts from dataset 21k, and very well only on huge dataset.</p>\n<p>Figure 5 shows the transfer performances versus pre-training costs on JFT-300M of several models. And it shows that ViT is cheaper than ResNet. Interestingly, the Hybrid model is competitive on low pre-training cost.</p>\n<p><img src=\"ViT ablation 2.png\" alt=\"ViT ablation 2\" style=\"zoom:55%;\" /></p>\n<h4 id=\"inspecting-vit\">Inspecting ViT</h4>\n<p><img src=\"ViT inspecting.png\" alt=\"ViT inspecting\" style=\"zoom:42%;\" /></p>\n<p><strong>Figure7 Left:</strong> The learned linear projection weight matrix <em>E [768*768]</em> is inspected by PCA, and the first 28 components(modes) are visualised as embedding filters. They look pretty much similar to the early layer filters(kernels) of CNN (for example, the first layer of a CNN shown by <a href=\"https://www.mdpi.com/2073-8994/8/12/144\">Brachnmann et.al</a>). This similarity indicates that the linear patch embedding manages to represent the low-dimension structure of each patch. <img src=\"CNN first layer filters.png\" alt=\"CNN first layer filters\" style=\"zoom:40%;\" /></p>\n<p><strong>Figure7 mid:</strong> Position embedding visualisation first shows that the spatial information is captured well by the E matrix, given that the similarity matrix between patches matches well with the distance relationships of patches. Second, patterns across rows (and columns) have similar representations, indicating the embedding layer has successful learned the row-column relationship. Overall, the 1D positional embedding has learned the 2D structure, coherent with the ablation experiment result.</p>\n<p><strong>Figure7 Right:</strong> The receptive fields of the multi-head attention layers are evaluated by the mean attention distance. Compared with CNN whose receptive field increases linearly with the depth, the ViT attends the whole picture from the first layer, leveraging the natural advantage of transformer.</p>\n<h4 id=\"self-supervision\">Self-supervision</h4>\n<p>A preliminary exploration on masked patch prediction for self-supervision (mimicking one of the BERT pre-training tasks) has been employed. But the result is not satisfying. And contrastive pre-training are mentioned as a future work.</p>\n<h2 id=\"review\">Review</h2>\n<p>This is a concise-written, fundamental article. Just as presented in the conclusion part, it inspired many flow-up works from any direction in the CV area, such as applying to more tasks, changing the architecture(tokenisation, transformer block(<a href=\"https://proceedings.neurips.cc/paper/2021/hash/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Abstract.html\">MLP-mixer</a>： changing multi-head attention layers to MLP , <a href=\"https://arxiv.org/abs/2111.11418\">meta-former</a>: substituting multi-head attention layers to average pooling), changing objective function(self-supervised, contrastive learning), and multi modality.</p>\n<p>It's still unclear whether convolution, attention or MLP will win this game.</p>\n<h2 id=\"reference\">Reference</h2>\n<p><a href=\"https://proceedings.neurips.cc/paper/2021/hash/c404a5adbf90e09631678b13b05d9d7a-Abstract.html\">Naseer, M. M., Ranasinghe, K., Khan, S. H., Hayat, M., Shahbaz Khan, F., &amp; Yang, M. H. (2021). Intriguing properties of vision transformers. <em>Advances in Neural Information Processing Systems</em>, <em>34</em>.</a></p>\n<p><a href=\"https://arxiv.org/abs/2012.09958\">Beal, J., Kim, E., Tzeng, E., Park, D. H., Zhai, A., &amp; Kislyuk, D. (2020). Toward transformer-based object detection. <em>arXiv preprint arXiv:2012.09958</em>.</a></p>\n<p><a href=\"http://openaccess.thecvf.com/content/CVPR2021/html/Zheng_Rethinking_Semantic_Segmentation_From_a_Sequence-to-Sequence_Perspective_With_Transformers_CVPR_2021_paper.html\">Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., ... &amp; Zhang, L. (2021). Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em> (pp. 6881-6890).</a></p>\n<p><a href=\"https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.html\">Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... &amp; Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em> (pp. 10012-10022).</a></p>\n<p><a href=\"https://arxiv.org/pdf/2005.12872.pdf,\">Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., &amp; Zagoruyko, S. (2020, August). End-to-end object detection with transformers. In <em>European conference on computer vision</em> (pp. 213-229). Springer, Cham.</a></p>\n<p><a href=\"https://arxiv.org/abs/2111.06377\">He, K., Chen, X., Xie, S., Li, Y., Dollár, P., &amp; Girshick, R. (2021). Masked autoencoders are scalable vision learners. <em>arXiv preprint arXiv:2111.06377</em>.</a></p>\n<p><a href=\"http://openaccess.thecvf.com/content/ICCV2021/html/Chen_An_Empirical_Study_of_Training_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html\">Chen, X., Xie, S., &amp; He, K. (2021). An empirical study of training self-supervised vision transformers. In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em> (pp. 9640-9649).</a></p>\n<p><a href=\"http://openaccess.thecvf.com/content/ICCV2021/html/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html\">Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., &amp; Joulin, A. (2021). Emerging properties in self-supervised vision transformers. In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em> (pp. 9650-9660).</a></p>\n<p><a href=\"https://arxiv.org/abs/2106.04560\">Zhai, X., Kolesnikov, A., Houlsby, N., &amp; Beyer, L. (2021). Scaling Vision Transformers. <em>ArXiv, abs/2106.04560</em>.</a></p>\n<p><a href=\"http://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.html\">Wang, X., Girshick, R., Gupta, A., &amp; He, K. (2018). Non-local neural networks. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> (pp. 7794-7803).</a></p>\n<p><a href=\"https://proceedings.neurips.cc/paper/2019/hash/3416a75f4cea9109507cacd8e2f2aefc-Abstract.html\">Ramachandran, P., Parmar, N., Vaswani, A., Bello, I., Levskaya, A., &amp; Shlens, J. (2019). Stand-alone self-attention in vision models. <em>Advances in Neural Information Processing Systems</em>, <em>32</em>.</a></p>\n<p><a href=\"https://link.springer.com/chapter/10.1007/978-3-030-58548-8_7\">Wang, H., Zhu, Y., Green, B., Adam, H., Yuille, A., &amp; Chen, L. C. (2020, August). Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. In <em>European Conference on Computer Vision</em> (pp. 108-126). Springer, Cham.</a></p>\n<p><a href=\"https://arxiv.org/abs/1911.03584\">Cordonnier, J. B., Loukas, A., &amp; Jaggi, M. (2019). On the relationship between self-attention and convolutional layers. <em>arXiv preprint arXiv:1911.03584</em>.</a></p>\n<p><a href=\"http://proceedings.mlr.press/v119/chen20s.html\">Chen, Mark, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. \"Generative pretraining from pixels.\" In <em>International Conference on Machine Learning</em>, pp. 1691-1703. PMLR, 2020.</a></p>\n<p><a href=\"https://proceedings.neurips.cc/paper/2019/hash/d03a857a23b5285736c4d55e0bb067c8-Abstract.html\">Touvron, H., Vedaldi, A., Douze, M., &amp; Jégou, H. (2019). Fixing the train-test resolution discrepancy. <em>Advances in neural information processing systems</em>, <em>32</em>.</a></p>\n<p><a href=\"https://www.mdpi.com/2073-8994/8/12/144\">Brachmann, A., &amp; Redies, C. (2016). Using convolutional neural network filters to measure left-right mirror symmetry in images. <em>Symmetry</em>, <em>8</em>(12), 144.</a></p>\n<p><a href=\"https://proceedings.neurips.cc/paper/2021/hash/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Abstract.html\">Tolstikhin, I. O., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., ... &amp; Dosovitskiy, A. (2021). Mlp-mixer: An all-mlp architecture for vision. <em>Advances in Neural Information Processing Systems</em>, <em>34</em>.</a></p>\n<p><a href=\"https://arxiv.org/abs/2111.11418\">Yu, W., Luo, M., Zhou, P., Si, C., Zhou, Y., Wang, X., ... &amp; Yan, S. (2021). Metaformer is actually what you need for vision. arXiv preprint arXiv:2111.11418.</a></p>","wordcount":12278},{"title":"paper reading: start","author":"Ryan LI","declare":true,"date":"2022-04-02T13:49:37.000Z","_content":"\n> This is a brief description of the 3-step method of reading a paper. And a reading list.\n\n> This is a [series of paper reading notes](https://daydreamatnight.github.io/2022/04/02/paper-reading-start/), hopefully, to push me to read paper casually and to leave some record of what I've learned.\n\n<!-- more -->\n\n### 🔝 Mindmap for quick indexing\n\n{% markmap 300px %}\n\n- Classic\n  - Vision\n    - [AlexNet](https://daydreamatnight.github.io/2022/04/07/paper-reading-AlexNet/)\n    - [ResNet](https://daydreamatnight.github.io/2022/04/09/paper-reading-ResNet/)\n- [Transformer](https://daydreamatnight.github.io/2022/04/12/paper-reading-transformer/)\n  - NLP\n    - [BERT](https://daydreamatnight.github.io/2022/04/15/paper-reading-bert/)\n    - [GPT1-3](https://daydreamatnight.github.io/2022/04/18/paper-reading-GPT1-3/)\n  - Vision\n    - [ViT](https://daydreamatnight.github.io/2022/04/21/paper-reading-Vision-Transformer/)\n      - [MAE](https://daydreamatnight.github.io/2022/04/27/paper-reading-MAE/)\n- Novel \n  - [ GNN intro](https://daydreamatnight.github.io/2022/04/14/paper-reading-A-gentle-introduction-to-graph-neural-networks/)\n\n{%endmarkmap%}\n\n### How to read a paper\n\nThe method is inspired by [Andrew Ng's lecutre on Stanford](https://youtu.be/733m6qBH-jI) and [Mu Li's online lecutre](https://www.bilibili.com/video/BV1H44y1t75x)\n\n#### Up to 3 passes for one paper:\n\n| Section       | 1st pass          | 2nd pass            | 3rd pass     |\n| ------------- | ----------------- | ------------------- | ------------ |\n| 1. Title      | √                 |                     |              |\n| 2. Abstruct   | √                 |                     |              |\n| 3. Intro      |                   | criticle references |              |\n| 4.Method      | key pics & tables | key pics & tables   | how to apply |\n| 5. Expriment  | key pics & tables | key pics & tables   | how to do it |\n| 6. Conclusion | √                 |                     |              |\n\n**First pass:** title, abstract, conclusion. Take a look at important figures and tables in the Methods and Experiments section. In this way, you can spend less than 15 minutes to understand whether the paper is suitable for your research direction.\n\n**Second pass:** After confirming that the paper is worth reading, you can quickly go through the whole paper. You don’t need to know all the details. You need to understand important figures and tables, know what each part is doing, and circle the relevant literature. If you think the article is too difficult, you can read the cited literature.\n\n**The third pass:** what problem was asked. How to solve this problem. How to apply the experiment. Close the article and recall what each section is about.\n\n#### Some rules:  \n\n- Efficient high informative content first then the harder material \n- Skip the parts which do not make sense unless trying to do deep research on it\n- The related work part is often unimportant\n\n#### Questions that keep in mind:\n\n- what the authors try to accomplish\n- what are the key elements of the approach\n- what can you use yourself\n- what other references do you want to follow\n\n### List of papers\n\n[paper reading: AlexNet](https://daydreamatnight.github.io/2022/04/07/paper-reading-AlexNet/)\n\n[paper reading: ResNet](https://daydreamatnight.github.io/2022/04/09/paper-reading-ResNet/)\n\n[paper reading: transformer](https://daydreamatnight.github.io/2022/04/12/paper-reading-transformer/)\n\n[paper reading: A gentle introduction to graph neural networks](https://daydreamatnight.github.io/2022/04/14/paper-reading-A-gentle-introduction-to-graph-neural-networks/)\n\n[paper reading: bert](https://daydreamatnight.github.io/2022/04/15/paper-reading-bert/)\n\n[paper reading: GPT1-3](https://daydreamatnight.github.io/2022/04/18/paper-reading-GPT1-3/)\n\n[paper reading: Vision Transformer](https://daydreamatnight.github.io/2022/04/21/paper-reading-Vision-Transformer/)\n\n[paper reading: MAE](https://daydreamatnight.github.io/2022/04/27/paper-reading-MAE/)\n","source":"_posts/paper-reading-start.md","raw":"---\ntitle: 'paper reading: start'\nauthor: Ryan LI\ndeclare: true\ndate: 2022-04-02 21:49:37\ntags: \n  - paper reading\n  - deep learning\n---\n\n> This is a brief description of the 3-step method of reading a paper. And a reading list.\n\n> This is a [series of paper reading notes](https://daydreamatnight.github.io/2022/04/02/paper-reading-start/), hopefully, to push me to read paper casually and to leave some record of what I've learned.\n\n<!-- more -->\n\n### 🔝 Mindmap for quick indexing\n\n{% markmap 300px %}\n\n- Classic\n  - Vision\n    - [AlexNet](https://daydreamatnight.github.io/2022/04/07/paper-reading-AlexNet/)\n    - [ResNet](https://daydreamatnight.github.io/2022/04/09/paper-reading-ResNet/)\n- [Transformer](https://daydreamatnight.github.io/2022/04/12/paper-reading-transformer/)\n  - NLP\n    - [BERT](https://daydreamatnight.github.io/2022/04/15/paper-reading-bert/)\n    - [GPT1-3](https://daydreamatnight.github.io/2022/04/18/paper-reading-GPT1-3/)\n  - Vision\n    - [ViT](https://daydreamatnight.github.io/2022/04/21/paper-reading-Vision-Transformer/)\n      - [MAE](https://daydreamatnight.github.io/2022/04/27/paper-reading-MAE/)\n- Novel \n  - [ GNN intro](https://daydreamatnight.github.io/2022/04/14/paper-reading-A-gentle-introduction-to-graph-neural-networks/)\n\n{%endmarkmap%}\n\n### How to read a paper\n\nThe method is inspired by [Andrew Ng's lecutre on Stanford](https://youtu.be/733m6qBH-jI) and [Mu Li's online lecutre](https://www.bilibili.com/video/BV1H44y1t75x)\n\n#### Up to 3 passes for one paper:\n\n| Section       | 1st pass          | 2nd pass            | 3rd pass     |\n| ------------- | ----------------- | ------------------- | ------------ |\n| 1. Title      | √                 |                     |              |\n| 2. Abstruct   | √                 |                     |              |\n| 3. Intro      |                   | criticle references |              |\n| 4.Method      | key pics & tables | key pics & tables   | how to apply |\n| 5. Expriment  | key pics & tables | key pics & tables   | how to do it |\n| 6. Conclusion | √                 |                     |              |\n\n**First pass:** title, abstract, conclusion. Take a look at important figures and tables in the Methods and Experiments section. In this way, you can spend less than 15 minutes to understand whether the paper is suitable for your research direction.\n\n**Second pass:** After confirming that the paper is worth reading, you can quickly go through the whole paper. You don’t need to know all the details. You need to understand important figures and tables, know what each part is doing, and circle the relevant literature. If you think the article is too difficult, you can read the cited literature.\n\n**The third pass:** what problem was asked. How to solve this problem. How to apply the experiment. Close the article and recall what each section is about.\n\n#### Some rules:  \n\n- Efficient high informative content first then the harder material \n- Skip the parts which do not make sense unless trying to do deep research on it\n- The related work part is often unimportant\n\n#### Questions that keep in mind:\n\n- what the authors try to accomplish\n- what are the key elements of the approach\n- what can you use yourself\n- what other references do you want to follow\n\n### List of papers\n\n[paper reading: AlexNet](https://daydreamatnight.github.io/2022/04/07/paper-reading-AlexNet/)\n\n[paper reading: ResNet](https://daydreamatnight.github.io/2022/04/09/paper-reading-ResNet/)\n\n[paper reading: transformer](https://daydreamatnight.github.io/2022/04/12/paper-reading-transformer/)\n\n[paper reading: A gentle introduction to graph neural networks](https://daydreamatnight.github.io/2022/04/14/paper-reading-A-gentle-introduction-to-graph-neural-networks/)\n\n[paper reading: bert](https://daydreamatnight.github.io/2022/04/15/paper-reading-bert/)\n\n[paper reading: GPT1-3](https://daydreamatnight.github.io/2022/04/18/paper-reading-GPT1-3/)\n\n[paper reading: Vision Transformer](https://daydreamatnight.github.io/2022/04/21/paper-reading-Vision-Transformer/)\n\n[paper reading: MAE](https://daydreamatnight.github.io/2022/04/27/paper-reading-MAE/)\n","slug":"paper-reading-start","published":1,"updated":"2022-04-30T19:31:21.778Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl2n5f40e001jp9ybgot3h257","content":"<blockquote>\n<p>This is a brief description of the 3-step method of reading a paper. And a reading list.</p>\n</blockquote>\n<blockquote>\n<p>This is a <a href=\"https://daydreamatnight.github.io/2022/04/02/paper-reading-start/\">series of paper reading notes</a>, hopefully, to push me to read paper casually and to leave some record of what I've learned.</p>\n</blockquote>\n<span id=\"more\"></span>\n<h3 id=\"mindmap-for-quick-indexing\">🔝 Mindmap for quick indexing</h3>\n\n    <div class=\"markmap-container\" style=\"height:300px\">\n      <svg data='{\"t\":\"root\",\"d\":0,\"v\":\"\",\"c\":[{\"t\":\"list_item\",\"d\":2,\"p\":{\"lines\":[0,1]},\"v\":\"Classic\",\"c\":[{\"t\":\"list_item\",\"d\":4,\"p\":{\"lines\":[1,2]},\"v\":\"Vision\",\"c\":[{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[2,3]},\"v\":\"<a href=\\\"https://daydreamatnight.github.io/2022/04/07/paper-reading-AlexNet/\\\">AlexNet</a>\"},{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[3,4]},\"v\":\"<a href=\\\"https://daydreamatnight.github.io/2022/04/09/paper-reading-ResNet/\\\">ResNet</a>\"}]}]},{\"t\":\"list_item\",\"d\":2,\"p\":{\"lines\":[4,5]},\"v\":\"<a href=\\\"https://daydreamatnight.github.io/2022/04/12/paper-reading-transformer/\\\">Transformer</a>\",\"c\":[{\"t\":\"list_item\",\"d\":4,\"p\":{\"lines\":[5,6]},\"v\":\"NLP\",\"c\":[{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[6,7]},\"v\":\"<a href=\\\"https://daydreamatnight.github.io/2022/04/15/paper-reading-bert/\\\">BERT</a>\"},{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[7,8]},\"v\":\"<a href=\\\"https://daydreamatnight.github.io/2022/04/18/paper-reading-GPT1-3/\\\">GPT1-3</a>\"}]},{\"t\":\"list_item\",\"d\":4,\"p\":{\"lines\":[8,9]},\"v\":\"Vision\",\"c\":[{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[9,10]},\"v\":\"<a href=\\\"https://daydreamatnight.github.io/2022/04/21/paper-reading-Vision-Transformer/\\\">ViT</a>\",\"c\":[{\"t\":\"list_item\",\"d\":8,\"p\":{\"lines\":[10,11]},\"v\":\"<a href=\\\"https://daydreamatnight.github.io/2022/04/27/paper-reading-MAE/\\\">MAE</a>\"}]}]}]},{\"t\":\"list_item\",\"d\":2,\"p\":{\"lines\":[11,12]},\"v\":\"Novel\",\"c\":[{\"t\":\"list_item\",\"d\":4,\"p\":{\"lines\":[12,13]},\"v\":\"<a href=\\\"https://daydreamatnight.github.io/2022/04/14/paper-reading-A-gentle-introduction-to-graph-neural-networks/\\\"> GNN intro</a>\"}]}],\"p\":{}}'></svg>\n    </div>\n  \n<h3 id=\"how-to-read-a-paper\">How to read a paper</h3>\n<p>The method is inspired by <a href=\"https://youtu.be/733m6qBH-jI\">Andrew Ng's lecutre on Stanford</a> and <a href=\"https://www.bilibili.com/video/BV1H44y1t75x\">Mu Li's online lecutre</a></p>\n<h4 id=\"up-to-3-passes-for-one-paper\">Up to 3 passes for one paper:</h4>\n<table>\n<thead>\n<tr class=\"header\">\n<th>Section</th>\n<th>1st pass</th>\n<th>2nd pass</th>\n<th>3rd pass</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td>1. Title</td>\n<td>√</td>\n<td></td>\n<td></td>\n</tr>\n<tr class=\"even\">\n<td>2. Abstruct</td>\n<td>√</td>\n<td></td>\n<td></td>\n</tr>\n<tr class=\"odd\">\n<td>3. Intro</td>\n<td></td>\n<td>criticle references</td>\n<td></td>\n</tr>\n<tr class=\"even\">\n<td>4.Method</td>\n<td>key pics &amp; tables</td>\n<td>key pics &amp; tables</td>\n<td>how to apply</td>\n</tr>\n<tr class=\"odd\">\n<td>5. Expriment</td>\n<td>key pics &amp; tables</td>\n<td>key pics &amp; tables</td>\n<td>how to do it</td>\n</tr>\n<tr class=\"even\">\n<td>6. Conclusion</td>\n<td>√</td>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p><strong>First pass:</strong> title, abstract, conclusion. Take a look at important figures and tables in the Methods and Experiments section. In this way, you can spend less than 15 minutes to understand whether the paper is suitable for your research direction.</p>\n<p><strong>Second pass:</strong> After confirming that the paper is worth reading, you can quickly go through the whole paper. You don’t need to know all the details. You need to understand important figures and tables, know what each part is doing, and circle the relevant literature. If you think the article is too difficult, you can read the cited literature.</p>\n<p><strong>The third pass:</strong> what problem was asked. How to solve this problem. How to apply the experiment. Close the article and recall what each section is about.</p>\n<h4 id=\"some-rules\">Some rules:</h4>\n<ul>\n<li>Efficient high informative content first then the harder material</li>\n<li>Skip the parts which do not make sense unless trying to do deep research on it</li>\n<li>The related work part is often unimportant</li>\n</ul>\n<h4 id=\"questions-that-keep-in-mind\">Questions that keep in mind:</h4>\n<ul>\n<li>what the authors try to accomplish</li>\n<li>what are the key elements of the approach</li>\n<li>what can you use yourself</li>\n<li>what other references do you want to follow</li>\n</ul>\n<h3 id=\"list-of-papers\">List of papers</h3>\n<p><a href=\"https://daydreamatnight.github.io/2022/04/07/paper-reading-AlexNet/\">paper reading: AlexNet</a></p>\n<p><a href=\"https://daydreamatnight.github.io/2022/04/09/paper-reading-ResNet/\">paper reading: ResNet</a></p>\n<p><a href=\"https://daydreamatnight.github.io/2022/04/12/paper-reading-transformer/\">paper reading: transformer</a></p>\n<p><a href=\"https://daydreamatnight.github.io/2022/04/14/paper-reading-A-gentle-introduction-to-graph-neural-networks/\">paper reading: A gentle introduction to graph neural networks</a></p>\n<p><a href=\"https://daydreamatnight.github.io/2022/04/15/paper-reading-bert/\">paper reading: bert</a></p>\n<p><a href=\"https://daydreamatnight.github.io/2022/04/18/paper-reading-GPT1-3/\">paper reading: GPT1-3</a></p>\n<p><a href=\"https://daydreamatnight.github.io/2022/04/21/paper-reading-Vision-Transformer/\">paper reading: Vision Transformer</a></p>\n<p><a href=\"https://daydreamatnight.github.io/2022/04/27/paper-reading-MAE/\">paper reading: MAE</a></p>\n\n    <style>.markmap-container{display:flex;justify-content:center;margin:0 auto;width:90%;height:500px}.markmap-container svg{width:100%;height:100%}@media(max-width:768px){.markmap-container{height:400px}}</style>\n    <script src=\"https://cdn.jsdelivr.net/npm/d3@6\"></script>\n    <script src=\"https://cdn.jsdelivr.net/npm/markmap-view\"></script>\n    <script> document.querySelectorAll('.markmap-container>svg').forEach(mindmap => markmap.Markmap.create(mindmap, null, JSON.parse(mindmap.getAttribute('data'))))</script>\n  ","site":{"data":{}},"excerpt":"<blockquote>\n<p>This is a brief description of the 3-step method of reading a paper. And a reading list.</p>\n</blockquote>\n<blockquote>\n<p>This is a <a href=\"https://daydreamatnight.github.io/2022/04/02/paper-reading-start/\">series of paper reading notes</a>, hopefully, to push me to read paper casually and to leave some record of what I've learned.</p>\n</blockquote>","more":"<h3 id=\"mindmap-for-quick-indexing\">🔝 Mindmap for quick indexing</h3>\n\n    <div class=\"markmap-container\" style=\"height:300px\">\n      <svg data='{\"t\":\"root\",\"d\":0,\"v\":\"\",\"c\":[{\"t\":\"list_item\",\"d\":2,\"p\":{\"lines\":[0,1]},\"v\":\"Classic\",\"c\":[{\"t\":\"list_item\",\"d\":4,\"p\":{\"lines\":[1,2]},\"v\":\"Vision\",\"c\":[{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[2,3]},\"v\":\"<a href=\\\"https://daydreamatnight.github.io/2022/04/07/paper-reading-AlexNet/\\\">AlexNet</a>\"},{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[3,4]},\"v\":\"<a href=\\\"https://daydreamatnight.github.io/2022/04/09/paper-reading-ResNet/\\\">ResNet</a>\"}]}]},{\"t\":\"list_item\",\"d\":2,\"p\":{\"lines\":[4,5]},\"v\":\"<a href=\\\"https://daydreamatnight.github.io/2022/04/12/paper-reading-transformer/\\\">Transformer</a>\",\"c\":[{\"t\":\"list_item\",\"d\":4,\"p\":{\"lines\":[5,6]},\"v\":\"NLP\",\"c\":[{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[6,7]},\"v\":\"<a href=\\\"https://daydreamatnight.github.io/2022/04/15/paper-reading-bert/\\\">BERT</a>\"},{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[7,8]},\"v\":\"<a href=\\\"https://daydreamatnight.github.io/2022/04/18/paper-reading-GPT1-3/\\\">GPT1-3</a>\"}]},{\"t\":\"list_item\",\"d\":4,\"p\":{\"lines\":[8,9]},\"v\":\"Vision\",\"c\":[{\"t\":\"list_item\",\"d\":6,\"p\":{\"lines\":[9,10]},\"v\":\"<a href=\\\"https://daydreamatnight.github.io/2022/04/21/paper-reading-Vision-Transformer/\\\">ViT</a>\",\"c\":[{\"t\":\"list_item\",\"d\":8,\"p\":{\"lines\":[10,11]},\"v\":\"<a href=\\\"https://daydreamatnight.github.io/2022/04/27/paper-reading-MAE/\\\">MAE</a>\"}]}]}]},{\"t\":\"list_item\",\"d\":2,\"p\":{\"lines\":[11,12]},\"v\":\"Novel\",\"c\":[{\"t\":\"list_item\",\"d\":4,\"p\":{\"lines\":[12,13]},\"v\":\"<a href=\\\"https://daydreamatnight.github.io/2022/04/14/paper-reading-A-gentle-introduction-to-graph-neural-networks/\\\"> GNN intro</a>\"}]}],\"p\":{}}'></svg>\n    </div>\n  \n<h3 id=\"how-to-read-a-paper\">How to read a paper</h3>\n<p>The method is inspired by <a href=\"https://youtu.be/733m6qBH-jI\">Andrew Ng's lecutre on Stanford</a> and <a href=\"https://www.bilibili.com/video/BV1H44y1t75x\">Mu Li's online lecutre</a></p>\n<h4 id=\"up-to-3-passes-for-one-paper\">Up to 3 passes for one paper:</h4>\n<table>\n<thead>\n<tr class=\"header\">\n<th>Section</th>\n<th>1st pass</th>\n<th>2nd pass</th>\n<th>3rd pass</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td>1. Title</td>\n<td>√</td>\n<td></td>\n<td></td>\n</tr>\n<tr class=\"even\">\n<td>2. Abstruct</td>\n<td>√</td>\n<td></td>\n<td></td>\n</tr>\n<tr class=\"odd\">\n<td>3. Intro</td>\n<td></td>\n<td>criticle references</td>\n<td></td>\n</tr>\n<tr class=\"even\">\n<td>4.Method</td>\n<td>key pics &amp; tables</td>\n<td>key pics &amp; tables</td>\n<td>how to apply</td>\n</tr>\n<tr class=\"odd\">\n<td>5. Expriment</td>\n<td>key pics &amp; tables</td>\n<td>key pics &amp; tables</td>\n<td>how to do it</td>\n</tr>\n<tr class=\"even\">\n<td>6. Conclusion</td>\n<td>√</td>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p><strong>First pass:</strong> title, abstract, conclusion. Take a look at important figures and tables in the Methods and Experiments section. In this way, you can spend less than 15 minutes to understand whether the paper is suitable for your research direction.</p>\n<p><strong>Second pass:</strong> After confirming that the paper is worth reading, you can quickly go through the whole paper. You don’t need to know all the details. You need to understand important figures and tables, know what each part is doing, and circle the relevant literature. If you think the article is too difficult, you can read the cited literature.</p>\n<p><strong>The third pass:</strong> what problem was asked. How to solve this problem. How to apply the experiment. Close the article and recall what each section is about.</p>\n<h4 id=\"some-rules\">Some rules:</h4>\n<ul>\n<li>Efficient high informative content first then the harder material</li>\n<li>Skip the parts which do not make sense unless trying to do deep research on it</li>\n<li>The related work part is often unimportant</li>\n</ul>\n<h4 id=\"questions-that-keep-in-mind\">Questions that keep in mind:</h4>\n<ul>\n<li>what the authors try to accomplish</li>\n<li>what are the key elements of the approach</li>\n<li>what can you use yourself</li>\n<li>what other references do you want to follow</li>\n</ul>\n<h3 id=\"list-of-papers\">List of papers</h3>\n<p><a href=\"https://daydreamatnight.github.io/2022/04/07/paper-reading-AlexNet/\">paper reading: AlexNet</a></p>\n<p><a href=\"https://daydreamatnight.github.io/2022/04/09/paper-reading-ResNet/\">paper reading: ResNet</a></p>\n<p><a href=\"https://daydreamatnight.github.io/2022/04/12/paper-reading-transformer/\">paper reading: transformer</a></p>\n<p><a href=\"https://daydreamatnight.github.io/2022/04/14/paper-reading-A-gentle-introduction-to-graph-neural-networks/\">paper reading: A gentle introduction to graph neural networks</a></p>\n<p><a href=\"https://daydreamatnight.github.io/2022/04/15/paper-reading-bert/\">paper reading: bert</a></p>\n<p><a href=\"https://daydreamatnight.github.io/2022/04/18/paper-reading-GPT1-3/\">paper reading: GPT1-3</a></p>\n<p><a href=\"https://daydreamatnight.github.io/2022/04/21/paper-reading-Vision-Transformer/\">paper reading: Vision Transformer</a></p>\n<p><a href=\"https://daydreamatnight.github.io/2022/04/27/paper-reading-MAE/\">paper reading: MAE</a></p>","wordcount":1985},{"title":"paper reading: transformer","author":"Ryan LI","toc":true,"declare":true,"date":"2022-04-12T15:31:21.000Z","_content":"\n> The transformer is the most important achievement in the last 5 years. It presents the fourth class of deep learning models besides MLP, CNN and RNN. And had a huge impact on the entire deep learning field, be it NLP or CV. Even the way the paper and network are named leads a trend.\n\n> This is a [series of paper reading notes](https://daydreamatnight.github.io/2022/04/02/paper-reading-start/), hopefully, to push me to read paper casually and to leave some record of what I've learned.\n\n<!-- more -->\n\nPaper link: [Attention is all you need](https://proceedings.neurips.cc/paper/7181-attention-is-all-you-need) \n\nUseful link: https://www.bilibili.com/video/BV1pu411o7BE\n\n## Notes by sections\n\n### 0. Abstract \n\nAccording to the abstract, this work is first presented in the small field of machine translation, but because of the strong ability of generalisation. The transformer architecture has been extended in other fields such as CV and video.\n\n### 6. Conclusion\n\nIn the last part of the conclusion section, the future of transformer is partly predicted by the authors, though most of these future work is done by other researchers.\n\n### 1. Introduction\n\nFirstly, traditional RNN, or GRU has been introduced. Then after a brief description of the sequential nature of RNN based models, the main problems of them are presented. That is, poor ability of parallelising, and poor long-range dependencies.\n\nLater after the introduction of the attention framework and how to combine attention into RNN, transformer is presented.\n\n### 2. Background\n\nFirst, with the goal of gaining the ability to parallelise, the authors look back on using CNN for sequential data. With CNN, parallelising ability is in its nature and with multiple channels, multiple features can be learned through training. Likewise, the transformer is also parallelisable and Multi-Head Attention is designed with the ability to learn multiple features. But additionally the transformer is easier to learn dependencies between distant positions. \n\nAfterwards, related work on self-attention and memory network is mentioned. And both the connections and distinctions with transformer are elaborated.\n\n### 3. Model Architecture\n\nAfter introducing the basic idea of the encoder-decoder architecture, the key transformer architecture diagram is given. And each part of the model is briefly described. (The diagram in this blog below is not the original, but a combination of 3 diagrams in the paper.)\n\n<img src=\"transformer.png\" alt=\"transformer figure 1\" style=\"zoom:50%;\" />\n\n#### 3.1 encoder decoder stacks\n\n**Encoder:**\n\nIn addition to briefly describing the encoder architecture, the authors mention that in order to avoid the projection step on the residual connection layers, the same input and output dimension is chosen in each sublayer, which is different from what CNN normally does. And this simple design yields the super parameters of the encoder to 2: Nx - the number of the \"encoder block\" and d_model - the feature dimension of the output layers. And this design makes the follow-up work such as bert, GPT-3 simple as well.\n\nHere is one thing that has been ignored: the definition of layer norm. \n\n> Unlike [batch normalization](https://paperswithcode.com/method/batch-normalization), **Layer Normalization** directly estimates the normalization statistics from the summed inputs to the neurons within a hidden layer so the normalization does not introduce any new dependencies between training cases. \n\nMore is on https://paperswithcode.com/method/layer-normalization\n\n**Decoder:**\n\nBecause in the prediction mode, the decoder is self-regressing, meaning that when predict y_t, only x_0 to x_t are available. However, the self-attention layer is able to attend to all the data i.e. x_0 - x_n. To prevent this, a masked multi-head attention layer is introduced so that when predict y_t, x_t+1 - x_n are masked.\n\n#### 3.2 Attention\n\nSimilarly, after running over the definition of attention mechanism, the 2 modifications: scaled dot-product attention and \n\nMulti-head attention are described.\n\nAs we all known, there are two methods (attention scoring functions) of calculating the similarity of query and key(attention weights over the value). One is additive attention. It is complex but allows different length of key (dk) and query (qk). And Additive attention layer includes learnable parameters that can be tuned during training. Another one is dot-product attention, it is simply dot product of the transverse query and key matrix. It requires same length of query and key and no learnable parameters are introduced. \n\n**scaled dot-product attention**\n\nIt pretty much is the dot-product attention multiply with a scaling factor of 1/√dk. And it is because for large dk, the deviation of the dot-product results might get too large, and the according gradient can get extremely small and hard to train.\n\nBut look the figure 2 of the paper, an optional mask node is shown in the computation graph. And the algorithm of mask is not described in detail. Basically when calculating q_t, in the mask node, the attention scores a_t+1 - a_t are substituted to a huge negative so that after softmax, the attention weight is 0 on values v_t+1 to v_n. And it is corresponding to the how the decoder works in the prediction mode.\n\n**multi-head attention**\n\nHow mutli-head works is introducing clearly in the paper:\n\n> linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values. These are concatenated and once again projected, resulting in the final values\n\nThe question is why doing that. First, no learnable parameters in the dot-product attention, and second, to mimic the CNN extracting the features from different perspectives.\n\n**Applications of Attention in our Model**\n\nIt is well shown on the whole architecture figure. In the figure it consists of 2 self-attention block and 1 attention block, where key-value is from the encoder and query is from the decoder.\n\n#### 3.3 Position-wise feed-forward networks\n\nIt is a simply an MLP with one hidden layer. And the hidden size is 4 times of the input and output size.\n\n#### 3.4 Embedding and softmax\n\nOne point is they multiply the weights by√d_model. And first same reason with before, and second, with scaled weights, the scale matches the scale of positional encoding described below.\n\n#### 3.5 Positional Encoding\n\nDifferent with RNN, transformer architecture reads no sequence information. And the authors add the positional information in the input data.\n\n### 4 Why Self-Attention\n\nBasically it explains the table 1 below:\n\n<img src=\"transformer table1.png\" alt=\"transformer figure 1\" style=\"zoom:30%;\" />\n\nIt might be the case but it's not been accepted by some researchers. And the restricted self-attention is rarely used. The table shows that if n and d are the same, self-attention, RNN, and CNN possess the same complexity per layer, but self-attention outperforms them on the sequential operations and the max path length. But actually, a huge mount of layers and parameters  and data are required for a self-attention model achieving a similar result as RNN and CNN. And nowadays all models based on the transformer are very expensive. \n\n### 6 Results\n\n#### 6.2 Model Variations\n\n<img src=\"transformer table 3.png\" alt=\"transformer table 3\" style=\"zoom:30%;\" />\n\nFrom Table 3, we can see the hyper parameters are not so much, and this simple design benefits the follow-ups. For example Bert and GPT.\n\n## Reviews\n\nWriting: It is concise and neat. But if the possible, it's better to describe why of doing it, and show more thoughts on the model to make the paper \"deeper\". \n\nModel: Transformer change the NLP filed just like how CNN change the CV filed. Through after transformer based model such as BERT, it is possible to pretrain a huge model to rise all the NLP performance. Besides, in other fields such as CV and audio, transformer becomes a great rising point. And the fact that transformer may suitable for all the deep learning tasks gives a new thought of muti-model learning. Maybe a general model able to extracting video, pictures, audio into a same semetic space is coming soon.\n\nYet, despite the great experiment performance, we still can't fully understand why transformer works. For example, \n\nattention is not all one need, because the residual connection and MLP are all critical. We still don't know why.\n\nWhy without explicitly model the sequence or the space, transformer outperform RNN and CNN. One explanation is that it is because transformer's inductive bias is more relaxed than either recurrent or convolutional architectures. And that is why huge amount of data are needed for transformer to achieve a good result.\n","source":"_posts/paper-reading-transformer.md","raw":"---\ntitle: 'paper reading: transformer'\nauthor: Ryan LI\ntoc: true\ndeclare: true\ndate: 2022-04-12 23:31:21\ntags:\n  - paper reading\n  - deep learning\n---\n\n> The transformer is the most important achievement in the last 5 years. It presents the fourth class of deep learning models besides MLP, CNN and RNN. And had a huge impact on the entire deep learning field, be it NLP or CV. Even the way the paper and network are named leads a trend.\n\n> This is a [series of paper reading notes](https://daydreamatnight.github.io/2022/04/02/paper-reading-start/), hopefully, to push me to read paper casually and to leave some record of what I've learned.\n\n<!-- more -->\n\nPaper link: [Attention is all you need](https://proceedings.neurips.cc/paper/7181-attention-is-all-you-need) \n\nUseful link: https://www.bilibili.com/video/BV1pu411o7BE\n\n## Notes by sections\n\n### 0. Abstract \n\nAccording to the abstract, this work is first presented in the small field of machine translation, but because of the strong ability of generalisation. The transformer architecture has been extended in other fields such as CV and video.\n\n### 6. Conclusion\n\nIn the last part of the conclusion section, the future of transformer is partly predicted by the authors, though most of these future work is done by other researchers.\n\n### 1. Introduction\n\nFirstly, traditional RNN, or GRU has been introduced. Then after a brief description of the sequential nature of RNN based models, the main problems of them are presented. That is, poor ability of parallelising, and poor long-range dependencies.\n\nLater after the introduction of the attention framework and how to combine attention into RNN, transformer is presented.\n\n### 2. Background\n\nFirst, with the goal of gaining the ability to parallelise, the authors look back on using CNN for sequential data. With CNN, parallelising ability is in its nature and with multiple channels, multiple features can be learned through training. Likewise, the transformer is also parallelisable and Multi-Head Attention is designed with the ability to learn multiple features. But additionally the transformer is easier to learn dependencies between distant positions. \n\nAfterwards, related work on self-attention and memory network is mentioned. And both the connections and distinctions with transformer are elaborated.\n\n### 3. Model Architecture\n\nAfter introducing the basic idea of the encoder-decoder architecture, the key transformer architecture diagram is given. And each part of the model is briefly described. (The diagram in this blog below is not the original, but a combination of 3 diagrams in the paper.)\n\n<img src=\"transformer.png\" alt=\"transformer figure 1\" style=\"zoom:50%;\" />\n\n#### 3.1 encoder decoder stacks\n\n**Encoder:**\n\nIn addition to briefly describing the encoder architecture, the authors mention that in order to avoid the projection step on the residual connection layers, the same input and output dimension is chosen in each sublayer, which is different from what CNN normally does. And this simple design yields the super parameters of the encoder to 2: Nx - the number of the \"encoder block\" and d_model - the feature dimension of the output layers. And this design makes the follow-up work such as bert, GPT-3 simple as well.\n\nHere is one thing that has been ignored: the definition of layer norm. \n\n> Unlike [batch normalization](https://paperswithcode.com/method/batch-normalization), **Layer Normalization** directly estimates the normalization statistics from the summed inputs to the neurons within a hidden layer so the normalization does not introduce any new dependencies between training cases. \n\nMore is on https://paperswithcode.com/method/layer-normalization\n\n**Decoder:**\n\nBecause in the prediction mode, the decoder is self-regressing, meaning that when predict y_t, only x_0 to x_t are available. However, the self-attention layer is able to attend to all the data i.e. x_0 - x_n. To prevent this, a masked multi-head attention layer is introduced so that when predict y_t, x_t+1 - x_n are masked.\n\n#### 3.2 Attention\n\nSimilarly, after running over the definition of attention mechanism, the 2 modifications: scaled dot-product attention and \n\nMulti-head attention are described.\n\nAs we all known, there are two methods (attention scoring functions) of calculating the similarity of query and key(attention weights over the value). One is additive attention. It is complex but allows different length of key (dk) and query (qk). And Additive attention layer includes learnable parameters that can be tuned during training. Another one is dot-product attention, it is simply dot product of the transverse query and key matrix. It requires same length of query and key and no learnable parameters are introduced. \n\n**scaled dot-product attention**\n\nIt pretty much is the dot-product attention multiply with a scaling factor of 1/√dk. And it is because for large dk, the deviation of the dot-product results might get too large, and the according gradient can get extremely small and hard to train.\n\nBut look the figure 2 of the paper, an optional mask node is shown in the computation graph. And the algorithm of mask is not described in detail. Basically when calculating q_t, in the mask node, the attention scores a_t+1 - a_t are substituted to a huge negative so that after softmax, the attention weight is 0 on values v_t+1 to v_n. And it is corresponding to the how the decoder works in the prediction mode.\n\n**multi-head attention**\n\nHow mutli-head works is introducing clearly in the paper:\n\n> linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values. These are concatenated and once again projected, resulting in the final values\n\nThe question is why doing that. First, no learnable parameters in the dot-product attention, and second, to mimic the CNN extracting the features from different perspectives.\n\n**Applications of Attention in our Model**\n\nIt is well shown on the whole architecture figure. In the figure it consists of 2 self-attention block and 1 attention block, where key-value is from the encoder and query is from the decoder.\n\n#### 3.3 Position-wise feed-forward networks\n\nIt is a simply an MLP with one hidden layer. And the hidden size is 4 times of the input and output size.\n\n#### 3.4 Embedding and softmax\n\nOne point is they multiply the weights by√d_model. And first same reason with before, and second, with scaled weights, the scale matches the scale of positional encoding described below.\n\n#### 3.5 Positional Encoding\n\nDifferent with RNN, transformer architecture reads no sequence information. And the authors add the positional information in the input data.\n\n### 4 Why Self-Attention\n\nBasically it explains the table 1 below:\n\n<img src=\"transformer table1.png\" alt=\"transformer figure 1\" style=\"zoom:30%;\" />\n\nIt might be the case but it's not been accepted by some researchers. And the restricted self-attention is rarely used. The table shows that if n and d are the same, self-attention, RNN, and CNN possess the same complexity per layer, but self-attention outperforms them on the sequential operations and the max path length. But actually, a huge mount of layers and parameters  and data are required for a self-attention model achieving a similar result as RNN and CNN. And nowadays all models based on the transformer are very expensive. \n\n### 6 Results\n\n#### 6.2 Model Variations\n\n<img src=\"transformer table 3.png\" alt=\"transformer table 3\" style=\"zoom:30%;\" />\n\nFrom Table 3, we can see the hyper parameters are not so much, and this simple design benefits the follow-ups. For example Bert and GPT.\n\n## Reviews\n\nWriting: It is concise and neat. But if the possible, it's better to describe why of doing it, and show more thoughts on the model to make the paper \"deeper\". \n\nModel: Transformer change the NLP filed just like how CNN change the CV filed. Through after transformer based model such as BERT, it is possible to pretrain a huge model to rise all the NLP performance. Besides, in other fields such as CV and audio, transformer becomes a great rising point. And the fact that transformer may suitable for all the deep learning tasks gives a new thought of muti-model learning. Maybe a general model able to extracting video, pictures, audio into a same semetic space is coming soon.\n\nYet, despite the great experiment performance, we still can't fully understand why transformer works. For example, \n\nattention is not all one need, because the residual connection and MLP are all critical. We still don't know why.\n\nWhy without explicitly model the sequence or the space, transformer outperform RNN and CNN. One explanation is that it is because transformer's inductive bias is more relaxed than either recurrent or convolutional architectures. And that is why huge amount of data are needed for transformer to achieve a good result.\n","slug":"paper-reading-transformer","published":1,"updated":"2022-04-30T19:31:26.294Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl2n5f40e001kp9yb21ny6ups","content":"<blockquote>\n<p>The transformer is the most important achievement in the last 5 years. It presents the fourth class of deep learning models besides MLP, CNN and RNN. And had a huge impact on the entire deep learning field, be it NLP or CV. Even the way the paper and network are named leads a trend.</p>\n</blockquote>\n<blockquote>\n<p>This is a <a href=\"https://daydreamatnight.github.io/2022/04/02/paper-reading-start/\">series of paper reading notes</a>, hopefully, to push me to read paper casually and to leave some record of what I've learned.</p>\n</blockquote>\n<span id=\"more\"></span>\n<p>Paper link: <a href=\"https://proceedings.neurips.cc/paper/7181-attention-is-all-you-need\">Attention is all you need</a></p>\n<p>Useful link: https://www.bilibili.com/video/BV1pu411o7BE</p>\n<h2 id=\"notes-by-sections\">Notes by sections</h2>\n<h3 id=\"abstract\">0. Abstract</h3>\n<p>According to the abstract, this work is first presented in the small field of machine translation, but because of the strong ability of generalisation. The transformer architecture has been extended in other fields such as CV and video.</p>\n<h3 id=\"conclusion\">6. Conclusion</h3>\n<p>In the last part of the conclusion section, the future of transformer is partly predicted by the authors, though most of these future work is done by other researchers.</p>\n<h3 id=\"introduction\">1. Introduction</h3>\n<p>Firstly, traditional RNN, or GRU has been introduced. Then after a brief description of the sequential nature of RNN based models, the main problems of them are presented. That is, poor ability of parallelising, and poor long-range dependencies.</p>\n<p>Later after the introduction of the attention framework and how to combine attention into RNN, transformer is presented.</p>\n<h3 id=\"background\">2. Background</h3>\n<p>First, with the goal of gaining the ability to parallelise, the authors look back on using CNN for sequential data. With CNN, parallelising ability is in its nature and with multiple channels, multiple features can be learned through training. Likewise, the transformer is also parallelisable and Multi-Head Attention is designed with the ability to learn multiple features. But additionally the transformer is easier to learn dependencies between distant positions.</p>\n<p>Afterwards, related work on self-attention and memory network is mentioned. And both the connections and distinctions with transformer are elaborated.</p>\n<h3 id=\"model-architecture\">3. Model Architecture</h3>\n<p>After introducing the basic idea of the encoder-decoder architecture, the key transformer architecture diagram is given. And each part of the model is briefly described. (The diagram in this blog below is not the original, but a combination of 3 diagrams in the paper.)</p>\n<p><img src=\"transformer.png\" srcset=\"/img/loading.gif\" lazyload alt=\"transformer figure 1\" style=\"zoom:50%;\" /></p>\n<h4 id=\"encoder-decoder-stacks\">3.1 encoder decoder stacks</h4>\n<p><strong>Encoder:</strong></p>\n<p>In addition to briefly describing the encoder architecture, the authors mention that in order to avoid the projection step on the residual connection layers, the same input and output dimension is chosen in each sublayer, which is different from what CNN normally does. And this simple design yields the super parameters of the encoder to 2: Nx - the number of the \"encoder block\" and d_model - the feature dimension of the output layers. And this design makes the follow-up work such as bert, GPT-3 simple as well.</p>\n<p>Here is one thing that has been ignored: the definition of layer norm.</p>\n<blockquote>\n<p>Unlike <a href=\"https://paperswithcode.com/method/batch-normalization\">batch normalization</a>, <strong>Layer Normalization</strong> directly estimates the normalization statistics from the summed inputs to the neurons within a hidden layer so the normalization does not introduce any new dependencies between training cases.</p>\n</blockquote>\n<p>More is on https://paperswithcode.com/method/layer-normalization</p>\n<p><strong>Decoder:</strong></p>\n<p>Because in the prediction mode, the decoder is self-regressing, meaning that when predict y_t, only x_0 to x_t are available. However, the self-attention layer is able to attend to all the data i.e. x_0 - x_n. To prevent this, a masked multi-head attention layer is introduced so that when predict y_t, x_t+1 - x_n are masked.</p>\n<h4 id=\"attention\">3.2 Attention</h4>\n<p>Similarly, after running over the definition of attention mechanism, the 2 modifications: scaled dot-product attention and</p>\n<p>Multi-head attention are described.</p>\n<p>As we all known, there are two methods (attention scoring functions) of calculating the similarity of query and key(attention weights over the value). One is additive attention. It is complex but allows different length of key (dk) and query (qk). And Additive attention layer includes learnable parameters that can be tuned during training. Another one is dot-product attention, it is simply dot product of the transverse query and key matrix. It requires same length of query and key and no learnable parameters are introduced.</p>\n<p><strong>scaled dot-product attention</strong></p>\n<p>It pretty much is the dot-product attention multiply with a scaling factor of 1/√dk. And it is because for large dk, the deviation of the dot-product results might get too large, and the according gradient can get extremely small and hard to train.</p>\n<p>But look the figure 2 of the paper, an optional mask node is shown in the computation graph. And the algorithm of mask is not described in detail. Basically when calculating q_t, in the mask node, the attention scores a_t+1 - a_t are substituted to a huge negative so that after softmax, the attention weight is 0 on values v_t+1 to v_n. And it is corresponding to the how the decoder works in the prediction mode.</p>\n<p><strong>multi-head attention</strong></p>\n<p>How mutli-head works is introducing clearly in the paper:</p>\n<blockquote>\n<p>linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values. These are concatenated and once again projected, resulting in the final values</p>\n</blockquote>\n<p>The question is why doing that. First, no learnable parameters in the dot-product attention, and second, to mimic the CNN extracting the features from different perspectives.</p>\n<p><strong>Applications of Attention in our Model</strong></p>\n<p>It is well shown on the whole architecture figure. In the figure it consists of 2 self-attention block and 1 attention block, where key-value is from the encoder and query is from the decoder.</p>\n<h4 id=\"position-wise-feed-forward-networks\">3.3 Position-wise feed-forward networks</h4>\n<p>It is a simply an MLP with one hidden layer. And the hidden size is 4 times of the input and output size.</p>\n<h4 id=\"embedding-and-softmax\">3.4 Embedding and softmax</h4>\n<p>One point is they multiply the weights by√d_model. And first same reason with before, and second, with scaled weights, the scale matches the scale of positional encoding described below.</p>\n<h4 id=\"positional-encoding\">3.5 Positional Encoding</h4>\n<p>Different with RNN, transformer architecture reads no sequence information. And the authors add the positional information in the input data.</p>\n<h3 id=\"why-self-attention\">4 Why Self-Attention</h3>\n<p>Basically it explains the table 1 below:</p>\n<p><img src=\"transformer table1.png\" srcset=\"/img/loading.gif\" lazyload alt=\"transformer figure 1\" style=\"zoom:30%;\" /></p>\n<p>It might be the case but it's not been accepted by some researchers. And the restricted self-attention is rarely used. The table shows that if n and d are the same, self-attention, RNN, and CNN possess the same complexity per layer, but self-attention outperforms them on the sequential operations and the max path length. But actually, a huge mount of layers and parameters and data are required for a self-attention model achieving a similar result as RNN and CNN. And nowadays all models based on the transformer are very expensive.</p>\n<h3 id=\"results\">6 Results</h3>\n<h4 id=\"model-variations\">6.2 Model Variations</h4>\n<p><img src=\"transformer table 3.png\" srcset=\"/img/loading.gif\" lazyload alt=\"transformer table 3\" style=\"zoom:30%;\" /></p>\n<p>From Table 3, we can see the hyper parameters are not so much, and this simple design benefits the follow-ups. For example Bert and GPT.</p>\n<h2 id=\"reviews\">Reviews</h2>\n<p>Writing: It is concise and neat. But if the possible, it's better to describe why of doing it, and show more thoughts on the model to make the paper \"deeper\".</p>\n<p>Model: Transformer change the NLP filed just like how CNN change the CV filed. Through after transformer based model such as BERT, it is possible to pretrain a huge model to rise all the NLP performance. Besides, in other fields such as CV and audio, transformer becomes a great rising point. And the fact that transformer may suitable for all the deep learning tasks gives a new thought of muti-model learning. Maybe a general model able to extracting video, pictures, audio into a same semetic space is coming soon.</p>\n<p>Yet, despite the great experiment performance, we still can't fully understand why transformer works. For example,</p>\n<p>attention is not all one need, because the residual connection and MLP are all critical. We still don't know why.</p>\n<p>Why without explicitly model the sequence or the space, transformer outperform RNN and CNN. One explanation is that it is because transformer's inductive bias is more relaxed than either recurrent or convolutional architectures. And that is why huge amount of data are needed for transformer to achieve a good result.</p>\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>The transformer is the most important achievement in the last 5 years. It presents the fourth class of deep learning models besides MLP, CNN and RNN. And had a huge impact on the entire deep learning field, be it NLP or CV. Even the way the paper and network are named leads a trend.</p>\n</blockquote>\n<blockquote>\n<p>This is a <a href=\"https://daydreamatnight.github.io/2022/04/02/paper-reading-start/\">series of paper reading notes</a>, hopefully, to push me to read paper casually and to leave some record of what I've learned.</p>\n</blockquote>","more":"<p>Paper link: <a href=\"https://proceedings.neurips.cc/paper/7181-attention-is-all-you-need\">Attention is all you need</a></p>\n<p>Useful link: https://www.bilibili.com/video/BV1pu411o7BE</p>\n<h2 id=\"notes-by-sections\">Notes by sections</h2>\n<h3 id=\"abstract\">0. Abstract</h3>\n<p>According to the abstract, this work is first presented in the small field of machine translation, but because of the strong ability of generalisation. The transformer architecture has been extended in other fields such as CV and video.</p>\n<h3 id=\"conclusion\">6. Conclusion</h3>\n<p>In the last part of the conclusion section, the future of transformer is partly predicted by the authors, though most of these future work is done by other researchers.</p>\n<h3 id=\"introduction\">1. Introduction</h3>\n<p>Firstly, traditional RNN, or GRU has been introduced. Then after a brief description of the sequential nature of RNN based models, the main problems of them are presented. That is, poor ability of parallelising, and poor long-range dependencies.</p>\n<p>Later after the introduction of the attention framework and how to combine attention into RNN, transformer is presented.</p>\n<h3 id=\"background\">2. Background</h3>\n<p>First, with the goal of gaining the ability to parallelise, the authors look back on using CNN for sequential data. With CNN, parallelising ability is in its nature and with multiple channels, multiple features can be learned through training. Likewise, the transformer is also parallelisable and Multi-Head Attention is designed with the ability to learn multiple features. But additionally the transformer is easier to learn dependencies between distant positions.</p>\n<p>Afterwards, related work on self-attention and memory network is mentioned. And both the connections and distinctions with transformer are elaborated.</p>\n<h3 id=\"model-architecture\">3. Model Architecture</h3>\n<p>After introducing the basic idea of the encoder-decoder architecture, the key transformer architecture diagram is given. And each part of the model is briefly described. (The diagram in this blog below is not the original, but a combination of 3 diagrams in the paper.)</p>\n<p><img src=\"transformer.png\" alt=\"transformer figure 1\" style=\"zoom:50%;\" /></p>\n<h4 id=\"encoder-decoder-stacks\">3.1 encoder decoder stacks</h4>\n<p><strong>Encoder:</strong></p>\n<p>In addition to briefly describing the encoder architecture, the authors mention that in order to avoid the projection step on the residual connection layers, the same input and output dimension is chosen in each sublayer, which is different from what CNN normally does. And this simple design yields the super parameters of the encoder to 2: Nx - the number of the \"encoder block\" and d_model - the feature dimension of the output layers. And this design makes the follow-up work such as bert, GPT-3 simple as well.</p>\n<p>Here is one thing that has been ignored: the definition of layer norm.</p>\n<blockquote>\n<p>Unlike <a href=\"https://paperswithcode.com/method/batch-normalization\">batch normalization</a>, <strong>Layer Normalization</strong> directly estimates the normalization statistics from the summed inputs to the neurons within a hidden layer so the normalization does not introduce any new dependencies between training cases.</p>\n</blockquote>\n<p>More is on https://paperswithcode.com/method/layer-normalization</p>\n<p><strong>Decoder:</strong></p>\n<p>Because in the prediction mode, the decoder is self-regressing, meaning that when predict y_t, only x_0 to x_t are available. However, the self-attention layer is able to attend to all the data i.e. x_0 - x_n. To prevent this, a masked multi-head attention layer is introduced so that when predict y_t, x_t+1 - x_n are masked.</p>\n<h4 id=\"attention\">3.2 Attention</h4>\n<p>Similarly, after running over the definition of attention mechanism, the 2 modifications: scaled dot-product attention and</p>\n<p>Multi-head attention are described.</p>\n<p>As we all known, there are two methods (attention scoring functions) of calculating the similarity of query and key(attention weights over the value). One is additive attention. It is complex but allows different length of key (dk) and query (qk). And Additive attention layer includes learnable parameters that can be tuned during training. Another one is dot-product attention, it is simply dot product of the transverse query and key matrix. It requires same length of query and key and no learnable parameters are introduced.</p>\n<p><strong>scaled dot-product attention</strong></p>\n<p>It pretty much is the dot-product attention multiply with a scaling factor of 1/√dk. And it is because for large dk, the deviation of the dot-product results might get too large, and the according gradient can get extremely small and hard to train.</p>\n<p>But look the figure 2 of the paper, an optional mask node is shown in the computation graph. And the algorithm of mask is not described in detail. Basically when calculating q_t, in the mask node, the attention scores a_t+1 - a_t are substituted to a huge negative so that after softmax, the attention weight is 0 on values v_t+1 to v_n. And it is corresponding to the how the decoder works in the prediction mode.</p>\n<p><strong>multi-head attention</strong></p>\n<p>How mutli-head works is introducing clearly in the paper:</p>\n<blockquote>\n<p>linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values. These are concatenated and once again projected, resulting in the final values</p>\n</blockquote>\n<p>The question is why doing that. First, no learnable parameters in the dot-product attention, and second, to mimic the CNN extracting the features from different perspectives.</p>\n<p><strong>Applications of Attention in our Model</strong></p>\n<p>It is well shown on the whole architecture figure. In the figure it consists of 2 self-attention block and 1 attention block, where key-value is from the encoder and query is from the decoder.</p>\n<h4 id=\"position-wise-feed-forward-networks\">3.3 Position-wise feed-forward networks</h4>\n<p>It is a simply an MLP with one hidden layer. And the hidden size is 4 times of the input and output size.</p>\n<h4 id=\"embedding-and-softmax\">3.4 Embedding and softmax</h4>\n<p>One point is they multiply the weights by√d_model. And first same reason with before, and second, with scaled weights, the scale matches the scale of positional encoding described below.</p>\n<h4 id=\"positional-encoding\">3.5 Positional Encoding</h4>\n<p>Different with RNN, transformer architecture reads no sequence information. And the authors add the positional information in the input data.</p>\n<h3 id=\"why-self-attention\">4 Why Self-Attention</h3>\n<p>Basically it explains the table 1 below:</p>\n<p><img src=\"transformer table1.png\" alt=\"transformer figure 1\" style=\"zoom:30%;\" /></p>\n<p>It might be the case but it's not been accepted by some researchers. And the restricted self-attention is rarely used. The table shows that if n and d are the same, self-attention, RNN, and CNN possess the same complexity per layer, but self-attention outperforms them on the sequential operations and the max path length. But actually, a huge mount of layers and parameters and data are required for a self-attention model achieving a similar result as RNN and CNN. And nowadays all models based on the transformer are very expensive.</p>\n<h3 id=\"results\">6 Results</h3>\n<h4 id=\"model-variations\">6.2 Model Variations</h4>\n<p><img src=\"transformer table 3.png\" alt=\"transformer table 3\" style=\"zoom:30%;\" /></p>\n<p>From Table 3, we can see the hyper parameters are not so much, and this simple design benefits the follow-ups. For example Bert and GPT.</p>\n<h2 id=\"reviews\">Reviews</h2>\n<p>Writing: It is concise and neat. But if the possible, it's better to describe why of doing it, and show more thoughts on the model to make the paper \"deeper\".</p>\n<p>Model: Transformer change the NLP filed just like how CNN change the CV filed. Through after transformer based model such as BERT, it is possible to pretrain a huge model to rise all the NLP performance. Besides, in other fields such as CV and audio, transformer becomes a great rising point. And the fact that transformer may suitable for all the deep learning tasks gives a new thought of muti-model learning. Maybe a general model able to extracting video, pictures, audio into a same semetic space is coming soon.</p>\n<p>Yet, despite the great experiment performance, we still can't fully understand why transformer works. For example,</p>\n<p>attention is not all one need, because the residual connection and MLP are all critical. We still don't know why.</p>\n<p>Why without explicitly model the sequence or the space, transformer outperform RNN and CNN. One explanation is that it is because transformer's inductive bias is more relaxed than either recurrent or convolutional architectures. And that is why huge amount of data are needed for transformer to achieve a good result.</p>","wordcount":6974}],"PostAsset":[{"_id":"source/_posts/Intro-and-Pytorch-Implementation-of-Label-Smoothing-Regularization-LSR/Cross entropy loss function.png","post":"cl2n5f4040001p9yb876h7dm4","slug":"Cross entropy loss function.png","modified":1,"renderable":1},{"_id":"source/_posts/Intro-and-Pytorch-Implementation-of-Label-Smoothing-Regularization-LSR/Label smoothing feature norm.png","post":"cl2n5f4040001p9yb876h7dm4","slug":"Label smoothing feature norm.png","modified":1,"renderable":1},{"_id":"source/_posts/Intro-and-Pytorch-Implementation-of-Label-Smoothing-Regularization-LSR/accuracy curve applying label smoothing.png","post":"cl2n5f4040001p9yb876h7dm4","slug":"accuracy curve applying label smoothing.png","modified":1,"renderable":1},{"_id":"source/_posts/Intro-and-Pytorch-Implementation-of-Label-Smoothing-Regularization-LSR/accuracy curve compare label smoothing with hard label.png","post":"cl2n5f4040001p9yb876h7dm4","slug":"accuracy curve compare label smoothing with hard label.png","modified":1,"renderable":1},{"_id":"source/_posts/Intro-and-Pytorch-Implementation-of-Label-Smoothing-Regularization-LSR/hard label.png","post":"cl2n5f4040001p9yb876h7dm4","slug":"hard label.png","modified":1,"renderable":1},{"_id":"source/_posts/Intro-and-Pytorch-Implementation-of-Label-Smoothing-Regularization-LSR/leave class code.png","post":"cl2n5f4040001p9yb876h7dm4","slug":"leave class code.png","modified":1,"renderable":1},{"_id":"source/_posts/Intro-and-Pytorch-Implementation-of-Label-Smoothing-Regularization-LSR/soft label.png","post":"cl2n5f4040001p9yb876h7dm4","slug":"soft label.png","modified":1,"renderable":1},{"_id":"source/_posts/Intro-and-Pytorch-Implementation-of-Label-Smoothing-Regularization-LSR/source ppt.pptx","post":"cl2n5f4040001p9yb876h7dm4","slug":"source ppt.pptx","modified":1,"renderable":1},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/Google sitemap inspect 2.png","post":"cl2n5f4080006p9yb6qu44826","slug":"Google sitemap inspect 2.png","modified":1,"renderable":1},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/Google sitemap inspect 3.png","post":"cl2n5f4080006p9yb6qu44826","slug":"Google sitemap inspect 3.png","modified":1,"renderable":1},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/Google sitemap inspect.png","post":"cl2n5f4080006p9yb6qu44826","slug":"Google sitemap inspect.png","modified":1,"renderable":1},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/Last snapshot.png","post":"cl2n5f4080006p9yb6qu44826","slug":"Last snapshot.png","modified":1,"renderable":1},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/baidu console sitemap.png","post":"cl2n5f4080006p9yb6qu44826","slug":"baidu console sitemap.png","modified":1,"renderable":1},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/baidu console varification.png","post":"cl2n5f4080006p9yb6qu44826","slug":"baidu console varification.png","modified":1,"renderable":1},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/baidu console.png","post":"cl2n5f4080006p9yb6qu44826","slug":"baidu console.png","modified":1,"renderable":1},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/being sitemap connect google.png","post":"cl2n5f4080006p9yb6qu44826","slug":"being sitemap connect google.png","modified":1,"renderable":1},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/bing search console success.png","post":"cl2n5f4080006p9yb6qu44826","slug":"bing search console success.png","modified":1,"renderable":1},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/bing sitemap.png","post":"cl2n5f4080006p9yb6qu44826","slug":"bing sitemap.png","modified":1,"renderable":1},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/can't fetch sitemap.png","post":"cl2n5f4080006p9yb6qu44826","slug":"can't fetch sitemap.png","modified":1,"renderable":1},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/check google search.png","post":"cl2n5f4080006p9yb6qu44826","slug":"check google search.png","modified":1,"renderable":1},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/github page.png","post":"cl2n5f4080006p9yb6qu44826","slug":"github page.png","modified":1,"renderable":1},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/google console varification.png","post":"cl2n5f4080006p9yb6qu44826","slug":"google console varification.png","modified":1,"renderable":1},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/google console.png","post":"cl2n5f4080006p9yb6qu44826","slug":"google console.png","modified":1,"renderable":1},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/node js install.png","post":"cl2n5f4080006p9yb6qu44826","slug":"node js install.png","modified":1,"renderable":1},{"_id":"source/_posts/Build-and-configure-a-personal-blog-via-hexo-and-yilia/typora setting.png","post":"cl2n5f4080006p9yb6qu44826","slug":"typora setting.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-A-gentle-introduction-to-graph-neural-networks/GNN graph level task.png","post":"cl2n5f4080007p9ybehc1eock","slug":"GNN graph level task.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-A-gentle-introduction-to-graph-neural-networks/GNN graph message passing.png","post":"cl2n5f4080007p9ybehc1eock","slug":"GNN graph message passing.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-A-gentle-introduction-to-graph-neural-networks/GNN interative archtecture.png","post":"cl2n5f4080007p9ybehc1eock","slug":"GNN interative archtecture.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-AlexNet/alexnet results.png","post":"cl2n5f4080009p9yb8uc8drd1","slug":"alexnet results.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-AlexNet/unsupervise learning cake.png","post":"cl2n5f4080009p9yb8uc8drd1","slug":"unsupervise learning cake.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-MAE/MAE architecture.png","post":"cl2n5f409000ep9yb44de8xtp","slug":"MAE architecture.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-MAE/MAE mask ratio.png","post":"cl2n5f409000ep9yb44de8xtp","slug":"MAE mask ratio.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-MAE/MAE result.png","post":"cl2n5f409000ep9yb44de8xtp","slug":"MAE result.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-MAE/MAE result2.png","post":"cl2n5f409000ep9yb44de8xtp","slug":"MAE result2.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-GPT1-3/BERT result with GPT.png","post":"cl2n5f40a000hp9ybhg9o1skr","slug":"BERT result with GPT.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-GPT1-3/GPT 3 result 5.png","post":"cl2n5f40a000hp9ybhg9o1skr","slug":"GPT 3 result 5.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-GPT1-3/GPT fine tune f1.png","post":"cl2n5f40a000hp9ybhg9o1skr","slug":"GPT fine tune f1.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-GPT1-3/GPT fine tune f2.png","post":"cl2n5f40a000hp9ybhg9o1skr","slug":"GPT fine tune f2.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-GPT1-3/GPT fine tune loss.png","post":"cl2n5f40a000hp9ybhg9o1skr","slug":"GPT fine tune loss.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-GPT1-3/GPT loss.png","post":"cl2n5f40a000hp9ybhg9o1skr","slug":"GPT loss.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-GPT1-3/GPT objectives.png","post":"cl2n5f40a000hp9ybhg9o1skr","slug":"GPT objectives.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-GPT1-3/GPT pretraining process.png","post":"cl2n5f40a000hp9ybhg9o1skr","slug":"GPT pretraining process.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-GPT1-3/GPT timeline.png","post":"cl2n5f40a000hp9ybhg9o1skr","slug":"GPT timeline.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-GPT1-3/GPT-3 approach.png","post":"cl2n5f40a000hp9ybhg9o1skr","slug":"GPT-3 approach.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-GPT1-3/GPT-3 result 2.png","post":"cl2n5f40a000hp9ybhg9o1skr","slug":"GPT-3 result 2.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-GPT1-3/GPT2 performance.png","post":"cl2n5f40a000hp9ybhg9o1skr","slug":"GPT2 performance.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-GPT1-3/GPT3 gender.png","post":"cl2n5f40a000hp9ybhg9o1skr","slug":"GPT3 gender.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-GPT1-3/GPT3 models.png","post":"cl2n5f40a000hp9ybhg9o1skr","slug":"GPT3 models.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-GPT1-3/GPT3 performance with compute.png","post":"cl2n5f40a000hp9ybhg9o1skr","slug":"GPT3 performance with compute.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-GPT1-3/GPT3 performance.png","post":"cl2n5f40a000hp9ybhg9o1skr","slug":"GPT3 performance.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-GPT1-3/GPT3 race.png","post":"cl2n5f40a000hp9ybhg9o1skr","slug":"GPT3 race.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-GPT1-3/GPT3 result 3.png","post":"cl2n5f40a000hp9ybhg9o1skr","slug":"GPT3 result 3.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-GPT1-3/GPT3 result.png","post":"cl2n5f40a000hp9ybhg9o1skr","slug":"GPT3 result.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-GPT1-3/GPT3 result4.png","post":"cl2n5f40a000hp9ybhg9o1skr","slug":"GPT3 result4.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-GPT1-3/GPT3 training data.png","post":"cl2n5f40a000hp9ybhg9o1skr","slug":"GPT3 training data.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-ResNet/ResNet figure 1.png","post":"cl2n5f40a000jp9yb7z537myw","slug":"ResNet figure 1.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-ResNet/ResNet figure 2.png","post":"cl2n5f40a000jp9yb7z537myw","slug":"ResNet figure 2.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-ResNet/ResNet figure 4.png","post":"cl2n5f40a000jp9yb7z537myw","slug":"ResNet figure 4.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-ResNet/ResNet figure 5.png","post":"cl2n5f40a000jp9yb7z537myw","slug":"ResNet figure 5.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-ResNet/ResNet table 1.png","post":"cl2n5f40a000jp9yb7z537myw","slug":"ResNet table 1.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-ResNet/ResNet table 3.png","post":"cl2n5f40a000jp9yb7z537myw","slug":"ResNet table 3.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-ResNet/ResNet table 6.png","post":"cl2n5f40a000jp9yb7z537myw","slug":"ResNet table 6.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-bert/BERT alibation study 1.png","post":"cl2n5f40b000mp9yb4ghuhlsz","slug":"BERT alibation study 1.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-bert/BERT learnable paramters.png","post":"cl2n5f40b000mp9yb4ghuhlsz","slug":"BERT learnable paramters.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-bert/BERT segment embedding.png","post":"cl2n5f40b000mp9yb4ghuhlsz","slug":"BERT segment embedding.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-bert/model size graph.webp","post":"cl2n5f40b000mp9yb4ghuhlsz","slug":"model size graph.webp","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-Vision-Transformer/CNN first layer filters.png","post":"cl2n5f40b000op9ybba8384ls","slug":"CNN first layer filters.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-Vision-Transformer/VIT algorithm.png","post":"cl2n5f40b000op9ybba8384ls","slug":"VIT algorithm.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-Vision-Transformer/VIT class token ablation.png","post":"cl2n5f40b000op9ybba8384ls","slug":"VIT class token ablation.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-Vision-Transformer/VIT model overview.png","post":"cl2n5f40b000op9ybba8384ls","slug":"VIT model overview.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-Vision-Transformer/VIT positional embedding ablation.png","post":"cl2n5f40b000op9ybba8384ls","slug":"VIT positional embedding ablation.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-Vision-Transformer/VIT properties.png","post":"cl2n5f40b000op9ybba8384ls","slug":"VIT properties.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-Vision-Transformer/ViT ablation 2.png","post":"cl2n5f40b000op9ybba8384ls","slug":"ViT ablation 2.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-Vision-Transformer/ViT inspecting.png","post":"cl2n5f40b000op9ybba8384ls","slug":"ViT inspecting.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-Vision-Transformer/ViT results.png","post":"cl2n5f40b000op9ybba8384ls","slug":"ViT results.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-Vision-Transformer/ViT variants.png","post":"cl2n5f40b000op9ybba8384ls","slug":"ViT variants.png","modified":1,"renderable":1},{"_id":"source/_posts/learning-rate-schedule/2d cyclic learning rate schedule.png","post":"cl2n5f409000ap9ybaswy20uo","slug":"2d cyclic learning rate schedule.png","modified":1,"renderable":1},{"_id":"source/_posts/learning-rate-schedule/CLR.png","post":"cl2n5f409000ap9ybaswy20uo","slug":"CLR.png","modified":1,"renderable":1},{"_id":"source/_posts/learning-rate-schedule/SGD with learning rate decay.png","post":"cl2n5f409000ap9ybaswy20uo","slug":"SGD with learning rate decay.png","modified":1,"renderable":1},{"_id":"source/_posts/learning-rate-schedule/SGDR.png","post":"cl2n5f409000ap9ybaswy20uo","slug":"SGDR.png","modified":1,"renderable":1},{"_id":"source/_posts/learning-rate-schedule/SGDR_REsult.png","post":"cl2n5f409000ap9ybaswy20uo","slug":"SGDR_REsult.png","modified":1,"renderable":1},{"_id":"source/_posts/learning-rate-schedule/cyclic learning rate schedule.png","post":"cl2n5f409000ap9ybaswy20uo","slug":"cyclic learning rate schedule.png","modified":1,"renderable":1},{"_id":"source/_posts/learning-rate-schedule/experiment.png","post":"cl2n5f409000ap9ybaswy20uo","slug":"experiment.png","modified":1,"renderable":1},{"_id":"source/_posts/learning-rate-schedule/source ppt.pptx","post":"cl2n5f409000ap9ybaswy20uo","slug":"source ppt.pptx","modified":1,"renderable":1},{"_id":"source/_posts/learning-rate-schedule/warmup on large batches.png","post":"cl2n5f409000ap9ybaswy20uo","slug":"warmup on large batches.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-transformer/layer norm.png","post":"cl2n5f40e001kp9yb21ny6ups","slug":"layer norm.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-transformer/transformer table 3.png","post":"cl2n5f40e001kp9yb21ny6ups","slug":"transformer table 3.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-transformer/transformer table1.png","post":"cl2n5f40e001kp9yb21ny6ups","slug":"transformer table1.png","modified":1,"renderable":1},{"_id":"source/_posts/paper-reading-transformer/transformer.png","post":"cl2n5f40e001kp9yb21ny6ups","slug":"transformer.png","modified":1,"renderable":1}],"PostCategory":[],"PostTag":[{"post_id":"cl2n5f4040001p9yb876h7dm4","tag_id":"cl2n5f4070004p9ybezco54bf","_id":"cl2n5f409000dp9yb3r4pa6r6"},{"post_id":"cl2n5f4040001p9yb876h7dm4","tag_id":"cl2n5f4080008p9ybhy7431m7","_id":"cl2n5f409000fp9yb7qh236po"},{"post_id":"cl2n5f409000ap9ybaswy20uo","tag_id":"cl2n5f4070004p9ybezco54bf","_id":"cl2n5f40a000ip9yb5kovb5hz"},{"post_id":"cl2n5f409000ap9ybaswy20uo","tag_id":"cl2n5f4080008p9ybhy7431m7","_id":"cl2n5f40a000kp9yb09psdq5q"},{"post_id":"cl2n5f4060003p9ybc2ke3ys7","tag_id":"cl2n5f409000bp9yb800jazgg","_id":"cl2n5f40b000np9ybg1919oqv"},{"post_id":"cl2n5f4070005p9yb48u836hy","tag_id":"cl2n5f40a000gp9ybhrqte13a","_id":"cl2n5f40c000qp9ybbznb1b4d"},{"post_id":"cl2n5f4070005p9yb48u836hy","tag_id":"cl2n5f40b000lp9yb4r3e3dr9","_id":"cl2n5f40c000rp9ybec4y45i3"},{"post_id":"cl2n5f4080006p9yb6qu44826","tag_id":"cl2n5f40a000gp9ybhrqte13a","_id":"cl2n5f40c000up9yb6ypt6ild"},{"post_id":"cl2n5f4080006p9yb6qu44826","tag_id":"cl2n5f40b000lp9yb4r3e3dr9","_id":"cl2n5f40c000vp9yb465a4lcl"},{"post_id":"cl2n5f4080007p9ybehc1eock","tag_id":"cl2n5f40c000tp9ybbqpd38hj","_id":"cl2n5f40c000xp9ybcr0acye9"},{"post_id":"cl2n5f4080007p9ybehc1eock","tag_id":"cl2n5f4070004p9ybezco54bf","_id":"cl2n5f40c000yp9ybas6vdn23"},{"post_id":"cl2n5f4080009p9yb8uc8drd1","tag_id":"cl2n5f40c000tp9ybbqpd38hj","_id":"cl2n5f40c0010p9ybhqvncjwh"},{"post_id":"cl2n5f4080009p9yb8uc8drd1","tag_id":"cl2n5f4070004p9ybezco54bf","_id":"cl2n5f40c0011p9ybfuof0855"},{"post_id":"cl2n5f409000cp9yb4h2thdq1","tag_id":"cl2n5f40c000zp9yb680nfuyb","_id":"cl2n5f40c0013p9yb1uyn1xak"},{"post_id":"cl2n5f409000cp9yb4h2thdq1","tag_id":"cl2n5f4070004p9ybezco54bf","_id":"cl2n5f40c0014p9yb35d66m4e"},{"post_id":"cl2n5f409000ep9yb44de8xtp","tag_id":"cl2n5f40c000tp9ybbqpd38hj","_id":"cl2n5f40c0016p9yb6rpu01f8"},{"post_id":"cl2n5f409000ep9yb44de8xtp","tag_id":"cl2n5f4070004p9ybezco54bf","_id":"cl2n5f40d0017p9yb7fprgqjo"},{"post_id":"cl2n5f40a000hp9ybhg9o1skr","tag_id":"cl2n5f40c000tp9ybbqpd38hj","_id":"cl2n5f40d0019p9yb4apobjy3"},{"post_id":"cl2n5f40a000hp9ybhg9o1skr","tag_id":"cl2n5f4070004p9ybezco54bf","_id":"cl2n5f40d001ap9yb99xlhf5j"},{"post_id":"cl2n5f40a000jp9yb7z537myw","tag_id":"cl2n5f40c000tp9ybbqpd38hj","_id":"cl2n5f40d001cp9ybc8lndj3d"},{"post_id":"cl2n5f40a000jp9yb7z537myw","tag_id":"cl2n5f4070004p9ybezco54bf","_id":"cl2n5f40d001dp9yb4oy93pbg"},{"post_id":"cl2n5f40b000mp9yb4ghuhlsz","tag_id":"cl2n5f40c000tp9ybbqpd38hj","_id":"cl2n5f40d001fp9yb2kd5ehq0"},{"post_id":"cl2n5f40b000mp9yb4ghuhlsz","tag_id":"cl2n5f4070004p9ybezco54bf","_id":"cl2n5f40d001gp9yb4l1v7tzr"},{"post_id":"cl2n5f40b000op9ybba8384ls","tag_id":"cl2n5f40c000tp9ybbqpd38hj","_id":"cl2n5f40d001hp9ybfiztfd7m"},{"post_id":"cl2n5f40b000op9ybba8384ls","tag_id":"cl2n5f4070004p9ybezco54bf","_id":"cl2n5f40d001ip9yb2x3ahr47"},{"post_id":"cl2n5f40e001jp9ybgot3h257","tag_id":"cl2n5f40c000tp9ybbqpd38hj","_id":"cl2n5f40e001lp9yb2z514wdw"},{"post_id":"cl2n5f40e001jp9ybgot3h257","tag_id":"cl2n5f4070004p9ybezco54bf","_id":"cl2n5f40e001mp9yb1ggdex6g"},{"post_id":"cl2n5f40e001kp9yb21ny6ups","tag_id":"cl2n5f40c000tp9ybbqpd38hj","_id":"cl2n5f40e001np9ybc1l71eja"},{"post_id":"cl2n5f40e001kp9yb21ny6ups","tag_id":"cl2n5f4070004p9ybezco54bf","_id":"cl2n5f40e001op9yb2dgm0p0i"}],"Tag":[{"name":"deep learning","_id":"cl2n5f4070004p9ybezco54bf"},{"name":"deep learning tricks","_id":"cl2n5f4080008p9ybhy7431m7"},{"name":"First Blog","_id":"cl2n5f409000bp9yb800jazgg"},{"name":"hexo","_id":"cl2n5f40a000gp9ybhrqte13a"},{"name":"blog","_id":"cl2n5f40b000lp9yb4r3e3dr9"},{"name":"paper reading","_id":"cl2n5f40c000tp9ybbqpd38hj"},{"name":"m1 mac","_id":"cl2n5f40c000zp9yb680nfuyb"}]}}